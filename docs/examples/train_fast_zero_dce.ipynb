{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/restorers/Train_Zero_DCE_Restorers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{restorers-zero-dce-train} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåà Restorers + WandB ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{restorers-mirnetv2-train} -->\n",
    "\n",
    "This notebook shows how to train a [Zero-DCE](https://arxiv.org/abs/2001.06826) model for zero-reference low-light enhancement using [**restorers**](https://github.com/soumik12345/restorers) and [**wandb**](https://wandb.ai/site). For more details regarding usage of restorers, refer to the following report:\n",
    "\n",
    "[![](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.ai/ml-colabs/low-light-enhancement/reports/Lighting-up-Images-in-the-Deep-Learning-Era--VmlldzozNzE4Njkz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip setuptools\n",
    "!pip install git+https://github.com/soumik12345/restorers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import wandb\n",
    "# import the wandb callbacks for keras\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "from restorers.model.zero_dce import ZeroDCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"low-light-enhancement\", job_type=\"train\")\n",
    "\n",
    "\n",
    "def load_data(image_path):\n",
    "    image = tf.io.read_file(image_path)\n",
    "    image = tf.image.decode_png(image, channels=3)\n",
    "    image = tf.image.resize(\n",
    "        images=image,\n",
    "        size=[256, 256]\n",
    "    )\n",
    "    image = image / ((2 ** 8) - 1)\n",
    "    return image\n",
    "\n",
    "\n",
    "def data_generator(low_light_images):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((low_light_images))\n",
    "    dataset = dataset.map(load_data, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    dataset = dataset.batch(8, drop_remainder=True)\n",
    "    return dataset\n",
    "\n",
    "\n",
    "artifact = wandb.use_artifact(\"ml-colabs/dataset/LoL:v0\", type='dataset')\n",
    "artifact_dir = artifact.download()\n",
    "\n",
    "train_low_light_images = sorted(glob(os.path.join(artifact_dir, \"our485\", \"low\", \"*\")))\n",
    "num_train_images = int((1 - 0.2) * len(train_low_light_images))\n",
    "val_low_light_images = train_low_light_images[num_train_images:]\n",
    "train_low_light_images = train_low_light_images[:num_train_images]\n",
    "\n",
    "train_dataset = data_generator(train_low_light_images)\n",
    "val_dataset = data_generator(val_low_light_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the ZeroDCE model; this gives us a `tf.keras.Model`\n",
    "model = ZeroDCE(\n",
    "    num_intermediate_filters=32, # number of filters in the intermediate convolutional layers\n",
    "    num_iterations=8, # number of iterations of enhancement\n",
    "    decoder_channel_factor=1 # factor by which number filters in the decoder of deep curve estimation layer is multiplied\n",
    ")\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
    "    weight_exposure_loss=1.0, # weight of the exposure control loss\n",
    "    weight_color_constancy_loss=0.5, # weight of the color constancy loss\n",
    "    weight_illumination_smoothness_loss=20, # weight of the illumination smoothness loss\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    # define the metrics logger callback;\n",
    "    # we set the `log_freq=\"batch\"` explicitly\n",
    "    # to the metrics are logged both batch-wise and epoch-wise\n",
    "    WandbMetricsLogger(log_freq=\"batch\"),\n",
    "    # define the model checkpoint callback\n",
    "    WandbModelCheckpoint(\n",
    "        filepath=\"checkpoint\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=False,\n",
    "        initial_value_threshold=None,\n",
    "    )\n",
    "]\n",
    "\n",
    "# call model.fit()\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
