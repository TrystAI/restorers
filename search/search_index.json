{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Restorers","text":"<p>Restorers provide out-of-the-box TensorFlow implementations of SoTA image and video restoration models for tasks such as low-light enhancement, denoising, deblurring, super-resolution, etc.</p> Lighting up Images in the Deep Learning Era: Digging into Low-Light Image Enhancement"},{"location":"metrics/","title":"Metrics","text":""},{"location":"metrics/#restorers.metrics.PSNRMetric","title":"<code>PSNRMetric</code>","text":"<p>         Bases: <code>tf.keras.metrics.Metric</code></p> <p>Stateful Tensorflow metric for caclulating Peak Signal-to-noise Ratio.</p> <p>Parameters:</p> Name Type Description Default <code>max_val</code> <code>float</code> <p>The dynamic range of the images (i.e., the difference between the maximum the and minimum allowed values).</p> required Source code in <code>restorers/metrics.py</code> <pre><code>class PSNRMetric(tf.keras.metrics.Metric):\n\"\"\"Stateful Tensorflow metric for caclulating\n    [Peak Signal-to-noise Ratio](https://en.wikipedia.org/wiki/Peak_signal-to-noise_ratio).\n    Args:\n        max_val (float): The dynamic range of the images\n            (i.e., the difference between the maximum the and minimum allowed values).\n    \"\"\"\ndef __init__(self, max_val: float, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.max_val = max_val\nself.psnr = tf.keras.metrics.Mean(name=\"psnr\")\ndef update_state(self, y_true, y_pred, *args, **kwargs):\npsnr = tf.image.psnr(\nscale_tensor(y_true), scale_tensor(y_pred), max_val=self.max_val\n)\nself.psnr.update_state(psnr, *args, **kwargs)\ndef result(self):\nreturn self.psnr.result()\ndef reset_state(self):\nself.psnr.reset_state()\n</code></pre>"},{"location":"metrics/#restorers.metrics.SSIMMetric","title":"<code>SSIMMetric</code>","text":"<p>         Bases: <code>tf.keras.metrics.Metric</code></p> <p>Stateful Tensorflow metric for caclulating Structural Similarity.</p> <p>Parameters:</p> Name Type Description Default <code>max_val</code> <code>float</code> <p>The dynamic range of the images (i.e., the difference between the maximum the and minimum allowed values).</p> required Source code in <code>restorers/metrics.py</code> <pre><code>class SSIMMetric(tf.keras.metrics.Metric):\n\"\"\"Stateful Tensorflow metric for caclulating\n    [Structural Similarity](https://en.wikipedia.org/wiki/Structural_similarity).\n    Args:\n        max_val (float): The dynamic range of the images\n            (i.e., the difference between the maximum the and minimum allowed values).\n    \"\"\"\ndef __init__(self, max_val: float, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.max_val = max_val\nself.ssim = tf.keras.metrics.Mean(name=\"ssim\")\ndef update_state(self, y_true, y_pred, *args, **kwargs):\nssim = tf.image.ssim(\nscale_tensor(y_true), scale_tensor(y_pred), max_val=self.max_val\n)\nself.ssim.update_state(ssim, *args, **kwargs)\ndef result(self):\nreturn self.ssim.result()\ndef reset_state(self):\nself.ssim.reset_state()\n</code></pre>"},{"location":"dataloader/base/","title":"Base Dataset Factory","text":""},{"location":"dataloader/base/#restorers.dataloader.base.base_dataloader.DatasetFactory","title":"<code>DatasetFactory</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract base class for building dataset factories or dataloaders.</p> <p>Abstract functions to be overriden are:</p> <ul> <li> <p><code>fetch_dataset(self, val_split: float, visualize_on_wandb: bool) -&gt; None</code></p> <ul> <li>Function to fetch the dataset from a hosted artifact on the web.</li> </ul> </li> <li> <p><code>sanity_tests(self) -&gt; None</code></p> <ul> <li>Function to perform sanity tests on the dataset. This could include logic     to visualize or perrform exploratory analysis on the dataset.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>The image resolution.</p> required <code>bit_depth</code> <code>int</code> <p>Bit depth of the images for normalization.</p> required <code>val_split</code> <code>float</code> <p>The percentage of validation split.</p> required <code>visualize_on_wandb</code> <code>bool</code> <p>Flag to visualize the dataset on wandb.</p> required Source code in <code>restorers/dataloader/base/base_dataloader.py</code> <pre><code>class DatasetFactory(ABC):\n\"\"\"\n    Abstract base class for building dataset factories or dataloaders.\n    Abstract functions to be overriden are:\n    - `fetch_dataset(self, val_split: float, visualize_on_wandb: bool) -&gt; None`\n        - Function to fetch the dataset from a hosted artifact on the web.\n    - `sanity_tests(self) -&gt; None`\n        - Function to perform sanity tests on the dataset. This could include logic\n            to visualize or perrform exploratory analysis on the dataset.\n    Args:\n        image_size (int): The image resolution.\n        bit_depth (int): Bit depth of the images for normalization.\n        val_split (float): The percentage of validation split.\n        visualize_on_wandb (bool): Flag to visualize the dataset on wandb.\n    \"\"\"\ndef __init__(\nself,\nimage_size: int,\nbit_depth: int,\nval_split: float,\nvisualize_on_wandb: bool,\n) -&gt; None:\nself.image_size = image_size\nself.normalization_factor = (2**bit_depth) - 1\nself.fetch_dataset(val_split, visualize_on_wandb)\n@abstractmethod\ndef fetch_dataset(self, val_split: float, visualize_on_wandb: bool) -&gt; None:\nraise NotImplementedError(f\"{self.__class__.__name__ }.fetch_dataset\")\n@abstractmethod\ndef sanity_tests(self) -&gt; None:\nraise NotImplementedError(f\"{self.__class__.__name__ }.sanity_tests\")\ndef random_crop(\nself, input_image: tf.Tensor, enhanced_image: tf.Tensor\n) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n\"\"\"\n        Function to apply random cropping.\n        Args:\n            input_image (`tf.Tensor`): Low light image.\n            enhanced_image (`tf.Tensor`): Enhanced image.\n        Returns:\n            (Tuple[tf.Tensor, tf.Tensor]): A tuple of random cropped image.\n        \"\"\"\n# Check whether the image size is smaller than the original image\nimage_size = tf.minimum(self.image_size, tf.shape(input_image)[0])\n# Concatenate the low light and enhanced image\nconcatenated_image = tf.concat([input_image, enhanced_image], axis=-1)\n# Apply same *random* crop to the concantenated image and split the stack\ncropped_concatenated_image = tf.image.random_crop(\nconcatenated_image, (image_size, image_size, 6)\n)\ncropped_input_image, cropped_enhanced_image = tf.split(\ncropped_concatenated_image, num_or_size_splits=2, axis=-1\n)\n# Ensuring the dataset tensor_spec is not None is the spatial dimensions\ncropped_input_image.set_shape([self.image_size, self.image_size, 3])\ncropped_enhanced_image.set_shape([self.image_size, self.image_size, 3])\nreturn cropped_input_image, cropped_enhanced_image\ndef resize(\nself, input_image: tf.Tensor, enhanced_image: tf.Tensor\n) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n\"\"\"\n        Function to resize images.\n        Args:\n            input_image (`tf.Tensor`): Low light image.\n            enhanced_image (`tf.Tensor`): Enhanced image.\n        Returns:\n            (Tuple[tf.Tensor, tf.Tensor]): A tuple of tf.Tensor resized images.\n        \"\"\"\n# Check whether the image size is smaller than the original image\nimage_size = tf.minimum(self.image_size, tf.shape(input_image)[0])\ninput_image = tf.image.resize(\ninput_image,\nsize=[image_size, image_size],\n)\nenhanced_image = tf.image.resize(\nenhanced_image,\nsize=[image_size, image_size],\n)\n# Ensuring the dataset tensor_spec is not None is the spatial dimensions\ninput_image.set_shape([self.image_size, self.image_size, 3])\nenhanced_image.set_shape([self.image_size, self.image_size, 3])\nreturn input_image, enhanced_image\ndef load_image(\nself, input_image_path: str, enhanced_image_path: str, apply_crop: bool\n) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n\"\"\"\n        Mapping function for `tf.data.Dataset`. Loads the image from file path,\n        applies `random_crop` based on a boolean flag.\n        Args:\n            input_image_path (str): The file path for low light image.\n            enhanced_image_path (str): The file path for enhanced image.\n            apply_crop (bool): Boolean flag to condition random cropping.\n        Returns:\n            (Tuple[tf.Tensor, tf.Tensor]): A tuple of preprocessed image tensors corresponding\n                to the input and ground-truth images.\n        \"\"\"\n# Read the image off the file path.\ninput_image = read_image(input_image_path, self.normalization_factor)\nenhanced_image = read_image(enhanced_image_path, self.normalization_factor)\n# Apply random cropping based on the boolean flag.\ninput_image, enhanced_image = (\nself.random_crop(input_image, enhanced_image)\nif apply_crop\nelse self.resize(input_image, enhanced_image)\n)\nreturn input_image, enhanced_image\ndef build_dataset(\nself,\ninput_images: List[str],\nenhanced_images: List[str],\nbatch_size: int,\napply_crop: bool,\napply_augmentations: bool,\n) -&gt; tf.data.Dataset:\n\"\"\"\n        Function to build a prefetched `tf.data.Dataset``.\n        Args:\n            input_images (List[str]): A list of image filenames.\n            enhanced_images (List[str]): A list of image filenames.\n            batch_size (int): Number of images in a single batch.\n            apply_crop (bool): Boolean flag to condition cropping.\n            apply_augmentations (bool): Boolean flag to condition augmentations.\n        \"\"\"\n# Build a `tf.data.Dataset` from the filenames.\ndataset = tf.data.Dataset.from_tensor_slices((input_images, enhanced_images))\n# Build the mapping function and apply it to the dataset.\nmap_fn = partial(self.load_image, apply_crop=apply_crop)\ndataset = dataset.map(\nmap_fn,\nnum_parallel_calls=_AUTOTUNE,\n)\n# Apply augmentations.\nif apply_augmentations:\ndataset = dataset.map(\nrandom_horiontal_flip,\nnum_parallel_calls=_AUTOTUNE,\n)\ndataset = dataset.map(\nrandom_vertical_flip,\nnum_parallel_calls=_AUTOTUNE,\n)\ndataset = dataset.batch(batch_size, drop_remainder=True)\nreturn dataset.prefetch(_AUTOTUNE)\ndef get_datasets(self, batch_size: int) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset]:\n\"\"\"\n        Function to retrieve the train and val dataset.\n        Args:\n            batch_size (int): Number of images in a single batch.\n        Returns:\n            (Tuple[tf.data.Dataset, tf.data.Dataset]): A tuple of `tf.data.Dataset` for\n                training and validation.\n        \"\"\"\ntrain_dataset = self.build_dataset(\ninput_images=self.train_input_images,\nenhanced_images=self.train_enhanced_images,\nbatch_size=batch_size,\napply_crop=True,\napply_augmentations=True,\n)\nval_dataset = self.build_dataset(\ninput_images=self.val_input_images,\nenhanced_images=self.val_enhanced_images,\nbatch_size=batch_size,\napply_crop=False,\napply_augmentations=False,\n)\nreturn train_dataset, val_dataset\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_dataloader.DatasetFactory.build_dataset","title":"<code>build_dataset(input_images, enhanced_images, batch_size, apply_crop, apply_augmentations)</code>","text":"<p>Function to build a prefetched `tf.data.Dataset``.</p> <p>Parameters:</p> Name Type Description Default <code>input_images</code> <code>List[str]</code> <p>A list of image filenames.</p> required <code>enhanced_images</code> <code>List[str]</code> <p>A list of image filenames.</p> required <code>batch_size</code> <code>int</code> <p>Number of images in a single batch.</p> required <code>apply_crop</code> <code>bool</code> <p>Boolean flag to condition cropping.</p> required <code>apply_augmentations</code> <code>bool</code> <p>Boolean flag to condition augmentations.</p> required Source code in <code>restorers/dataloader/base/base_dataloader.py</code> <pre><code>def build_dataset(\nself,\ninput_images: List[str],\nenhanced_images: List[str],\nbatch_size: int,\napply_crop: bool,\napply_augmentations: bool,\n) -&gt; tf.data.Dataset:\n\"\"\"\n    Function to build a prefetched `tf.data.Dataset``.\n    Args:\n        input_images (List[str]): A list of image filenames.\n        enhanced_images (List[str]): A list of image filenames.\n        batch_size (int): Number of images in a single batch.\n        apply_crop (bool): Boolean flag to condition cropping.\n        apply_augmentations (bool): Boolean flag to condition augmentations.\n    \"\"\"\n# Build a `tf.data.Dataset` from the filenames.\ndataset = tf.data.Dataset.from_tensor_slices((input_images, enhanced_images))\n# Build the mapping function and apply it to the dataset.\nmap_fn = partial(self.load_image, apply_crop=apply_crop)\ndataset = dataset.map(\nmap_fn,\nnum_parallel_calls=_AUTOTUNE,\n)\n# Apply augmentations.\nif apply_augmentations:\ndataset = dataset.map(\nrandom_horiontal_flip,\nnum_parallel_calls=_AUTOTUNE,\n)\ndataset = dataset.map(\nrandom_vertical_flip,\nnum_parallel_calls=_AUTOTUNE,\n)\ndataset = dataset.batch(batch_size, drop_remainder=True)\nreturn dataset.prefetch(_AUTOTUNE)\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_dataloader.DatasetFactory.get_datasets","title":"<code>get_datasets(batch_size)</code>","text":"<p>Function to retrieve the train and val dataset.</p> <p>Parameters:</p> Name Type Description Default <code>batch_size</code> <code>int</code> <p>Number of images in a single batch.</p> required <p>Returns:</p> Type Description <code>Tuple[tf.data.Dataset, tf.data.Dataset]</code> <p>A tuple of <code>tf.data.Dataset</code> for training and validation.</p> Source code in <code>restorers/dataloader/base/base_dataloader.py</code> <pre><code>def get_datasets(self, batch_size: int) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset]:\n\"\"\"\n    Function to retrieve the train and val dataset.\n    Args:\n        batch_size (int): Number of images in a single batch.\n    Returns:\n        (Tuple[tf.data.Dataset, tf.data.Dataset]): A tuple of `tf.data.Dataset` for\n            training and validation.\n    \"\"\"\ntrain_dataset = self.build_dataset(\ninput_images=self.train_input_images,\nenhanced_images=self.train_enhanced_images,\nbatch_size=batch_size,\napply_crop=True,\napply_augmentations=True,\n)\nval_dataset = self.build_dataset(\ninput_images=self.val_input_images,\nenhanced_images=self.val_enhanced_images,\nbatch_size=batch_size,\napply_crop=False,\napply_augmentations=False,\n)\nreturn train_dataset, val_dataset\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_dataloader.DatasetFactory.load_image","title":"<code>load_image(input_image_path, enhanced_image_path, apply_crop)</code>","text":"<p>Mapping function for <code>tf.data.Dataset</code>. Loads the image from file path, applies <code>random_crop</code> based on a boolean flag.</p> <p>Parameters:</p> Name Type Description Default <code>input_image_path</code> <code>str</code> <p>The file path for low light image.</p> required <code>enhanced_image_path</code> <code>str</code> <p>The file path for enhanced image.</p> required <code>apply_crop</code> <code>bool</code> <p>Boolean flag to condition random cropping.</p> required <p>Returns:</p> Type Description <code>Tuple[tf.Tensor, tf.Tensor]</code> <p>A tuple of preprocessed image tensors corresponding to the input and ground-truth images.</p> Source code in <code>restorers/dataloader/base/base_dataloader.py</code> <pre><code>def load_image(\nself, input_image_path: str, enhanced_image_path: str, apply_crop: bool\n) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n\"\"\"\n    Mapping function for `tf.data.Dataset`. Loads the image from file path,\n    applies `random_crop` based on a boolean flag.\n    Args:\n        input_image_path (str): The file path for low light image.\n        enhanced_image_path (str): The file path for enhanced image.\n        apply_crop (bool): Boolean flag to condition random cropping.\n    Returns:\n        (Tuple[tf.Tensor, tf.Tensor]): A tuple of preprocessed image tensors corresponding\n            to the input and ground-truth images.\n    \"\"\"\n# Read the image off the file path.\ninput_image = read_image(input_image_path, self.normalization_factor)\nenhanced_image = read_image(enhanced_image_path, self.normalization_factor)\n# Apply random cropping based on the boolean flag.\ninput_image, enhanced_image = (\nself.random_crop(input_image, enhanced_image)\nif apply_crop\nelse self.resize(input_image, enhanced_image)\n)\nreturn input_image, enhanced_image\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_dataloader.DatasetFactory.random_crop","title":"<code>random_crop(input_image, enhanced_image)</code>","text":"<p>Function to apply random cropping.</p> <p>Parameters:</p> Name Type Description Default <code>input_image</code> <code>`tf.Tensor`</code> <p>Low light image.</p> required <code>enhanced_image</code> <code>`tf.Tensor`</code> <p>Enhanced image.</p> required <p>Returns:</p> Type Description <code>Tuple[tf.Tensor, tf.Tensor]</code> <p>A tuple of random cropped image.</p> Source code in <code>restorers/dataloader/base/base_dataloader.py</code> <pre><code>def random_crop(\nself, input_image: tf.Tensor, enhanced_image: tf.Tensor\n) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n\"\"\"\n    Function to apply random cropping.\n    Args:\n        input_image (`tf.Tensor`): Low light image.\n        enhanced_image (`tf.Tensor`): Enhanced image.\n    Returns:\n        (Tuple[tf.Tensor, tf.Tensor]): A tuple of random cropped image.\n    \"\"\"\n# Check whether the image size is smaller than the original image\nimage_size = tf.minimum(self.image_size, tf.shape(input_image)[0])\n# Concatenate the low light and enhanced image\nconcatenated_image = tf.concat([input_image, enhanced_image], axis=-1)\n# Apply same *random* crop to the concantenated image and split the stack\ncropped_concatenated_image = tf.image.random_crop(\nconcatenated_image, (image_size, image_size, 6)\n)\ncropped_input_image, cropped_enhanced_image = tf.split(\ncropped_concatenated_image, num_or_size_splits=2, axis=-1\n)\n# Ensuring the dataset tensor_spec is not None is the spatial dimensions\ncropped_input_image.set_shape([self.image_size, self.image_size, 3])\ncropped_enhanced_image.set_shape([self.image_size, self.image_size, 3])\nreturn cropped_input_image, cropped_enhanced_image\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_dataloader.DatasetFactory.resize","title":"<code>resize(input_image, enhanced_image)</code>","text":"<p>Function to resize images.</p> <p>Parameters:</p> Name Type Description Default <code>input_image</code> <code>`tf.Tensor`</code> <p>Low light image.</p> required <code>enhanced_image</code> <code>`tf.Tensor`</code> <p>Enhanced image.</p> required <p>Returns:</p> Type Description <code>Tuple[tf.Tensor, tf.Tensor]</code> <p>A tuple of tf.Tensor resized images.</p> Source code in <code>restorers/dataloader/base/base_dataloader.py</code> <pre><code>def resize(\nself, input_image: tf.Tensor, enhanced_image: tf.Tensor\n) -&gt; Tuple[tf.Tensor, tf.Tensor]:\n\"\"\"\n    Function to resize images.\n    Args:\n        input_image (`tf.Tensor`): Low light image.\n        enhanced_image (`tf.Tensor`): Enhanced image.\n    Returns:\n        (Tuple[tf.Tensor, tf.Tensor]): A tuple of tf.Tensor resized images.\n    \"\"\"\n# Check whether the image size is smaller than the original image\nimage_size = tf.minimum(self.image_size, tf.shape(input_image)[0])\ninput_image = tf.image.resize(\ninput_image,\nsize=[image_size, image_size],\n)\nenhanced_image = tf.image.resize(\nenhanced_image,\nsize=[image_size, image_size],\n)\n# Ensuring the dataset tensor_spec is not None is the spatial dimensions\ninput_image.set_shape([self.image_size, self.image_size, 3])\nenhanced_image.set_shape([self.image_size, self.image_size, 3])\nreturn input_image, enhanced_image\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_low_light_dataloader.LowLightDatasetFactory","title":"<code>LowLightDatasetFactory</code>","text":"<p>         Bases: <code>DatasetFactory</code></p> <p>Abstract base class for building dataset factories or dataloaders for supervised low-light enhancement.</p> <p>Abstract functions to be overriden are:</p> <ul> <li><code>define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None</code><ul> <li>Function to define the structure of the dataset.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>The image resolution.</p> required <code>bit_depth</code> <code>int</code> <p>Bit depth of the images for normalization.</p> required <code>val_split</code> <code>float</code> <p>The percentage of validation split.</p> required <code>visualize_on_wandb</code> <code>bool</code> <p>Flag to visualize the dataset on wandb.</p> required <code>dataset_artifact_address</code> <code>Union[str, None]</code> <p>The address of the dataset artifact on Weights &amp; Biases.</p> <code>None</code> <code>dataset_url</code> <code>Union[str, None]</code> <p>The URL of the dataset hosted on the web. This is not necessary in case <code>dataset_artifact_address</code> has been specified.</p> <code>None</code> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>class LowLightDatasetFactory(DatasetFactory):\n\"\"\"Abstract base class for building dataset factories or dataloaders for\n    supervised low-light enhancement.\n    Abstract functions to be overriden are:\n    - `define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None`\n        - Function to define the structure of the dataset.\n    Args:\n        image_size (int): The image resolution.\n        bit_depth (int): Bit depth of the images for normalization.\n        val_split (float): The percentage of validation split.\n        visualize_on_wandb (bool): Flag to visualize the dataset on wandb.\n        dataset_artifact_address (Union[str, None]): The address of the dataset artifact on\n            Weights &amp; Biases.\n        dataset_url (Union[str, None]): The URL of the dataset hosted on the web. This is not necessary\n            in case `dataset_artifact_address` has been specified.\n    \"\"\"\ndef __init__(\nself,\nimage_size: int,\nbit_depth: int,\nval_split: float,\nvisualize_on_wandb: bool,\ndataset_artifact_address: Union[str, None] = None,\ndataset_url: Union[str, None] = None,\n):\nif visualize_on_wandb:\nself.table = wandb.Table(\ncolumns=[\"Image-ID\", \"Split\", \"Low-Light-Image\", \"Ground-Truth-Image\"]\n)\nself.dataset_url = dataset_url\nself.dataset_artifact_address = dataset_artifact_address\nsuper().__init__(image_size, bit_depth, val_split, visualize_on_wandb)\n@abstractmethod\ndef define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None:\n\"\"\"Abstract function to define the structure of the dataset.\"\"\"\nraise NotImplementedError(\nf\"{self.__class__.__name__ }.define_dataset_structure\"\n)\ndef __len__(self):\nreturn self.num_data_points\ndef _create_data_table(self, low_light_images, enhanced_images, split):\nfor idx in tqdm(\nrange(len(low_light_images)),\ndesc=f\"Generating visualizations for {split} images\",\n):\nself.table.add_data(\nint(low_light_images[idx].split(\"/\")[-1][:-4]),\nsplit,\nwandb.Image(Image.open(low_light_images[idx])),\nwandb.Image(Image.open(enhanced_images[idx])),\n)\ndef sanity_tests(self):\n\"\"\"This function is used to visualize the dataset on Weights &amp; Biases and enable\n        interactive exploratory analysis.\n        \"\"\"\ntry:\nself._create_data_table(\nself.train_input_images, self.train_enhanced_images, split=\"Train\"\n)\nexcept:\nlogging.warning(\"Train Set not found.\")\ntry:\nself._create_data_table(\nself.val_input_images, self.val_enhanced_images, split=\"Validation\"\n)\nexcept:\nlogging.warning(\"Validation Set not found.\")\ntry:\nself._create_data_table(\nself.test_low_light_images, self.test_enhanced_images, split=\"Test\"\n)\nexcept:\nlogging.warning(\"Test Set not found.\")\nwandb.log({f\"Lol-Dataset\": self.table})\ndef fetch_dataset(self, val_split, visualize_on_wandb: bool):\n\"\"\"Function to fetch the dataset from a URL or a Weights &amp; Biases dataset artifact.\n        This function also executes the sanity tests.\n        \"\"\"\nif self.dataset_url is not None:\ndataset_path = tf.keras.utils.get_file(\nfname=\"lol_dataset.zip\",\norigin=self.dataset_url,\nextract=True,\narchive_format=\"zip\",\n).split(\".zip\")[0]\nelif self.dataset_artifact_address is not None:\ndataset_path = fetch_wandb_artifact(\nself.dataset_artifact_address, artifact_type=\"dataset\"\n)\nelse:\nraise ValueError(\n\"Both dataset_url and dataset_artifact_address cannot be None\"\n)\nself.define_dataset_structure(dataset_path=dataset_path, val_split=val_split)\nif visualize_on_wandb and wandb.run is not None:\nself.sanity_tests()\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_low_light_dataloader.LowLightDatasetFactory.define_dataset_structure","title":"<code>define_dataset_structure(dataset_path, val_split)</code>  <code>abstractmethod</code>","text":"<p>Abstract function to define the structure of the dataset.</p> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>@abstractmethod\ndef define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None:\n\"\"\"Abstract function to define the structure of the dataset.\"\"\"\nraise NotImplementedError(\nf\"{self.__class__.__name__ }.define_dataset_structure\"\n)\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_low_light_dataloader.LowLightDatasetFactory.fetch_dataset","title":"<code>fetch_dataset(val_split, visualize_on_wandb)</code>","text":"<p>Function to fetch the dataset from a URL or a Weights &amp; Biases dataset artifact. This function also executes the sanity tests.</p> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>def fetch_dataset(self, val_split, visualize_on_wandb: bool):\n\"\"\"Function to fetch the dataset from a URL or a Weights &amp; Biases dataset artifact.\n    This function also executes the sanity tests.\n    \"\"\"\nif self.dataset_url is not None:\ndataset_path = tf.keras.utils.get_file(\nfname=\"lol_dataset.zip\",\norigin=self.dataset_url,\nextract=True,\narchive_format=\"zip\",\n).split(\".zip\")[0]\nelif self.dataset_artifact_address is not None:\ndataset_path = fetch_wandb_artifact(\nself.dataset_artifact_address, artifact_type=\"dataset\"\n)\nelse:\nraise ValueError(\n\"Both dataset_url and dataset_artifact_address cannot be None\"\n)\nself.define_dataset_structure(dataset_path=dataset_path, val_split=val_split)\nif visualize_on_wandb and wandb.run is not None:\nself.sanity_tests()\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_low_light_dataloader.LowLightDatasetFactory.sanity_tests","title":"<code>sanity_tests()</code>","text":"<p>This function is used to visualize the dataset on Weights &amp; Biases and enable interactive exploratory analysis.</p> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>def sanity_tests(self):\n\"\"\"This function is used to visualize the dataset on Weights &amp; Biases and enable\n    interactive exploratory analysis.\n    \"\"\"\ntry:\nself._create_data_table(\nself.train_input_images, self.train_enhanced_images, split=\"Train\"\n)\nexcept:\nlogging.warning(\"Train Set not found.\")\ntry:\nself._create_data_table(\nself.val_input_images, self.val_enhanced_images, split=\"Validation\"\n)\nexcept:\nlogging.warning(\"Validation Set not found.\")\ntry:\nself._create_data_table(\nself.test_low_light_images, self.test_enhanced_images, split=\"Test\"\n)\nexcept:\nlogging.warning(\"Test Set not found.\")\nwandb.log({f\"Lol-Dataset\": self.table})\n</code></pre>"},{"location":"dataloader/base/#restorers.dataloader.base.base_low_light_dataloader.UnsupervisedLowLightDatasetFactory","title":"<code>UnsupervisedLowLightDatasetFactory</code>","text":"<p>         Bases: <code>LowLightDatasetFactory</code></p> <p>Abstract base class for building dataset factories or dataloaders for unsupervised low-light enhancement.</p> <p>Abstract functions to be overriden are:</p> <ul> <li><code>define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None</code><ul> <li>Function to define the structure of the dataset.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>The image resolution.</p> required <code>bit_depth</code> <code>int</code> <p>Bit depth of the images for normalization.</p> required <code>val_split</code> <code>float</code> <p>The percentage of validation split.</p> required <code>visualize_on_wandb</code> <code>bool</code> <p>Flag to visualize the dataset on wandb.</p> required <code>dataset_artifact_address</code> <code>Union[str, None]</code> <p>The address of the dataset artifact on Weights &amp; Biases.</p> <code>None</code> <code>dataset_url</code> <code>Union[str, None]</code> <p>The URL of the dataset hosted on the web. This is not necessary in case <code>dataset_artifact_address</code> has been specified.</p> <code>None</code> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>class UnsupervisedLowLightDatasetFactory(LowLightDatasetFactory):\n\"\"\"Abstract base class for building dataset factories or dataloaders for\n    unsupervised low-light enhancement.\n    Abstract functions to be overriden are:\n    - `define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None`\n        - Function to define the structure of the dataset.\n    Args:\n        image_size (int): The image resolution.\n        bit_depth (int): Bit depth of the images for normalization.\n        val_split (float): The percentage of validation split.\n        visualize_on_wandb (bool): Flag to visualize the dataset on wandb.\n        dataset_artifact_address (Union[str, None]): The address of the dataset artifact on\n            Weights &amp; Biases.\n        dataset_url (Union[str, None]): The URL of the dataset hosted on the web. This is not necessary\n            in case `dataset_artifact_address` has been specified.\n    \"\"\"\ndef __init__(\nself,\nimage_size: int,\nbit_depth: int,\nval_split: float,\nvisualize_on_wandb: bool,\ndataset_artifact_address: Union[str, None] = None,\ndataset_url: Union[str, None] = None,\n):\nsuper().__init__(\nimage_size,\nbit_depth,\nval_split,\nvisualize_on_wandb,\ndataset_artifact_address,\ndataset_url,\n)\ndef random_crop(self, input_image: tf.Tensor) -&gt; Tuple[tf.Tensor]:\n# Check whether the image size is smaller than the original image\nimage_size = tf.minimum(self.image_size, tf.shape(input_image)[0])\n# Apply same *random* crop to the concantenated image and split the stack\ncropped_input_image = tf.image.random_crop(\ninput_image, (image_size, image_size, 3)\n)\n# Ensuring the dataset tensor_spec is not None is the spatial dimensions\ncropped_input_image.set_shape([self.image_size, self.image_size, 3])\nreturn cropped_input_image\ndef resize(self, input_image: tf.Tensor) -&gt; Tuple[tf.Tensor]:\n# Check whether the image size is smaller than the original image\nimage_size = tf.minimum(self.image_size, tf.shape(input_image)[0])\nresized_input_image = tf.image.resize(\ninput_image,\nsize=[image_size, image_size],\n)\n# Ensuring the dataset tensor_spec is not None is the spatial dimensions\nresized_input_image.set_shape([self.image_size, self.image_size, 3])\nreturn resized_input_image\ndef load_image(self, input_image_path: str, apply_crop: bool):\n# Read the image off the file path.\ninput_image = read_image(input_image_path, self.normalization_factor)\n# Apply random cropping based on the boolean flag.\nreturn self.random_crop(input_image) if apply_crop else self.resize(input_image)\ndef build_dataset(\nself,\ninput_images: List[str],\nbatch_size: int,\napply_crop: bool,\napply_augmentations: bool,\n) -&gt; tf.data.Dataset:\n# Build a `tf.data.Dataset` from the filenames.\ndataset = tf.data.Dataset.from_tensor_slices(input_images)\n# Build the mapping function and apply it to the dataset.\nmap_fn = partial(self.load_image, apply_crop=apply_crop)\ndataset = dataset.map(\nmap_fn,\nnum_parallel_calls=_AUTOTUNE,\n)\n# Apply augmentations.\nif apply_augmentations:\ndataset = dataset.map(\nrandom_unpaired_horiontal_flip,\nnum_parallel_calls=_AUTOTUNE,\n)\ndataset = dataset.map(\nrandom_unpaired_vertical_flip,\nnum_parallel_calls=_AUTOTUNE,\n)\ndataset = dataset.batch(batch_size, drop_remainder=True)\nreturn dataset.prefetch(_AUTOTUNE)\ndef get_datasets(self, batch_size: int) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset]:\ntrain_dataset = self.build_dataset(\ninput_images=self.train_input_images,\nbatch_size=batch_size,\napply_crop=True,\napply_augmentations=True,\n)\nval_dataset = self.build_dataset(\ninput_images=self.val_input_images,\nbatch_size=batch_size,\napply_crop=False,\napply_augmentations=False,\n)\nreturn train_dataset, val_dataset\n</code></pre>"},{"location":"dataloader/base_low_light/","title":"Base Low-light Dataset Factory","text":""},{"location":"dataloader/base_low_light/#restorers.dataloader.base.base_low_light_dataloader.LowLightDatasetFactory","title":"<code>LowLightDatasetFactory</code>","text":"<p>         Bases: <code>DatasetFactory</code></p> <p>Abstract base class for building dataset factories or dataloaders for supervised low-light enhancement.</p> <p>Abstract functions to be overriden are:</p> <ul> <li><code>define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None</code><ul> <li>Function to define the structure of the dataset.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>The image resolution.</p> required <code>bit_depth</code> <code>int</code> <p>Bit depth of the images for normalization.</p> required <code>val_split</code> <code>float</code> <p>The percentage of validation split.</p> required <code>visualize_on_wandb</code> <code>bool</code> <p>Flag to visualize the dataset on wandb.</p> required <code>dataset_artifact_address</code> <code>Union[str, None]</code> <p>The address of the dataset artifact on Weights &amp; Biases.</p> <code>None</code> <code>dataset_url</code> <code>Union[str, None]</code> <p>The URL of the dataset hosted on the web. This is not necessary in case <code>dataset_artifact_address</code> has been specified.</p> <code>None</code> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>class LowLightDatasetFactory(DatasetFactory):\n\"\"\"Abstract base class for building dataset factories or dataloaders for\n    supervised low-light enhancement.\n    Abstract functions to be overriden are:\n    - `define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None`\n        - Function to define the structure of the dataset.\n    Args:\n        image_size (int): The image resolution.\n        bit_depth (int): Bit depth of the images for normalization.\n        val_split (float): The percentage of validation split.\n        visualize_on_wandb (bool): Flag to visualize the dataset on wandb.\n        dataset_artifact_address (Union[str, None]): The address of the dataset artifact on\n            Weights &amp; Biases.\n        dataset_url (Union[str, None]): The URL of the dataset hosted on the web. This is not necessary\n            in case `dataset_artifact_address` has been specified.\n    \"\"\"\ndef __init__(\nself,\nimage_size: int,\nbit_depth: int,\nval_split: float,\nvisualize_on_wandb: bool,\ndataset_artifact_address: Union[str, None] = None,\ndataset_url: Union[str, None] = None,\n):\nif visualize_on_wandb:\nself.table = wandb.Table(\ncolumns=[\"Image-ID\", \"Split\", \"Low-Light-Image\", \"Ground-Truth-Image\"]\n)\nself.dataset_url = dataset_url\nself.dataset_artifact_address = dataset_artifact_address\nsuper().__init__(image_size, bit_depth, val_split, visualize_on_wandb)\n@abstractmethod\ndef define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None:\n\"\"\"Abstract function to define the structure of the dataset.\"\"\"\nraise NotImplementedError(\nf\"{self.__class__.__name__ }.define_dataset_structure\"\n)\ndef __len__(self):\nreturn self.num_data_points\ndef _create_data_table(self, low_light_images, enhanced_images, split):\nfor idx in tqdm(\nrange(len(low_light_images)),\ndesc=f\"Generating visualizations for {split} images\",\n):\nself.table.add_data(\nint(low_light_images[idx].split(\"/\")[-1][:-4]),\nsplit,\nwandb.Image(Image.open(low_light_images[idx])),\nwandb.Image(Image.open(enhanced_images[idx])),\n)\ndef sanity_tests(self):\n\"\"\"This function is used to visualize the dataset on Weights &amp; Biases and enable\n        interactive exploratory analysis.\n        \"\"\"\ntry:\nself._create_data_table(\nself.train_input_images, self.train_enhanced_images, split=\"Train\"\n)\nexcept:\nlogging.warning(\"Train Set not found.\")\ntry:\nself._create_data_table(\nself.val_input_images, self.val_enhanced_images, split=\"Validation\"\n)\nexcept:\nlogging.warning(\"Validation Set not found.\")\ntry:\nself._create_data_table(\nself.test_low_light_images, self.test_enhanced_images, split=\"Test\"\n)\nexcept:\nlogging.warning(\"Test Set not found.\")\nwandb.log({f\"Lol-Dataset\": self.table})\ndef fetch_dataset(self, val_split, visualize_on_wandb: bool):\n\"\"\"Function to fetch the dataset from a URL or a Weights &amp; Biases dataset artifact.\n        This function also executes the sanity tests.\n        \"\"\"\nif self.dataset_url is not None:\ndataset_path = tf.keras.utils.get_file(\nfname=\"lol_dataset.zip\",\norigin=self.dataset_url,\nextract=True,\narchive_format=\"zip\",\n).split(\".zip\")[0]\nelif self.dataset_artifact_address is not None:\ndataset_path = fetch_wandb_artifact(\nself.dataset_artifact_address, artifact_type=\"dataset\"\n)\nelse:\nraise ValueError(\n\"Both dataset_url and dataset_artifact_address cannot be None\"\n)\nself.define_dataset_structure(dataset_path=dataset_path, val_split=val_split)\nif visualize_on_wandb and wandb.run is not None:\nself.sanity_tests()\n</code></pre>"},{"location":"dataloader/base_low_light/#restorers.dataloader.base.base_low_light_dataloader.LowLightDatasetFactory.define_dataset_structure","title":"<code>define_dataset_structure(dataset_path, val_split)</code>  <code>abstractmethod</code>","text":"<p>Abstract function to define the structure of the dataset.</p> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>@abstractmethod\ndef define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None:\n\"\"\"Abstract function to define the structure of the dataset.\"\"\"\nraise NotImplementedError(\nf\"{self.__class__.__name__ }.define_dataset_structure\"\n)\n</code></pre>"},{"location":"dataloader/base_low_light/#restorers.dataloader.base.base_low_light_dataloader.LowLightDatasetFactory.fetch_dataset","title":"<code>fetch_dataset(val_split, visualize_on_wandb)</code>","text":"<p>Function to fetch the dataset from a URL or a Weights &amp; Biases dataset artifact. This function also executes the sanity tests.</p> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>def fetch_dataset(self, val_split, visualize_on_wandb: bool):\n\"\"\"Function to fetch the dataset from a URL or a Weights &amp; Biases dataset artifact.\n    This function also executes the sanity tests.\n    \"\"\"\nif self.dataset_url is not None:\ndataset_path = tf.keras.utils.get_file(\nfname=\"lol_dataset.zip\",\norigin=self.dataset_url,\nextract=True,\narchive_format=\"zip\",\n).split(\".zip\")[0]\nelif self.dataset_artifact_address is not None:\ndataset_path = fetch_wandb_artifact(\nself.dataset_artifact_address, artifact_type=\"dataset\"\n)\nelse:\nraise ValueError(\n\"Both dataset_url and dataset_artifact_address cannot be None\"\n)\nself.define_dataset_structure(dataset_path=dataset_path, val_split=val_split)\nif visualize_on_wandb and wandb.run is not None:\nself.sanity_tests()\n</code></pre>"},{"location":"dataloader/base_low_light/#restorers.dataloader.base.base_low_light_dataloader.LowLightDatasetFactory.sanity_tests","title":"<code>sanity_tests()</code>","text":"<p>This function is used to visualize the dataset on Weights &amp; Biases and enable interactive exploratory analysis.</p> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>def sanity_tests(self):\n\"\"\"This function is used to visualize the dataset on Weights &amp; Biases and enable\n    interactive exploratory analysis.\n    \"\"\"\ntry:\nself._create_data_table(\nself.train_input_images, self.train_enhanced_images, split=\"Train\"\n)\nexcept:\nlogging.warning(\"Train Set not found.\")\ntry:\nself._create_data_table(\nself.val_input_images, self.val_enhanced_images, split=\"Validation\"\n)\nexcept:\nlogging.warning(\"Validation Set not found.\")\ntry:\nself._create_data_table(\nself.test_low_light_images, self.test_enhanced_images, split=\"Test\"\n)\nexcept:\nlogging.warning(\"Test Set not found.\")\nwandb.log({f\"Lol-Dataset\": self.table})\n</code></pre>"},{"location":"dataloader/base_low_light/#restorers.dataloader.base.base_low_light_dataloader.UnsupervisedLowLightDatasetFactory","title":"<code>UnsupervisedLowLightDatasetFactory</code>","text":"<p>         Bases: <code>LowLightDatasetFactory</code></p> <p>Abstract base class for building dataset factories or dataloaders for unsupervised low-light enhancement.</p> <p>Abstract functions to be overriden are:</p> <ul> <li><code>define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None</code><ul> <li>Function to define the structure of the dataset.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>The image resolution.</p> required <code>bit_depth</code> <code>int</code> <p>Bit depth of the images for normalization.</p> required <code>val_split</code> <code>float</code> <p>The percentage of validation split.</p> required <code>visualize_on_wandb</code> <code>bool</code> <p>Flag to visualize the dataset on wandb.</p> required <code>dataset_artifact_address</code> <code>Union[str, None]</code> <p>The address of the dataset artifact on Weights &amp; Biases.</p> <code>None</code> <code>dataset_url</code> <code>Union[str, None]</code> <p>The URL of the dataset hosted on the web. This is not necessary in case <code>dataset_artifact_address</code> has been specified.</p> <code>None</code> Source code in <code>restorers/dataloader/base/base_low_light_dataloader.py</code> <pre><code>class UnsupervisedLowLightDatasetFactory(LowLightDatasetFactory):\n\"\"\"Abstract base class for building dataset factories or dataloaders for\n    unsupervised low-light enhancement.\n    Abstract functions to be overriden are:\n    - `define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None`\n        - Function to define the structure of the dataset.\n    Args:\n        image_size (int): The image resolution.\n        bit_depth (int): Bit depth of the images for normalization.\n        val_split (float): The percentage of validation split.\n        visualize_on_wandb (bool): Flag to visualize the dataset on wandb.\n        dataset_artifact_address (Union[str, None]): The address of the dataset artifact on\n            Weights &amp; Biases.\n        dataset_url (Union[str, None]): The URL of the dataset hosted on the web. This is not necessary\n            in case `dataset_artifact_address` has been specified.\n    \"\"\"\ndef __init__(\nself,\nimage_size: int,\nbit_depth: int,\nval_split: float,\nvisualize_on_wandb: bool,\ndataset_artifact_address: Union[str, None] = None,\ndataset_url: Union[str, None] = None,\n):\nsuper().__init__(\nimage_size,\nbit_depth,\nval_split,\nvisualize_on_wandb,\ndataset_artifact_address,\ndataset_url,\n)\ndef random_crop(self, input_image: tf.Tensor) -&gt; Tuple[tf.Tensor]:\n# Check whether the image size is smaller than the original image\nimage_size = tf.minimum(self.image_size, tf.shape(input_image)[0])\n# Apply same *random* crop to the concantenated image and split the stack\ncropped_input_image = tf.image.random_crop(\ninput_image, (image_size, image_size, 3)\n)\n# Ensuring the dataset tensor_spec is not None is the spatial dimensions\ncropped_input_image.set_shape([self.image_size, self.image_size, 3])\nreturn cropped_input_image\ndef resize(self, input_image: tf.Tensor) -&gt; Tuple[tf.Tensor]:\n# Check whether the image size is smaller than the original image\nimage_size = tf.minimum(self.image_size, tf.shape(input_image)[0])\nresized_input_image = tf.image.resize(\ninput_image,\nsize=[image_size, image_size],\n)\n# Ensuring the dataset tensor_spec is not None is the spatial dimensions\nresized_input_image.set_shape([self.image_size, self.image_size, 3])\nreturn resized_input_image\ndef load_image(self, input_image_path: str, apply_crop: bool):\n# Read the image off the file path.\ninput_image = read_image(input_image_path, self.normalization_factor)\n# Apply random cropping based on the boolean flag.\nreturn self.random_crop(input_image) if apply_crop else self.resize(input_image)\ndef build_dataset(\nself,\ninput_images: List[str],\nbatch_size: int,\napply_crop: bool,\napply_augmentations: bool,\n) -&gt; tf.data.Dataset:\n# Build a `tf.data.Dataset` from the filenames.\ndataset = tf.data.Dataset.from_tensor_slices(input_images)\n# Build the mapping function and apply it to the dataset.\nmap_fn = partial(self.load_image, apply_crop=apply_crop)\ndataset = dataset.map(\nmap_fn,\nnum_parallel_calls=_AUTOTUNE,\n)\n# Apply augmentations.\nif apply_augmentations:\ndataset = dataset.map(\nrandom_unpaired_horiontal_flip,\nnum_parallel_calls=_AUTOTUNE,\n)\ndataset = dataset.map(\nrandom_unpaired_vertical_flip,\nnum_parallel_calls=_AUTOTUNE,\n)\ndataset = dataset.batch(batch_size, drop_remainder=True)\nreturn dataset.prefetch(_AUTOTUNE)\ndef get_datasets(self, batch_size: int) -&gt; Tuple[tf.data.Dataset, tf.data.Dataset]:\ntrain_dataset = self.build_dataset(\ninput_images=self.train_input_images,\nbatch_size=batch_size,\napply_crop=True,\napply_augmentations=True,\n)\nval_dataset = self.build_dataset(\ninput_images=self.val_input_images,\nbatch_size=batch_size,\napply_crop=False,\napply_augmentations=False,\n)\nreturn train_dataset, val_dataset\n</code></pre>"},{"location":"dataloader/lol_dataloader/","title":"LOL Dataloader","text":"<p>The LOL dataset is composed of <code>500</code> low-light and normal-light image pairs and is divided into <code>485</code> training pairs and <code>15</code> testing pairs. The low-light images contain noise produced during the photo capture process. Most of the images are indoor scenes. All the images have a resolution of <code>400\u00d7600</code>. The dataset was introduced in the paper Deep Retinex Decomposition for Low-Light Enhancement.</p>"},{"location":"dataloader/lol_dataloader/#restorers.dataloader.lol_dataloader.LOLDataLoader","title":"<code>LOLDataLoader</code>","text":"<p>         Bases: <code>LowLightDatasetFactory</code></p> <p>DataLoader for the LOL dataset. This dataloader can be used to build datasets for training supervised low-light image enhancement models using the LOL Dataset.</p> <p>Usage:</p> <pre><code># define dataloader for the LoL dataset\ndata_loader = LOLDataLoader(\n# size of image crops on which we will train\nimage_size=128,\n# bit depth of the images\nbit_depth=8,\n# fraction of images for validation\nval_split=0.2,\n# visualize the dataset on WandB or not\nvisualize_on_wandb=True,\n# the wandb artifact address of the dataset,\n# this can be found from the `Usage` tab of\n# the aforemenioned weave panel\ndataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n)\n# call `get_datasets` on the `data_loader` to get\n# the TensorFlow datasets corresponding to the\n# training and validation splits\ndatasets = data_loader.get_datasets(batch_size=2)\n</code></pre> Examples <ul> <li>Training a supervised low-light enhancement model using MirNetv2.</li> <li>Training a supervised low-light enhancement model using NAFNet.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>The image resolution.</p> required <code>bit_depth</code> <code>int</code> <p>Bit depth of the images for normalization.</p> required <code>val_split</code> <code>float</code> <p>The percentage of validation split.</p> required <code>visualize_on_wandb</code> <code>bool</code> <p>Flag to visualize the dataset on wandb.</p> required <code>dataset_artifact_address</code> <code>Union[str, None]</code> <p>The address of the dataset artifact on Weights &amp; Biases.</p> <code>None</code> <code>dataset_url</code> <code>Union[str, None]</code> <p>The URL of the dataset hosted on the web. This is not necessary in case <code>dataset_artifact_address</code> has been specified.</p> <code>None</code> Source code in <code>restorers/dataloader/lol_dataloader.py</code> <pre><code>class LOLDataLoader(LowLightDatasetFactory):\n\"\"\"DataLoader for the [LOL dataset](https://www.kaggle.com/datasets/soumikrakshit/lol-dataset). This\n    dataloader can be used to build datasets for training supervised low-light image enhancement models\n    using the LOL Dataset.\n    Usage:\n    ```py\n    # define dataloader for the LoL dataset\n    data_loader = LOLDataLoader(\n        # size of image crops on which we will train\n        image_size=128,\n        # bit depth of the images\n        bit_depth=8,\n        # fraction of images for validation\n        val_split=0.2,\n        # visualize the dataset on WandB or not\n        visualize_on_wandb=True,\n        # the wandb artifact address of the dataset,\n        # this can be found from the `Usage` tab of\n        # the aforemenioned weave panel\n        dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n    )\n    # call `get_datasets` on the `data_loader` to get\n    # the TensorFlow datasets corresponding to the\n    # training and validation splits\n    datasets = data_loader.get_datasets(batch_size=2)\n    ```\n    ??? example \"Examples\"\n        - [Training a supervised low-light enhancement model using MirNetv2](../../examples/train_mirnetv2).\n        - [Training a supervised low-light enhancement model using NAFNet](../../examples/train_nafnet).\n    Args:\n        image_size (int): The image resolution.\n        bit_depth (int): Bit depth of the images for normalization.\n        val_split (float): The percentage of validation split.\n        visualize_on_wandb (bool): Flag to visualize the dataset on wandb.\n        dataset_artifact_address (Union[str, None]): The address of the dataset artifact on\n            Weights &amp; Biases.\n        dataset_url (Union[str, None]): The URL of the dataset hosted on the web. This is not necessary\n            in case `dataset_artifact_address` has been specified.\n    \"\"\"\ndef __init__(\nself,\nimage_size: int,\nbit_depth: int,\nval_split: float,\nvisualize_on_wandb: bool,\ndataset_artifact_address: Union[str, None] = None,\ndataset_url: Union[str, None] = None,\n):\nsuper().__init__(\nimage_size,\nbit_depth,\nval_split,\nvisualize_on_wandb,\ndataset_artifact_address,\ndataset_url,\n)\ndef define_dataset_structure(self, dataset_path, val_split):\nself.num_data_points, data_paths = _define_lol_dataset_structure(\ndataset_path, val_split\n)\nself.train_input_images, self.train_enhanced_images = data_paths[\"train\"]\nself.val_input_images, self.val_enhanced_images = data_paths[\"val\"]\nself.test_input_images, self.test_enhanced_images = data_paths[\"test\"]\n</code></pre>"},{"location":"dataloader/lol_dataloader/#restorers.dataloader.lol_dataloader.UnsupervisedLOLDataLoader","title":"<code>UnsupervisedLOLDataLoader</code>","text":"<p>         Bases: <code>UnsupervisedLowLightDatasetFactory</code></p> <p>Unsupervised dataLoader for the LOL dataset. This dataloader can be used to build datasets for training unsupervised low-light image enhancement models using the LOL Dataset. Usage: <pre><code># define unsupervised dataloader for the LoL dataset\ndata_loader = UnsupervisedLOLDataLoader(\n# size of image crops on which we will train\nimage_size=128,\n# bit depth of the images\nbit_depth=8,\n# fraction of images for validation\nval_split=0.2,\n# visualize the dataset on WandB or not\nvisualize_on_wandb=True,\n# the wandb artifact address of the dataset,\n# this can be found from the `Usage` tab of\n# the aforemenioned weave panel\ndataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n# train on all images including low-light and ground-truth or not\ntrain_on_all_images=True,\n)\n# call `get_datasets` on the `data_loader` to get\n# the TensorFlow datasets corresponding to the\n# training and validation splits\ndatasets = data_loader.get_datasets(batch_size=2)\n</code></pre></p> Examples <ul> <li>Training an unsupervised low-light enhancement model using Zero-DCE.</li> <li>Training an unsupervised low-light enhancement model using Fast Zero-DCE.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>image_size</code> <code>int</code> <p>The image resolution.</p> required <code>bit_depth</code> <code>int</code> <p>Bit depth of the images for normalization.</p> required <code>val_split</code> <code>float</code> <p>The percentage of validation split.</p> required <code>visualize_on_wandb</code> <code>bool</code> <p>Flag to visualize the dataset on wandb.</p> required <code>dataset_artifact_address</code> <code>Union[str, None]</code> <p>The address of the dataset artifact on Weights &amp; Biases.</p> <code>None</code> <code>dataset_url</code> <code>Union[str, None]</code> <p>The URL of the dataset hosted on the web. This is not necessary in case <code>dataset_artifact_address</code> has been specified.</p> <code>None</code> <code>train_on_all_images</code> <code>bool</code> <p>Use both input and ground-truth images for training or not.</p> <code>False</code> Source code in <code>restorers/dataloader/lol_dataloader.py</code> <pre><code>class UnsupervisedLOLDataLoader(UnsupervisedLowLightDatasetFactory):\n\"\"\"Unsupervised dataLoader for the [LOL dataset](https://www.kaggle.com/datasets/soumikrakshit/lol-dataset).\n    This dataloader can be used to build datasets for training unsupervised low-light image enhancement models\n    using the LOL Dataset.\n    Usage:\n    ```py\n    # define unsupervised dataloader for the LoL dataset\n    data_loader = UnsupervisedLOLDataLoader(\n        # size of image crops on which we will train\n        image_size=128,\n        # bit depth of the images\n        bit_depth=8,\n        # fraction of images for validation\n        val_split=0.2,\n        # visualize the dataset on WandB or not\n        visualize_on_wandb=True,\n        # the wandb artifact address of the dataset,\n        # this can be found from the `Usage` tab of\n        # the aforemenioned weave panel\n        dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n        # train on all images including low-light and ground-truth or not\n        train_on_all_images=True,\n    )\n    # call `get_datasets` on the `data_loader` to get\n    # the TensorFlow datasets corresponding to the\n    # training and validation splits\n    datasets = data_loader.get_datasets(batch_size=2)\n    ```\n    ??? example \"Examples\"\n        - [Training an unsupervised low-light enhancement model using Zero-DCE](../../examples/train_zero_dce).\n        - [Training an unsupervised low-light enhancement model using Fast Zero-DCE](../../examples/train_fast_zero_dce).\n    Args:\n        image_size (int): The image resolution.\n        bit_depth (int): Bit depth of the images for normalization.\n        val_split (float): The percentage of validation split.\n        visualize_on_wandb (bool): Flag to visualize the dataset on wandb.\n        dataset_artifact_address (Union[str, None]): The address of the dataset artifact on\n            Weights &amp; Biases.\n        dataset_url (Union[str, None]): The URL of the dataset hosted on the web. This is not necessary\n            in case `dataset_artifact_address` has been specified.\n        train_on_all_images (bool): Use both input and ground-truth images for training or not.\n    \"\"\"\ndef __init__(\nself,\nimage_size: int,\nbit_depth: int,\nval_split: float,\nvisualize_on_wandb: bool,\ndataset_artifact_address: Union[str, None] = None,\ndataset_url: Union[str, None] = None,\ntrain_on_all_images: bool = False,\n):\nself.train_on_all_images = train_on_all_images\nsuper().__init__(\nimage_size,\nbit_depth,\nval_split,\nvisualize_on_wandb,\ndataset_artifact_address,\ndataset_url,\n)\ndef define_dataset_structure(self, dataset_path: str, val_split: float) -&gt; None:\nself.num_data_points, data_paths = _define_lol_dataset_structure(\ndataset_path, val_split\n)\nself.train_input_images, self.train_enhanced_images = data_paths[\"train\"]\nself.val_input_images, self.val_enhanced_images = data_paths[\"val\"]\nself.test_input_images, self.test_enhanced_images = data_paths[\"test\"]\nif self.train_on_all_images:\nself.train_input_images = (\nself.train_input_images + self.train_enhanced_images\n)\nself.val_input_images = self.val_input_images + self.val_enhanced_images\nself.num_data_points *= 2\n</code></pre>"},{"location":"evaluation/base/","title":"Base Evaluator","text":""},{"location":"evaluation/base/#restorers.evaluation.base.BaseEvaluator","title":"<code>BaseEvaluator</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract base class for building Evaluators for different datasets/benchmarks.</p> <ul> <li>An evaluator not only evaluates a model on a dataset/benchmark, but also evaluates     its performance with respect to the specified metrics on individual images in that     dataset/benchmark as well.</li> <li>The evaluator also logs the number of parameters (both trainable and not-trainable)     and the GFLOPs of the model on a specified input shape.</li> <li>Moreover, it logs the holistic as well as individual performances to Weights &amp; Biases     which enables not only easy analysis and comparison quantitativelu, but also in a     qualitative manner.</li> </ul> <p>Abstract functions to be overriden are:</p> <ul> <li> <p><code>preprocess(self, image_path: Image) -&gt; Union[np.ndarray, tf.Tensor]</code></p> <ul> <li>Add preprocessing logic that would preprocess a <code>PIL.Image</code> and add a batch     dimension. This function should return a <code>np.ndarray</code> or a <code>tf.Tensor</code>     that would be consumed by the model.</li> </ul> </li> <li> <p><code>postprocess(self, model_output: np.ndarray) -&gt; PIL.Image</code></p> <ul> <li>Add postprocessing logic that would convert the output of the model to a     <code>PIL.Image</code>.</li> </ul> </li> <li> <p><code>populate_image_paths(self) -&gt; Dict[str, Tuple[List[str], List[str]]]</code></p> <ul> <li>Add logic to populate the split-wise image paths necessary for the evaluation.     For example, for a dataset with train, validation and test sets, the function     could return the following dictionary:</li> </ul> </li> </ul> <pre><code>{\n\"Train\": (train_low_light_images, train_ground_truth_images),\n\"Validation\": (val_low_light_images, val_ground_truth_images),\n\"Test\": (test_low_light_images, test_ground_truth_images),\n}\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>List[tf.keras.metrics.Metric]</code> <p>A list of metrics to be evaluated for.</p> required <code>model</code> <code>Optional[tf.keras.Model]</code> <p>The model that is to be evaluated. Note that passing the model during initializing the evaluator is not compulsory. The model can also be set using the function <code>initialize_model_from_wandb_artifact</code>.</p> <code>None</code> <code>input_size</code> <code>Optional[int]</code> <p>The input size for computing GFLOPs.</p> <code>None</code> <code>resize_target</code> <code>Optional[Tuple[int, int]]</code> <p>The size that the input and the corresponding ground truth image should be resized to for inference and evaluation.</p> <code>None</code> Source code in <code>restorers/evaluation/base.py</code> <pre><code>class BaseEvaluator(ABC):\n\"\"\"Abstract base class for building Evaluators for different datasets/benchmarks.\n    - An evaluator not only evaluates a model on a dataset/benchmark, but also evaluates\n        its performance with respect to the specified metrics on individual images in that\n        dataset/benchmark as well.\n    - The evaluator also logs the number of parameters (both trainable and not-trainable)\n        and the GFLOPs of the model on a specified input shape.\n    - Moreover, it logs the holistic as well as individual performances to Weights &amp; Biases\n        which enables not only easy analysis and comparison quantitativelu, but also in a\n        qualitative manner.\n    Abstract functions to be overriden are:\n    - `preprocess(self, image_path: Image) -&gt; Union[np.ndarray, tf.Tensor]`\n        - Add preprocessing logic that would preprocess a `PIL.Image` and add a batch\n            dimension. This function should return a `np.ndarray` or a `tf.Tensor`\n            that would be consumed by the model.\n    - `postprocess(self, model_output: np.ndarray) -&gt; PIL.Image`\n        - Add postprocessing logic that would convert the output of the model to a\n            `PIL.Image`.\n    - `populate_image_paths(self) -&gt; Dict[str, Tuple[List[str], List[str]]]`\n        - Add logic to populate the split-wise image paths necessary for the evaluation.\n            For example, for a dataset with train, validation and test sets, the function\n            could return the following dictionary:\n    ```python\n    {\n        \"Train\": (train_low_light_images, train_ground_truth_images),\n        \"Validation\": (val_low_light_images, val_ground_truth_images),\n        \"Test\": (test_low_light_images, test_ground_truth_images),\n    }\n    ```\n    Args:\n        metrics (List[tf.keras.metrics.Metric]): A list of metrics to be evaluated for.\n        model (Optional[tf.keras.Model]): The model that is to be evaluated. Note that passing\n            the model during initializing the evaluator is not compulsory. The model can also\n            be set using the function `initialize_model_from_wandb_artifact`.\n        input_size (Optional[int]): The input size for computing GFLOPs.\n        resize_target (Optional[Tuple[int, int]]): The size that the input and the corresponding\n            ground truth image should be resized to for inference and evaluation.\n    \"\"\"\ndef __init__(\nself,\nmetrics: List[tf.keras.metrics.Metric],\nmodel: Optional[tf.keras.Model] = None,\ninput_size: Optional[int] = None,\nresize_target: Optional[Tuple[int, int]] = None,\n) -&gt; None:\nsuper().__init__()\nself.metrics = metrics\nself.model = model\nself.input_size = input_size\nself.resize_target = resize_target\nself.image_paths = self.populate_image_paths()\nself.wandb_table = self.create_wandb_table() if wandb.run is not None else None\n@abstractmethod\ndef preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]:\n\"\"\"This is an abstract method that would hold the custom preprocessing logic that would\n        preprocess a `PIL.Image` and add a batch dimension.\n        Args:\n            image (PIL.Image): A PIL Image.\n        Returns:\n            (Union[np.ndarray, tf.Tensor]): A numpy or Tensorflow tensor that would be fed to\n                the model.\n        \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.preprocess\")\n@abstractmethod\ndef postprocess(self, model_output: np.ndarray) -&gt; Image:\n\"\"\"This is an abstract method that would hold the custom postprocessing logic that\n        would convert the output of the model to a `PIL.Image`.\n        Args:\n            model_output (np.ndarray): Output of the model.\n        Returns:\n            (PIL.Image): The model output postprocessed to a PIL Image.\n        \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.postprocess\")\n@abstractmethod\ndef populate_image_paths(self) -&gt; Dict[str, Tuple[List[str], List[str]]]:\n\"\"\"This is an abstract method that would hold the custom logic to populate the\n        split-wise image paths necessary for the evaluation. For example, for a dataset with\n        train, validation and test sets, the function could return the following dictionary:\n        ```python\n        {\n            \"Train\": (train_low_light_images, train_ground_truth_images),\n            \"Validation\": (val_low_light_images, val_ground_truth_images),\n            \"Test\": (test_low_light_images, test_ground_truth_images),\n        }\n        ```\n        Returns:\n            (Dict[str, Tuple[List[str], List[str]]]): A dictionary of Image splits mapped to list\n                of paths of input and corresponding ground-truth images.\n        \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.populate_image_paths\")\ndef create_wandb_table(self) -&gt; wandb.Table:\ncolumns = [\n\"Split\",\n\"Input-Image\",\n\"Ground-Truth-Image\",\n\"Enhanced-Image\",\n\"Inference-Time\",\n]\nmetric_alias = [type(metric).__name__ for metric in self.metrics]\ncolumns = columns + metric_alias\nreturn wandb.Table(columns=columns)\ndef initialize_model_from_wandb_artifact(self, artifact_address: str) -&gt; None:\n\"\"\"Initialize a `tf.keras.Model` that is to be evaluated from a\n        [Weights &amp; Biases artifact](https://docs.wandb.ai/guides/artifacts).\n        Args:\n            artifact_address (str): Address to the Weights &amp; Biases artifact hosting the model to be\n                evaluated.\n        \"\"\"\nself.model_path = fetch_wandb_artifact(artifact_address, artifact_type=\"model\")\nself.model = tf.keras.models.load_model(self.model_path, compile=False)\ndef evaluate_split(\nself,\ninput_image_paths: List[str],\nground_truth_image_paths: List[str],\nsplit_name: str,\n):\nprogress_bar = tqdm(\nzip(input_image_paths, ground_truth_image_paths),\ntotal=len(input_image_paths),\ndesc=f\"Evaluating {split_name} split\",\n)\ntotal_metric_values = [0.0] * len(self.metrics)\ntotal_inference_time = 0\nfor input_image_path, ground_truth_image_path in progress_bar:\ninput_image = Image.open(input_image_path)\nground_truth_image = Image.open(ground_truth_image_path)\nif self.resize_target is not None:\ninput_image = input_image.resize(self.resize_target[::-1])\nground_truth_image = ground_truth_image.resize(self.resize_target[::-1])\npreprocessed_input_image = self.preprocess(input_image)\npreprocessed_ground_truth_image = self.preprocess(ground_truth_image)\nstart_time = time()\nmodel_output = self.model(preprocessed_input_image)\ninference_time = time() - start_time\ntotal_inference_time += inference_time\nmodel_output = model_output.numpy()\nmetric_results = []\nfor idx, metric in enumerate(self.metrics):\nmetric_value = (\nmetric(preprocessed_ground_truth_image, model_output).numpy().item()\n)\nmetric_results.append(metric_value)\ntotal_metric_values[idx] += metric_value\npost_processed_image = self.postprocess(model_output)\nif self.wandb_table is not None:\ntable_row = [\nsplit_name,\nwandb.Image(input_image),\nwandb.Image(ground_truth_image),\nwandb.Image(post_processed_image),\ninference_time,\n] + metric_results\nself.wandb_table.add_data(*table_row)\nmean_metric_values = [\nvalue / len(input_image_paths) for value in total_metric_values\n]\nmetric_values = {\nsplit_name + \"/\" + type(self.metrics[idx]).__name__: metic_value\nfor idx, metic_value in enumerate(mean_metric_values)\n}\nmetric_values[split_name + \"/Inference-Time\"] = total_inference_time / len(\ninput_image_paths\n)\nreturn metric_values\ndef evaluate(self):\n\"\"\"Function to perform evaluation.\"\"\"\nlog_dict = {}\nfor split_name, (\ninput_image_paths,\nground_truth_image_paths,\n) in self.image_paths.items():\nmetric_values = self.evaluate_split(\ninput_image_paths, ground_truth_image_paths, split_name\n)\nlog_dict = {**log_dict, **metric_values}\nlog_dict[\"Evaluation\"] = self.wandb_table\ntrainable_parameters = (\ncount_params(self.model._collected_trainable_weights)\nif hasattr(self.model, \"_collected_trainable_weights\")\nelse count_params(self.model.trainable_weights)\n)\nnon_trainable_parameters = count_params(self.model.non_trainable_weights)\nlog_dict[\"Trainable Parameters\"] = trainable_parameters\nlog_dict[\"Non-Trainable Parameters\"] = non_trainable_parameters\nlog_dict[\"Total Parameters\"] = trainable_parameters + non_trainable_parameters\nif self.input_size is not None:\ntry:\nlog_dict[\"GFLOPs\"] = calculate_gflops(\nmodel=self.model, input_shape=[self.input_size, self.input_size, 3]\n)\nexcept:\npass\nif wandb.run is not None:\nwandb.log(log_dict)\n</code></pre>"},{"location":"evaluation/base/#restorers.evaluation.base.BaseEvaluator.evaluate","title":"<code>evaluate()</code>","text":"<p>Function to perform evaluation.</p> Source code in <code>restorers/evaluation/base.py</code> <pre><code>def evaluate(self):\n\"\"\"Function to perform evaluation.\"\"\"\nlog_dict = {}\nfor split_name, (\ninput_image_paths,\nground_truth_image_paths,\n) in self.image_paths.items():\nmetric_values = self.evaluate_split(\ninput_image_paths, ground_truth_image_paths, split_name\n)\nlog_dict = {**log_dict, **metric_values}\nlog_dict[\"Evaluation\"] = self.wandb_table\ntrainable_parameters = (\ncount_params(self.model._collected_trainable_weights)\nif hasattr(self.model, \"_collected_trainable_weights\")\nelse count_params(self.model.trainable_weights)\n)\nnon_trainable_parameters = count_params(self.model.non_trainable_weights)\nlog_dict[\"Trainable Parameters\"] = trainable_parameters\nlog_dict[\"Non-Trainable Parameters\"] = non_trainable_parameters\nlog_dict[\"Total Parameters\"] = trainable_parameters + non_trainable_parameters\nif self.input_size is not None:\ntry:\nlog_dict[\"GFLOPs\"] = calculate_gflops(\nmodel=self.model, input_shape=[self.input_size, self.input_size, 3]\n)\nexcept:\npass\nif wandb.run is not None:\nwandb.log(log_dict)\n</code></pre>"},{"location":"evaluation/base/#restorers.evaluation.base.BaseEvaluator.initialize_model_from_wandb_artifact","title":"<code>initialize_model_from_wandb_artifact(artifact_address)</code>","text":"<p>Initialize a <code>tf.keras.Model</code> that is to be evaluated from a Weights &amp; Biases artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_address</code> <code>str</code> <p>Address to the Weights &amp; Biases artifact hosting the model to be evaluated.</p> required Source code in <code>restorers/evaluation/base.py</code> <pre><code>def initialize_model_from_wandb_artifact(self, artifact_address: str) -&gt; None:\n\"\"\"Initialize a `tf.keras.Model` that is to be evaluated from a\n    [Weights &amp; Biases artifact](https://docs.wandb.ai/guides/artifacts).\n    Args:\n        artifact_address (str): Address to the Weights &amp; Biases artifact hosting the model to be\n            evaluated.\n    \"\"\"\nself.model_path = fetch_wandb_artifact(artifact_address, artifact_type=\"model\")\nself.model = tf.keras.models.load_model(self.model_path, compile=False)\n</code></pre>"},{"location":"evaluation/base/#restorers.evaluation.base.BaseEvaluator.populate_image_paths","title":"<code>populate_image_paths()</code>  <code>abstractmethod</code>","text":"<p>This is an abstract method that would hold the custom logic to populate the split-wise image paths necessary for the evaluation. For example, for a dataset with train, validation and test sets, the function could return the following dictionary:</p> <pre><code>{\n\"Train\": (train_low_light_images, train_ground_truth_images),\n\"Validation\": (val_low_light_images, val_ground_truth_images),\n\"Test\": (test_low_light_images, test_ground_truth_images),\n}\n</code></pre> <p>Returns:</p> Type Description <code>Dict[str, Tuple[List[str], List[str]]]</code> <p>A dictionary of Image splits mapped to list of paths of input and corresponding ground-truth images.</p> Source code in <code>restorers/evaluation/base.py</code> <pre><code>@abstractmethod\ndef populate_image_paths(self) -&gt; Dict[str, Tuple[List[str], List[str]]]:\n\"\"\"This is an abstract method that would hold the custom logic to populate the\n    split-wise image paths necessary for the evaluation. For example, for a dataset with\n    train, validation and test sets, the function could return the following dictionary:\n    ```python\n    {\n        \"Train\": (train_low_light_images, train_ground_truth_images),\n        \"Validation\": (val_low_light_images, val_ground_truth_images),\n        \"Test\": (test_low_light_images, test_ground_truth_images),\n    }\n    ```\n    Returns:\n        (Dict[str, Tuple[List[str], List[str]]]): A dictionary of Image splits mapped to list\n            of paths of input and corresponding ground-truth images.\n    \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.populate_image_paths\")\n</code></pre>"},{"location":"evaluation/base/#restorers.evaluation.base.BaseEvaluator.postprocess","title":"<code>postprocess(model_output)</code>  <code>abstractmethod</code>","text":"<p>This is an abstract method that would hold the custom postprocessing logic that would convert the output of the model to a <code>PIL.Image</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>np.ndarray</code> <p>Output of the model.</p> required <p>Returns:</p> Type Description <code>PIL.Image</code> <p>The model output postprocessed to a PIL Image.</p> Source code in <code>restorers/evaluation/base.py</code> <pre><code>@abstractmethod\ndef postprocess(self, model_output: np.ndarray) -&gt; Image:\n\"\"\"This is an abstract method that would hold the custom postprocessing logic that\n    would convert the output of the model to a `PIL.Image`.\n    Args:\n        model_output (np.ndarray): Output of the model.\n    Returns:\n        (PIL.Image): The model output postprocessed to a PIL Image.\n    \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.postprocess\")\n</code></pre>"},{"location":"evaluation/base/#restorers.evaluation.base.BaseEvaluator.preprocess","title":"<code>preprocess(image)</code>  <code>abstractmethod</code>","text":"<p>This is an abstract method that would hold the custom preprocessing logic that would preprocess a <code>PIL.Image</code> and add a batch dimension.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>PIL.Image</code> <p>A PIL Image.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, tf.Tensor]</code> <p>A numpy or Tensorflow tensor that would be fed to the model.</p> Source code in <code>restorers/evaluation/base.py</code> <pre><code>@abstractmethod\ndef preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]:\n\"\"\"This is an abstract method that would hold the custom preprocessing logic that would\n    preprocess a `PIL.Image` and add a batch dimension.\n    Args:\n        image (PIL.Image): A PIL Image.\n    Returns:\n        (Union[np.ndarray, tf.Tensor]): A numpy or Tensorflow tensor that would be fed to\n            the model.\n    \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.preprocess\")\n</code></pre>"},{"location":"evaluation/lol_eval/","title":"LOL Evaluator","text":""},{"location":"evaluation/lol_eval/#restorers.evaluation.lol_eval.LoLEvaluator","title":"<code>LoLEvaluator</code>","text":"<p>         Bases: <code>BaseEvaluator</code></p> <p>Evaluator for LoL Dataset.</p> <p>Usage:</p> <pre><code>import wandb\nfrom restorers.evaluation import LoLEvaluator\nfrom restorers.metrics import PSNRMetric, SSIMMetric\n# initialize a wandb run for inference\nwandb.init(project=\"low-light-enhancement\", job_type=\"evaluation\")\n# Define the Evaluator for LoL dataset\nevaluator = LoLEvaluator(\n# pass the list of Keras metrics to be evaluated for\nmetrics=[PSNRMetric(max_val=1.0), SSIMMetric(max_val=1.0)],\n# pass the wandb artifact for the LoL dataset\ndataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\ninput_size=256,\n)\n# initialize model from wandb artifacts\nevaluator.initialize_model_from_wandb_artifact(\n\"artifact-address-of-your-model-checkpoint\"\n)\n# evaluate\nevaluator.evaluate()\n</code></pre> Examples <ul> <li>Evaluating a low-light enhancement model.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>metrics</code> <code>List[tf.keras.metrics.Metric]</code> <p>A list of metrics to be evaluated for.</p> required <code>model</code> <code>Optional[tf.keras.Model]</code> <p>The model that is to be evaluated. Note that passing the model during initializing the evaluator is not compulsory. The model can also be set using the function <code>initialize_model_from_wandb_artifact</code>.</p> <code>None</code> <code>input_size</code> <code>Optional[int]</code> <p>The input size for computing GFLOPs.</p> <code>None</code> <code>resize_target</code> <code>Optional[Tuple[int, int]]</code> <p>The size that the input and the corresponding ground truth image should be resized to for inference and evaluation.</p> <code>None</code> <code>dataset_artifact_address</code> <code>str</code> <p>Address of the WandB artifact hosting the LoL dataset.</p> <code>None</code> Source code in <code>restorers/evaluation/lol_eval.py</code> <pre><code>class LoLEvaluator(BaseEvaluator):\n\"\"\"Evaluator for [LoL Dataset](https://www.kaggle.com/datasets/soumikrakshit/lol-dataset).\n    **Usage:**\n    ```py\n    import wandb\n    from restorers.evaluation import LoLEvaluator\n    from restorers.metrics import PSNRMetric, SSIMMetric\n    # initialize a wandb run for inference\n    wandb.init(project=\"low-light-enhancement\", job_type=\"evaluation\")\n    # Define the Evaluator for LoL dataset\n    evaluator = LoLEvaluator(\n        # pass the list of Keras metrics to be evaluated for\n        metrics=[PSNRMetric(max_val=1.0), SSIMMetric(max_val=1.0)],\n        # pass the wandb artifact for the LoL dataset\n        dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n        input_size=256,\n    )\n    # initialize model from wandb artifacts\n    evaluator.initialize_model_from_wandb_artifact(\n        \"artifact-address-of-your-model-checkpoint\"\n    )\n    # evaluate\n    evaluator.evaluate()\n    ```\n    ??? example \"Examples\"\n        - [Evaluating a low-light enhancement model](../../examples/evaluate_low_light).\n    Args:\n        metrics (List[tf.keras.metrics.Metric]): A list of metrics to be evaluated for.\n        model (Optional[tf.keras.Model]): The model that is to be evaluated. Note that passing\n            the model during initializing the evaluator is not compulsory. The model can also\n            be set using the function `initialize_model_from_wandb_artifact`.\n        input_size (Optional[int]): The input size for computing GFLOPs.\n        resize_target (Optional[Tuple[int, int]]): The size that the input and the corresponding\n            ground truth image should be resized to for inference and evaluation.\n        dataset_artifact_address (str): Address of the WandB artifact hosting the LoL dataset.\n    \"\"\"\ndef __init__(\nself,\nmetrics: List[tf.keras.metrics.Metric],\nmodel: Optional[tf.keras.Model] = None,\ninput_size: Optional[List[int]] = None,\nresize_target: Optional[Tuple[int, int]] = None,\ndataset_artifact_address: str = None,\n) -&gt; None:\nself.dataset_artifact_address = dataset_artifact_address\nsuper().__init__(metrics, model, input_size, resize_target)\ndef preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]:\n\"\"\"Preprocessing logic for preprocessing a `PIL.Image` and add a batch dimension.\n        Args:\n            image (PIL.Image): A PIL Image.\n        Returns:\n            (Union[np.ndarray, tf.Tensor]): A numpy or Tensorflow tensor that would be fed to\n                the model.\n        \"\"\"\nimage = tf.keras.preprocessing.image.img_to_array(image)\nimage = image.astype(\"float32\") / 255.0\nreturn np.expand_dims(image, axis=0)\ndef postprocess(self, model_output: np.ndarray) -&gt; Image:\n\"\"\"Postprocessing logic for converting the output of the model to a `PIL.Image`.\n        Args:\n            model_output (np.ndarray): Output of the model.\n        Returns:\n            (PIL.Image): The model output postprocessed to a PIL Image.\n        \"\"\"\nmodel_output = model_output * 255.0\nmodel_output = model_output.clip(0, 255)\nimage = model_output[0].reshape(\n(np.shape(model_output)[1], np.shape(model_output)[2], 3)\n)\nreturn Image.fromarray(np.uint8(image))\ndef populate_image_paths(self) -&gt; Dict[str, Tuple[List[str], List[str]]]:\n\"\"\"Populate the split-wise image paths necessary for the evaluation.\n        Returns:\n            (Dict[str, Tuple[List[str], List[str]]]): A dictionary of Image splits mapped to list\n                of paths of input and corresponding ground-truth images. The dictionary in this case would be\n        ```python\n        {\n            \"Train-Val\": (train_low_light_images, train_ground_truth_images),\n            \"Eval15\": (test_low_light_images, test_ground_truth_images),\n        }\n        ```\n        \"\"\"\ndataset_path = fetch_wandb_artifact(\nself.dataset_artifact_address, artifact_type=\"dataset\"\n)\ntrain_low_light_images = sorted(\nglob(os.path.join(dataset_path, \"our485\", \"low\", \"*\"))\n)\ntrain_ground_truth_images = sorted(\nglob(os.path.join(dataset_path, \"our485\", \"high\", \"*\"))\n)\ntest_low_light_images = sorted(\nglob(os.path.join(dataset_path, \"eval15\", \"low\", \"*\"))\n)\ntest_ground_truth_images = sorted(\nglob(os.path.join(dataset_path, \"eval15\", \"high\", \"*\"))\n)\nreturn {\n\"Train-Val\": (train_low_light_images, train_ground_truth_images),\n\"Eval15\": (test_low_light_images, test_ground_truth_images),\n}\n</code></pre>"},{"location":"evaluation/lol_eval/#restorers.evaluation.lol_eval.LoLEvaluator.populate_image_paths","title":"<code>populate_image_paths()</code>","text":"<p>Populate the split-wise image paths necessary for the evaluation.</p> <p>Returns:</p> Type Description <code>Dict[str, Tuple[List[str], List[str]]]</code> <p>A dictionary of Image splits mapped to list of paths of input and corresponding ground-truth images. The dictionary in this case would be</p> <pre><code>{\n\"Train-Val\": (train_low_light_images, train_ground_truth_images),\n\"Eval15\": (test_low_light_images, test_ground_truth_images),\n}\n</code></pre> Source code in <code>restorers/evaluation/lol_eval.py</code> <pre><code>def populate_image_paths(self) -&gt; Dict[str, Tuple[List[str], List[str]]]:\n\"\"\"Populate the split-wise image paths necessary for the evaluation.\n    Returns:\n        (Dict[str, Tuple[List[str], List[str]]]): A dictionary of Image splits mapped to list\n            of paths of input and corresponding ground-truth images. The dictionary in this case would be\n    ```python\n    {\n        \"Train-Val\": (train_low_light_images, train_ground_truth_images),\n        \"Eval15\": (test_low_light_images, test_ground_truth_images),\n    }\n    ```\n    \"\"\"\ndataset_path = fetch_wandb_artifact(\nself.dataset_artifact_address, artifact_type=\"dataset\"\n)\ntrain_low_light_images = sorted(\nglob(os.path.join(dataset_path, \"our485\", \"low\", \"*\"))\n)\ntrain_ground_truth_images = sorted(\nglob(os.path.join(dataset_path, \"our485\", \"high\", \"*\"))\n)\ntest_low_light_images = sorted(\nglob(os.path.join(dataset_path, \"eval15\", \"low\", \"*\"))\n)\ntest_ground_truth_images = sorted(\nglob(os.path.join(dataset_path, \"eval15\", \"high\", \"*\"))\n)\nreturn {\n\"Train-Val\": (train_low_light_images, train_ground_truth_images),\n\"Eval15\": (test_low_light_images, test_ground_truth_images),\n}\n</code></pre>"},{"location":"evaluation/lol_eval/#restorers.evaluation.lol_eval.LoLEvaluator.postprocess","title":"<code>postprocess(model_output)</code>","text":"<p>Postprocessing logic for converting the output of the model to a <code>PIL.Image</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>np.ndarray</code> <p>Output of the model.</p> required <p>Returns:</p> Type Description <code>PIL.Image</code> <p>The model output postprocessed to a PIL Image.</p> Source code in <code>restorers/evaluation/lol_eval.py</code> <pre><code>def postprocess(self, model_output: np.ndarray) -&gt; Image:\n\"\"\"Postprocessing logic for converting the output of the model to a `PIL.Image`.\n    Args:\n        model_output (np.ndarray): Output of the model.\n    Returns:\n        (PIL.Image): The model output postprocessed to a PIL Image.\n    \"\"\"\nmodel_output = model_output * 255.0\nmodel_output = model_output.clip(0, 255)\nimage = model_output[0].reshape(\n(np.shape(model_output)[1], np.shape(model_output)[2], 3)\n)\nreturn Image.fromarray(np.uint8(image))\n</code></pre>"},{"location":"evaluation/lol_eval/#restorers.evaluation.lol_eval.LoLEvaluator.preprocess","title":"<code>preprocess(image)</code>","text":"<p>Preprocessing logic for preprocessing a <code>PIL.Image</code> and add a batch dimension.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>PIL.Image</code> <p>A PIL Image.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, tf.Tensor]</code> <p>A numpy or Tensorflow tensor that would be fed to the model.</p> Source code in <code>restorers/evaluation/lol_eval.py</code> <pre><code>def preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]:\n\"\"\"Preprocessing logic for preprocessing a `PIL.Image` and add a batch dimension.\n    Args:\n        image (PIL.Image): A PIL Image.\n    Returns:\n        (Union[np.ndarray, tf.Tensor]): A numpy or Tensorflow tensor that would be fed to\n            the model.\n    \"\"\"\nimage = tf.keras.preprocessing.image.img_to_array(image)\nimage = image.astype(\"float32\") / 255.0\nreturn np.expand_dims(image, axis=0)\n</code></pre>"},{"location":"examples/evaluate_low_light/","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q --upgrade pip setuptools\n!pip install git+https://github.com/soumik12345/restorers.git\n</pre> !pip install -q --upgrade pip setuptools !pip install git+https://github.com/soumik12345/restorers.git In\u00a0[\u00a0]: Copied! <pre>import wandb\nfrom restorers.evaluation import LoLEvaluator\nfrom restorers.metrics import PSNRMetric, SSIMMetric\n</pre> import wandb from restorers.evaluation import LoLEvaluator from restorers.metrics import PSNRMetric, SSIMMetric In\u00a0[\u00a0]: Copied! <pre># initialize a wandb run for inference\nwandb.init(project=\"low-light-enhancement\", job_type=\"evaluation\")\n</pre> # initialize a wandb run for inference wandb.init(project=\"low-light-enhancement\", job_type=\"evaluation\") In\u00a0[\u00a0]: Copied! <pre># Define the Evaluator for LoL dataset\nevaluator = LoLEvaluator(\n    # pass the list of Keras metrics to be evaluated for\n    metrics=[PSNRMetric(max_val=1.0), SSIMMetric(max_val=1.0)],\n    # pass the wandb artifact for the LoL dataset\n    dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n    input_size=256,\n)\n# initialize model from wandb artifacts\nevaluator.initialize_model_from_wandb_artifact(\"artifact-address-of-your-model-checkpoint\")\n# evaluate\nevaluator.evaluate()\n</pre> # Define the Evaluator for LoL dataset evaluator = LoLEvaluator(     # pass the list of Keras metrics to be evaluated for     metrics=[PSNRMetric(max_val=1.0), SSIMMetric(max_val=1.0)],     # pass the wandb artifact for the LoL dataset     dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",     input_size=256, ) # initialize model from wandb artifacts evaluator.initialize_model_from_wandb_artifact(\"artifact-address-of-your-model-checkpoint\") # evaluate evaluator.evaluate()  In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n</pre> wandb.finish()"},{"location":"examples/evaluate_low_light/#restorers-wandb","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d\u00b6","text":"<p>This notebook shows how to perform inference with a low-light enhancement using restorers and wandb. For more details regarding usage of restorers, refer to the following report:</p> <p></p>"},{"location":"examples/inference_low_light/","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q --upgrade pip setuptools\n!pip install git+https://github.com/soumik12345/restorers.git\n</pre> !pip install -q --upgrade pip setuptools !pip install git+https://github.com/soumik12345/restorers.git In\u00a0[\u00a0]: Copied! <pre>import os\nimport wandb\nfrom restorers.inference import LowLightInferer\n</pre> import os import wandb from restorers.inference import LowLightInferer In\u00a0[\u00a0]: Copied! <pre># initialize a wandb run for inference\nwandb.init(project=\"low-light-enhancement\", job_type=\"inference\")\n</pre> # initialize a wandb run for inference wandb.init(project=\"low-light-enhancement\", job_type=\"inference\") In\u00a0[\u00a0]: Copied! <pre>images_artifact = wandb.use_artifact('ml-colabs/low-light-enhancement/run-7ngsohcn-DarkImagesTable:v0', type='run_table')\nimages_artifact_dir = images_artifact.download()\nsample_image = os.path.join(images_artifact_dir, \"media/images/0b63c6b0cfdfd95675f7/image_9.png\")\n</pre> images_artifact = wandb.use_artifact('ml-colabs/low-light-enhancement/run-7ngsohcn-DarkImagesTable:v0', type='run_table') images_artifact_dir = images_artifact.download() sample_image = os.path.join(images_artifact_dir, \"media/images/0b63c6b0cfdfd95675f7/image_9.png\") In\u00a0[\u00a0]: Copied! <pre># initialize the inferer\ninferer = LowLightInferer(\n    resize_factor=1, model_alias=\"Zero-DCE\"\n)\n# intialize the model from wandb artifacts\ninferer.initialize_model_from_wandb_artifact(\n    # This artifact address corresponds to a Zero-DCE model trained on the LoL dataset\n    \"ml-colabs/low-light-enhancement/run_oaa25znm_model:v99\"\n)\n# infer on a directory of images\n# inferer.infer(\"./dark_images\")\n# or infer on a single image\ninferer.infer(sample_image)\n</pre> # initialize the inferer inferer = LowLightInferer(     resize_factor=1, model_alias=\"Zero-DCE\" ) # intialize the model from wandb artifacts inferer.initialize_model_from_wandb_artifact(     # This artifact address corresponds to a Zero-DCE model trained on the LoL dataset     \"ml-colabs/low-light-enhancement/run_oaa25znm_model:v99\" ) # infer on a directory of images # inferer.infer(\"./dark_images\") # or infer on a single image inferer.infer(sample_image) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n</pre> wandb.finish()"},{"location":"examples/inference_low_light/#restorers-wandb","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d\u00b6","text":"<p>This notebook shows how to perform inference with a low-light enhancement using restorers and wandb. For more details regarding usage of restorers, refer to the following report:</p> <p></p>"},{"location":"examples/train_fast_zero_dce/","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q --upgrade pip setuptools\n!pip install git+https://github.com/soumik12345/restorers.git\n</pre> !pip install -q --upgrade pip setuptools !pip install git+https://github.com/soumik12345/restorers.git In\u00a0[\u00a0]: Copied! <pre>import os\nfrom glob import glob\n\nimport tensorflow as tf\n\nimport wandb\n# import the wandb callbacks for keras\nfrom wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n\nfrom restorers.dataloader import UnsupervisedLOLDataLoader\nfrom restorers.model.zero_dce import ZeroDCE\n</pre> import os from glob import glob  import tensorflow as tf  import wandb # import the wandb callbacks for keras from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint  from restorers.dataloader import UnsupervisedLOLDataLoader from restorers.model.zero_dce import ZeroDCE In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"low-light-enhancement\", job_type=\"train\")\n\n\ndata_loader = UnsupervisedLOLDataLoader(\n    # size of image crops on which we will train\n    image_size=128,\n    # bit depth of the images\n    bit_depth=8,\n    # fraction of images for validation\n    val_split=0.2,\n    # visualize the dataset on WandB or not\n    visualize_on_wandb=True,\n    # the wandb artifact address of the dataset,\n    # this can be found from the `Usage` tab of\n    # the aforemenioned weave panel\n    dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n)\n\n# call `get_datasets` on the `data_loader` to get\n# the TensorFlow datasets corresponding to the \n# training and validation splits\ntrain_dataset, val_dataset = data_loader.get_datasets(batch_size=16)\n</pre> wandb.init(project=\"low-light-enhancement\", job_type=\"train\")   data_loader = UnsupervisedLOLDataLoader(     # size of image crops on which we will train     image_size=128,     # bit depth of the images     bit_depth=8,     # fraction of images for validation     val_split=0.2,     # visualize the dataset on WandB or not     visualize_on_wandb=True,     # the wandb artifact address of the dataset,     # this can be found from the `Usage` tab of     # the aforemenioned weave panel     dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\", )  # call `get_datasets` on the `data_loader` to get # the TensorFlow datasets corresponding to the  # training and validation splits train_dataset, val_dataset = data_loader.get_datasets(batch_size=16) In\u00a0[\u00a0]: Copied! <pre># define the ZeroDCE model; this gives us a `tf.keras.Model`\nmodel = ZeroDCE(\n    num_intermediate_filters=32, # number of filters in the intermediate convolutional layers\n    num_iterations=8, # number of iterations of enhancement\n    decoder_channel_factor=1 # factor by which number filters in the decoder of deep curve estimation layer is multiplied\n)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    weight_exposure_loss=1.0, # weight of the exposure control loss\n    weight_color_constancy_loss=0.5, # weight of the color constancy loss\n    weight_illumination_smoothness_loss=20, # weight of the illumination smoothness loss\n)\n</pre> # define the ZeroDCE model; this gives us a `tf.keras.Model` model = ZeroDCE(     num_intermediate_filters=32, # number of filters in the intermediate convolutional layers     num_iterations=8, # number of iterations of enhancement     decoder_channel_factor=1 # factor by which number filters in the decoder of deep curve estimation layer is multiplied )  model.compile(     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),     weight_exposure_loss=1.0, # weight of the exposure control loss     weight_color_constancy_loss=0.5, # weight of the color constancy loss     weight_illumination_smoothness_loss=20, # weight of the illumination smoothness loss ) In\u00a0[\u00a0]: Copied! <pre>callbacks = [\n    # define the metrics logger callback;\n    # we set the `log_freq=\"batch\"` explicitly\n    # to the metrics are logged both batch-wise and epoch-wise\n    WandbMetricsLogger(log_freq=\"batch\"),\n    # define the model checkpoint callback\n    WandbModelCheckpoint(\n        filepath=\"checkpoint\",\n        monitor=\"val_loss\",\n        save_best_only=False,\n        save_weights_only=False,\n        initial_value_threshold=None,\n    )\n]\n\n# call model.fit()\nmodel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=50,\n    callbacks=callbacks,\n)\n</pre> callbacks = [     # define the metrics logger callback;     # we set the `log_freq=\"batch\"` explicitly     # to the metrics are logged both batch-wise and epoch-wise     WandbMetricsLogger(log_freq=\"batch\"),     # define the model checkpoint callback     WandbModelCheckpoint(         filepath=\"checkpoint\",         monitor=\"val_loss\",         save_best_only=False,         save_weights_only=False,         initial_value_threshold=None,     ) ]  # call model.fit() model.fit(     train_dataset,     validation_data=val_dataset,     epochs=50,     callbacks=callbacks, ) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n</pre> wandb.finish()"},{"location":"examples/train_fast_zero_dce/#restorers-wandb","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d\u00b6","text":"<p>This notebook shows how to train a Zero-DCE model for zero-reference low-light enhancement using restorers and wandb. For more details regarding usage of restorers, refer to the following report:</p> <p></p>"},{"location":"examples/train_mirnetv2/","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q --upgrade pip setuptools\n!pip install git+https://github.com/soumik12345/restorers.git\n</pre> !pip install -q --upgrade pip setuptools !pip install git+https://github.com/soumik12345/restorers.git In\u00a0[\u00a0]: Copied! <pre>import wandb\nimport tensorflow as tf\nfrom restorers.dataloader import LOLDataLoader\n</pre> import wandb import tensorflow as tf from restorers.dataloader import LOLDataLoader In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"low-light-enhancement\")\n\n# define dataloader for the LoL dataset\ndata_loader = LOLDataLoader(\n    # size of image crops on which we will train\n    image_size=128,\n    # bit depth of the images\n    bit_depth=8,\n    # fraction of images for validation\n    val_split=0.2,\n    # visualize the dataset on WandB or not\n    visualize_on_wandb=True,\n    # the wandb artifact address of the dataset,\n    # this can be found from the `Usage` tab of\n    # the aforemenioned weave panel\n    dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n)\n\n# call `get_datasets` on the `data_loader` to get\n# the TensorFlow datasets corresponding to the \n# training and validation splits\ndatasets = data_loader.get_datasets(batch_size=2)\ntrain_dataset, val_dataset = datasets\n</pre> wandb.init(project=\"low-light-enhancement\")  # define dataloader for the LoL dataset data_loader = LOLDataLoader(     # size of image crops on which we will train     image_size=128,     # bit depth of the images     bit_depth=8,     # fraction of images for validation     val_split=0.2,     # visualize the dataset on WandB or not     visualize_on_wandb=True,     # the wandb artifact address of the dataset,     # this can be found from the `Usage` tab of     # the aforemenioned weave panel     dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\", )  # call `get_datasets` on the `data_loader` to get # the TensorFlow datasets corresponding to the  # training and validation splits datasets = data_loader.get_datasets(batch_size=2) train_dataset, val_dataset = datasets In\u00a0[\u00a0]: Copied! <pre># import MirNetv2 from restorers\nfrom restorers.model import MirNetv2\n\n\n# define the MirNetv2 model; this gives us a `tf.keras.Model`\nmodel = MirNetv2(\n    # number of channels in the feature map\n    channels=80,\n    # number of multi-scale residual blocks\n    channel_factor=1.5,\n    # factor by which number of the number of output channels vary\n    num_mrb_blocks=2,\n    # number of groups in which the input is split along the\n    # channel axis in the convolution layers.\n    add_residual_connection=True,\n)\n</pre> # import MirNetv2 from restorers from restorers.model import MirNetv2   # define the MirNetv2 model; this gives us a `tf.keras.Model` model = MirNetv2(     # number of channels in the feature map     channels=80,     # number of multi-scale residual blocks     channel_factor=1.5,     # factor by which number of the number of output channels vary     num_mrb_blocks=2,     # number of groups in which the input is split along the     # channel axis in the convolution layers.     add_residual_connection=True, ) In\u00a0[\u00a0]: Copied! <pre>from restorers.losses import CharbonnierLoss\n# import Peak Signal-to-Noise Ratio and Structural Similarity metrics,\n# implemented as part of restorers\nfrom restorers.metrics import PSNRMetric, SSIMMetric\n\n\nloss = CharbonnierLoss(\n    # a small constant to avoid division by zero\n    epsilon=1e-3,\n    # type of reduction applied to the loss, it needs to be\n    # explicitly specified in case of distributed training\n    reduction=tf.keras.losses.Reduction.SUM,\n)\n\n\noptimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=2e-4,)\n\npsnr_metric = PSNRMetric(max_val=1.0) # peak signal-to-noise ratio metric\nssim_metric = SSIMMetric(max_val=1.0) # structural similarity metric\n\nmodel.compile(\n    optimizer=optimizer, loss=loss, metrics=[psnr_metric, ssim_metric]\n)\n</pre> from restorers.losses import CharbonnierLoss # import Peak Signal-to-Noise Ratio and Structural Similarity metrics, # implemented as part of restorers from restorers.metrics import PSNRMetric, SSIMMetric   loss = CharbonnierLoss(     # a small constant to avoid division by zero     epsilon=1e-3,     # type of reduction applied to the loss, it needs to be     # explicitly specified in case of distributed training     reduction=tf.keras.losses.Reduction.SUM, )   optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=2e-4,)  psnr_metric = PSNRMetric(max_val=1.0) # peak signal-to-noise ratio metric ssim_metric = SSIMMetric(max_val=1.0) # structural similarity metric  model.compile(     optimizer=optimizer, loss=loss, metrics=[psnr_metric, ssim_metric] ) In\u00a0[\u00a0]: Copied! <pre># import the wandb callbacks for keras\nfrom wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n\n\ncallbacks = [\n    # define the metrics logger callback;\n    # we set the `log_freq=\"batch\"` explicitly\n    # to the metrics are logged both batch-wise and epoch-wise\n    WandbMetricsLogger(log_freq=\"batch\"),\n    # define the model checkpoint callback\n    WandbModelCheckpoint(\n        filepath=\"checkpoint\",\n        monitor=\"val_loss\",\n        save_best_only=False,\n        save_weights_only=False,\n        initial_value_threshold=None,\n    )\n]\n\n# call model.fit()\nmodel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=50,\n    callbacks=callbacks,\n)\n</pre> # import the wandb callbacks for keras from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint   callbacks = [     # define the metrics logger callback;     # we set the `log_freq=\"batch\"` explicitly     # to the metrics are logged both batch-wise and epoch-wise     WandbMetricsLogger(log_freq=\"batch\"),     # define the model checkpoint callback     WandbModelCheckpoint(         filepath=\"checkpoint\",         monitor=\"val_loss\",         save_best_only=False,         save_weights_only=False,         initial_value_threshold=None,     ) ]  # call model.fit() model.fit(     train_dataset,     validation_data=val_dataset,     epochs=50,     callbacks=callbacks, ) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n</pre> wandb.finish()"},{"location":"examples/train_mirnetv2/#restorers-wandb","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d\u00b6","text":"<p>This notebook shows how to train a MirNetv2 model for low-light enhancement using restorers and wandb. For more details regarding usage of restorers, refer to the following report:</p> <p></p>"},{"location":"examples/train_nafnet/","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d","text":"In\u00a0[\u00a0]: Copied! <pre>!pip install -q --upgrade pip setuptools\n!pip install git+https://github.com/soumik12345/restorers.git\n</pre> !pip install -q --upgrade pip setuptools !pip install git+https://github.com/soumik12345/restorers.git In\u00a0[\u00a0]: Copied! <pre>import wandb\nimport tensorflow as tf\n\nfrom restorers.dataloader import LOLDataLoader\n</pre> import wandb import tensorflow as tf  from restorers.dataloader import LOLDataLoader In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"low-light-enhancement\")\n\n# define dataloader for the LoL dataset\ndata_loader = LOLDataLoader(\n    # size of image crops on which we will train\n    image_size=128,\n    # bit depth of the images\n    bit_depth=8,\n    # fraction of images for validation\n    val_split=0.2,\n    # visualize the dataset on WandB or not\n    visualize_on_wandb=True,\n    # the wandb artifact address of the dataset,\n    # this can be found from the `Usage` tab of\n    # the aforemenioned weave panel\n    dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n)\n\n# call `get_datasets` on the `data_loader` to get\n# the TensorFlow datasets corresponding to the \n# training and validation splits\ndatasets = data_loader.get_datasets(batch_size=2)\ntrain_dataset, val_dataset = datasets\n</pre> wandb.init(project=\"low-light-enhancement\")  # define dataloader for the LoL dataset data_loader = LOLDataLoader(     # size of image crops on which we will train     image_size=128,     # bit depth of the images     bit_depth=8,     # fraction of images for validation     val_split=0.2,     # visualize the dataset on WandB or not     visualize_on_wandb=True,     # the wandb artifact address of the dataset,     # this can be found from the `Usage` tab of     # the aforemenioned weave panel     dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\", )  # call `get_datasets` on the `data_loader` to get # the TensorFlow datasets corresponding to the  # training and validation splits datasets = data_loader.get_datasets(batch_size=2) train_dataset, val_dataset = datasets In\u00a0[\u00a0]: Copied! <pre># import MirNetv2 from restorers\nfrom restorers.model import NAFNet\n\n\n# define the MirNetv2 model; this gives us a `tf.keras.Model`\nmodel = NAFNet(\n    filters=16,\n    middle_block_num=1,\n    encoder_block_nums=(1, 1, 1, 1),\n    decoder_block_nums=(1, 1, 1, 1)\n)\n</pre> # import MirNetv2 from restorers from restorers.model import NAFNet   # define the MirNetv2 model; this gives us a `tf.keras.Model` model = NAFNet(     filters=16,     middle_block_num=1,     encoder_block_nums=(1, 1, 1, 1),     decoder_block_nums=(1, 1, 1, 1) ) In\u00a0[\u00a0]: Copied! <pre>from restorers.losses import CharbonnierLoss\n# import Peak Signal-to-Noise Ratio and Structural Similarity metrics,\n# implemented as part of restorers\nfrom restorers.metrics import PSNRMetric, SSIMMetric\n\n\nloss = CharbonnierLoss(\n    # a small constant to avoid division by zero\n    epsilon=1e-3,\n    # type of reduction applied to the loss, it needs to be\n    # explicitly specified in case of distributed training\n    reduction=tf.keras.losses.Reduction.SUM,\n)\n\n\noptimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=2e-4,)\n\npsnr_metric = PSNRMetric(max_val=1.0) # peak signal-to-noise ratio metric\nssim_metric = SSIMMetric(max_val=1.0) # structural similarity metric\n\nmodel.compile(\n    optimizer=optimizer, loss=loss, metrics=[psnr_metric, ssim_metric]\n)\n</pre> from restorers.losses import CharbonnierLoss # import Peak Signal-to-Noise Ratio and Structural Similarity metrics, # implemented as part of restorers from restorers.metrics import PSNRMetric, SSIMMetric   loss = CharbonnierLoss(     # a small constant to avoid division by zero     epsilon=1e-3,     # type of reduction applied to the loss, it needs to be     # explicitly specified in case of distributed training     reduction=tf.keras.losses.Reduction.SUM, )   optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=2e-4,)  psnr_metric = PSNRMetric(max_val=1.0) # peak signal-to-noise ratio metric ssim_metric = SSIMMetric(max_val=1.0) # structural similarity metric  model.compile(     optimizer=optimizer, loss=loss, metrics=[psnr_metric, ssim_metric] ) In\u00a0[\u00a0]: Copied! <pre># import the wandb callbacks for keras\nfrom wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n\n\ncallbacks = [\n    # define the metrics logger callback;\n    # we set the `log_freq=\"batch\"` explicitly\n    # to the metrics are logged both batch-wise and epoch-wise\n    WandbMetricsLogger(log_freq=\"batch\"),\n    # define the model checkpoint callback\n    WandbModelCheckpoint(\n        filepath=\"checkpoint\",\n        monitor=\"val_loss\",\n        save_best_only=False,\n        save_weights_only=False,\n        initial_value_threshold=None,\n    )\n]\n\n# call model.fit()\nmodel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=50,\n    callbacks=callbacks,\n)\n</pre> # import the wandb callbacks for keras from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint   callbacks = [     # define the metrics logger callback;     # we set the `log_freq=\"batch\"` explicitly     # to the metrics are logged both batch-wise and epoch-wise     WandbMetricsLogger(log_freq=\"batch\"),     # define the model checkpoint callback     WandbModelCheckpoint(         filepath=\"checkpoint\",         monitor=\"val_loss\",         save_best_only=False,         save_weights_only=False,         initial_value_threshold=None,     ) ]  # call model.fit() model.fit(     train_dataset,     validation_data=val_dataset,     epochs=50,     callbacks=callbacks, ) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n</pre> wandb.finish()"},{"location":"examples/train_nafnet/#restorers-wandb","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d\u00b6","text":"<p>This notebook shows how to train a NAFNet model for low-light enhancement using restorers and wandb. For more details regarding usage of restorers, refer to the following report:</p> <p></p>"},{"location":"examples/train_zero_dce/","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d","text":"In\u00a0[\u00a0]: Copied! <pre># !pip install -q --upgrade pip setuptools\n# !pip install git+https://github.com/soumik12345/restorers.git\n</pre> # !pip install -q --upgrade pip setuptools # !pip install git+https://github.com/soumik12345/restorers.git In\u00a0[\u00a0]: Copied! <pre>import os\nfrom glob import glob\n\nimport tensorflow as tf\n\nimport wandb\n# import the wandb callbacks for keras\nfrom wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n\nfrom restorers.dataloader import UnsupervisedLOLDataLoader\nfrom restorers.model.zero_dce import FastZeroDce\n</pre> import os from glob import glob  import tensorflow as tf  import wandb # import the wandb callbacks for keras from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint  from restorers.dataloader import UnsupervisedLOLDataLoader from restorers.model.zero_dce import FastZeroDce In\u00a0[\u00a0]: Copied! <pre>wandb.init(project=\"low-light-enhancement\", job_type=\"train\")\n\n\ndata_loader = UnsupervisedLOLDataLoader(\n    # size of image crops on which we will train\n    image_size=128,\n    # bit depth of the images\n    bit_depth=8,\n    # fraction of images for validation\n    val_split=0.2,\n    # visualize the dataset on WandB or not\n    visualize_on_wandb=True,\n    # the wandb artifact address of the dataset,\n    # this can be found from the `Usage` tab of\n    # the aforemenioned weave panel\n    dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n)\n\n# call `get_datasets` on the `data_loader` to get\n# the TensorFlow datasets corresponding to the \n# training and validation splits\ntrain_dataset, val_dataset = data_loader.get_datasets(batch_size=16)\n</pre> wandb.init(project=\"low-light-enhancement\", job_type=\"train\")   data_loader = UnsupervisedLOLDataLoader(     # size of image crops on which we will train     image_size=128,     # bit depth of the images     bit_depth=8,     # fraction of images for validation     val_split=0.2,     # visualize the dataset on WandB or not     visualize_on_wandb=True,     # the wandb artifact address of the dataset,     # this can be found from the `Usage` tab of     # the aforemenioned weave panel     dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\", )  # call `get_datasets` on the `data_loader` to get # the TensorFlow datasets corresponding to the  # training and validation splits train_dataset, val_dataset = data_loader.get_datasets(batch_size=16) In\u00a0[\u00a0]: Copied! <pre># define the FastZeroDce model; this gives us a `tf.keras.Model`\nmodel = FastZeroDce(\n    num_intermediate_filters=32, # number of filters in the intermediate convolutional layers\n    num_iterations=8, # number of iterations of enhancement\n    decoder_channel_factor=1 # factor by which number filters in the decoder of deep curve estimation layer is multiplied\n)\n\nmodel.compile(\n    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n    weight_exposure_loss=1.0, # weight of the exposure control loss\n    weight_color_constancy_loss=0.5, # weight of the color constancy loss\n    weight_illumination_smoothness_loss=20, # weight of the illumination smoothness loss\n)\n</pre> # define the FastZeroDce model; this gives us a `tf.keras.Model` model = FastZeroDce(     num_intermediate_filters=32, # number of filters in the intermediate convolutional layers     num_iterations=8, # number of iterations of enhancement     decoder_channel_factor=1 # factor by which number filters in the decoder of deep curve estimation layer is multiplied )  model.compile(     optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),     weight_exposure_loss=1.0, # weight of the exposure control loss     weight_color_constancy_loss=0.5, # weight of the color constancy loss     weight_illumination_smoothness_loss=20, # weight of the illumination smoothness loss ) In\u00a0[\u00a0]: Copied! <pre>callbacks = [\n    # define the metrics logger callback;\n    # we set the `log_freq=\"batch\"` explicitly\n    # to the metrics are logged both batch-wise and epoch-wise\n    WandbMetricsLogger(log_freq=\"batch\"),\n    # define the model checkpoint callback\n    WandbModelCheckpoint(\n        filepath=\"checkpoint\",\n        monitor=\"val_loss\",\n        save_best_only=False,\n        save_weights_only=False,\n        initial_value_threshold=None,\n    )\n]\n\n# call model.fit()\nmodel.fit(\n    train_dataset,\n    validation_data=val_dataset,\n    epochs=50,\n    callbacks=callbacks,\n    verbose=0\n)\n</pre> callbacks = [     # define the metrics logger callback;     # we set the `log_freq=\"batch\"` explicitly     # to the metrics are logged both batch-wise and epoch-wise     WandbMetricsLogger(log_freq=\"batch\"),     # define the model checkpoint callback     WandbModelCheckpoint(         filepath=\"checkpoint\",         monitor=\"val_loss\",         save_best_only=False,         save_weights_only=False,         initial_value_threshold=None,     ) ]  # call model.fit() model.fit(     train_dataset,     validation_data=val_dataset,     epochs=50,     callbacks=callbacks,     verbose=0 ) In\u00a0[\u00a0]: Copied! <pre>wandb.finish()\n</pre> wandb.finish()"},{"location":"examples/train_zero_dce/#restorers-wandb","title":"\ud83c\udf08 Restorers + WandB \ud83e\ude84\ud83d\udc1d\u00b6","text":"<p>This notebook shows how to train a Zero-DCE model for zero-reference low-light enhancement using restorers and wandb. For more details regarding usage of restorers, refer to the following report:</p> <p></p>"},{"location":"inference/base/","title":"Base Inferer","text":""},{"location":"inference/base/#restorers.inference.base.BaseInferer","title":"<code>BaseInferer</code>","text":"<p>         Bases: <code>ABC</code></p> <p>Abstract base class for building Inferers for different tasks or models.</p> <ul> <li>The inferer can perform inference on a single image or on a directory of images.</li> <li>The inferred images are saved to the specified output directory.</li> <li>The inferer can also log the inference time and the input and output images to     Weights &amp; Biases.</li> </ul> <p>Abstract functions to be overriden are:</p> <ul> <li> <p><code>preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]</code></p> <ul> <li>Add custom preprocessing logic that would preprocess a <code>PIL.Image</code> and add     a batch dimension. This function should return a <code>np.ndarray</code> or a <code>tf.Tensor</code>     that would be consumed by the model.</li> </ul> </li> <li> <p><code>postprocess(self, model_output: np.ndarray) -&gt; Image</code></p> <ul> <li>Add postprocessing logic that would convert the output of the model to a     <code>PIL.Image</code>.</li> </ul> </li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[tf.keras.Model]</code> <p>The model that is to be evaluated. Note that passing the model during initializing the evaluator is not compulsory. The model can also be set using the function <code>initialize_model_from_wandb_artifact</code>.</p> <code>None</code> <code>resize_factor</code> <code>Optional[int]</code> <p>The factor by which the input image should be resized for inference.</p> <code>1</code> <code>model_alias</code> <code>Optional[str]</code> <p>The alias of the model that is to be logged to Weights &amp; Biases. This is useful for qualitative comparison of results of multiple models.</p> <code>None</code> Source code in <code>restorers/inference/base.py</code> <pre><code>class BaseInferer(ABC):\n\"\"\"Abstract base class for building Inferers for different tasks or models.\n    - The inferer can perform inference on a single image or on a directory of images.\n    - The inferred images are saved to the specified output directory.\n    - The inferer can also log the inference time and the input and output images to\n        Weights &amp; Biases.\n    Abstract functions to be overriden are:\n    - `preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]`\n        - Add custom preprocessing logic that would preprocess a `PIL.Image` and add\n            a batch dimension. This function should return a `np.ndarray` or a `tf.Tensor`\n            that would be consumed by the model.\n    - `postprocess(self, model_output: np.ndarray) -&gt; Image`\n        - Add postprocessing logic that would convert the output of the model to a\n            `PIL.Image`.\n    Args:\n        model (Optional[tf.keras.Model]): The model that is to be evaluated. Note that passing\n            the model during initializing the evaluator is not compulsory. The model can also\n            be set using the function `initialize_model_from_wandb_artifact`.\n        resize_factor (Optional[int]): The factor by which the input image should be resized\n            for inference.\n        model_alias (Optional[str]): The alias of the model that is to be logged to\n            Weights &amp; Biases. This is useful for qualitative comparison of results of multiple\n            models.\n    \"\"\"\ndef __init__(\nself,\nmodel: Optional[tf.keras.Model] = None,\nresize_factor: Optional[int] = 1,\nmodel_alias: Optional[str] = None,\n) -&gt; None:\nsuper().__init__()\nself.model = model\nself.resize_factor = resize_factor\nself.model_alias = model_alias\nself.create_wandb_table()\n@abstractmethod\ndef preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]:\n\"\"\"This is an abstract method that would hold the custom preprocessing logic that would\n        preprocess a `PIL.Image` and add a batch dimension.\n        Args:\n            image (PIL.Image): A PIL Image.\n        Returns:\n            (Union[np.ndarray, tf.Tensor]): A numpy or Tensorflow tensor that would be fed to\n                the model.\n        \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.preprocess\")\n@abstractmethod\ndef postprocess(self, model_output: np.ndarray) -&gt; Image:\n\"\"\"This is an abstract method that would hold the custom postprocessing logic that\n        would convert the output of the model to a `PIL.Image`.\n        Args:\n            model_output (np.ndarray): Output of the model.\n        Returns:\n            (PIL.Image): The model output postprocessed to a PIL Image.\n        \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.postprocess\")\ndef initialize_model_from_wandb_artifact(self, artifact_address: str) -&gt; None:\n\"\"\"Initialize a `tf.keras.Model` that is to be evaluated from a\n        [Weights &amp; Biases artifact](https://docs.wandb.ai/guides/artifacts).\n        Args:\n            artifact_address (str): Address to the Weights &amp; Biases artifact hosting the model to be\n                evaluated.\n        \"\"\"\nself.model_path = fetch_wandb_artifact(artifact_address, artifact_type=\"model\")\nself.model = tf.keras.models.load_model(self.model_path, compile=False)\ndef create_wandb_table(self):\ncolumns = [\"Input-Image\", \"Enhanced-Image\", \"Inference-Time\"]\ncolumns = columns + [\"Model-Alias\"] if self.model_alias is not None else columns\nself.wandb_table = wandb.Table(columns=columns)\ndef _infer_on_single_image(self, input_path: str, output_path: str):\ninput_image = Image.open(input_path).convert(\"RGB\")\nif self.resize_factor &gt; 1:\nwidth, height = input_image.size\nwidth = (width // self.resize_factor) * self.resize_factor\nheight = (height // self.resize_factor) * self.resize_factor\ninput_image = input_image.resize((width, height))\npreprocessed_input_image = self.preprocess(input_image)\nstart_time = time()\nmodel_output = self.model(preprocessed_input_image)\ninference_time = time() - start_time\npost_processed_image = self.postprocess(model_output.numpy())\nif output_path is not None:\npost_processed_image.save(post_processed_image)\ntable_data = [\nwandb.Image(input_image),\nwandb.Image(post_processed_image),\ninference_time,\n]\ntable_data = (\ntable_data + [self.model_alias]\nif self.model_alias is not None\nelse table_data\n)\nself.wandb_table.add_data(*table_data)\ndef infer(self, input_path: str, output_path: Optional[str] = None) -&gt; None:\n\"\"\"Perform inference on a single image or a directory of images. The images are logged\n        to a Weights &amp; Biases table in case it is called in the context of a\n        [run](https://docs.wandb.ai/guides/runs).\n        Args:\n            input_path (str): Path to the input image or directory of images.\n            output_path (Optional[str]): Path to the output directory where the enhanced images.\n        \"\"\"\nif os.path.isdir(input_path):\ninput_images = glob(os.path.join(input_path, \"*\"))\nfor input_image_path in tqdm(input_images):\noutput_path = (\nos.path.join(output_path, os.path.basename(input_image_path))\nif output_path is not None\nelse None\n)\nself._infer_on_single_image(\ninput_path=input_image_path, output_path=output_path\n)\nelse:\nself._infer_on_single_image(input_path=input_path, output_path=output_path)\nif wandb.run is not None:\nwandb.log({\"Inference\": self.wandb_table})\n</code></pre>"},{"location":"inference/base/#restorers.inference.base.BaseInferer.infer","title":"<code>infer(input_path, output_path=None)</code>","text":"<p>Perform inference on a single image or a directory of images. The images are logged to a Weights &amp; Biases table in case it is called in the context of a run.</p> <p>Parameters:</p> Name Type Description Default <code>input_path</code> <code>str</code> <p>Path to the input image or directory of images.</p> required <code>output_path</code> <code>Optional[str]</code> <p>Path to the output directory where the enhanced images.</p> <code>None</code> Source code in <code>restorers/inference/base.py</code> <pre><code>def infer(self, input_path: str, output_path: Optional[str] = None) -&gt; None:\n\"\"\"Perform inference on a single image or a directory of images. The images are logged\n    to a Weights &amp; Biases table in case it is called in the context of a\n    [run](https://docs.wandb.ai/guides/runs).\n    Args:\n        input_path (str): Path to the input image or directory of images.\n        output_path (Optional[str]): Path to the output directory where the enhanced images.\n    \"\"\"\nif os.path.isdir(input_path):\ninput_images = glob(os.path.join(input_path, \"*\"))\nfor input_image_path in tqdm(input_images):\noutput_path = (\nos.path.join(output_path, os.path.basename(input_image_path))\nif output_path is not None\nelse None\n)\nself._infer_on_single_image(\ninput_path=input_image_path, output_path=output_path\n)\nelse:\nself._infer_on_single_image(input_path=input_path, output_path=output_path)\nif wandb.run is not None:\nwandb.log({\"Inference\": self.wandb_table})\n</code></pre>"},{"location":"inference/base/#restorers.inference.base.BaseInferer.initialize_model_from_wandb_artifact","title":"<code>initialize_model_from_wandb_artifact(artifact_address)</code>","text":"<p>Initialize a <code>tf.keras.Model</code> that is to be evaluated from a Weights &amp; Biases artifact.</p> <p>Parameters:</p> Name Type Description Default <code>artifact_address</code> <code>str</code> <p>Address to the Weights &amp; Biases artifact hosting the model to be evaluated.</p> required Source code in <code>restorers/inference/base.py</code> <pre><code>def initialize_model_from_wandb_artifact(self, artifact_address: str) -&gt; None:\n\"\"\"Initialize a `tf.keras.Model` that is to be evaluated from a\n    [Weights &amp; Biases artifact](https://docs.wandb.ai/guides/artifacts).\n    Args:\n        artifact_address (str): Address to the Weights &amp; Biases artifact hosting the model to be\n            evaluated.\n    \"\"\"\nself.model_path = fetch_wandb_artifact(artifact_address, artifact_type=\"model\")\nself.model = tf.keras.models.load_model(self.model_path, compile=False)\n</code></pre>"},{"location":"inference/base/#restorers.inference.base.BaseInferer.postprocess","title":"<code>postprocess(model_output)</code>  <code>abstractmethod</code>","text":"<p>This is an abstract method that would hold the custom postprocessing logic that would convert the output of the model to a <code>PIL.Image</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>np.ndarray</code> <p>Output of the model.</p> required <p>Returns:</p> Type Description <code>PIL.Image</code> <p>The model output postprocessed to a PIL Image.</p> Source code in <code>restorers/inference/base.py</code> <pre><code>@abstractmethod\ndef postprocess(self, model_output: np.ndarray) -&gt; Image:\n\"\"\"This is an abstract method that would hold the custom postprocessing logic that\n    would convert the output of the model to a `PIL.Image`.\n    Args:\n        model_output (np.ndarray): Output of the model.\n    Returns:\n        (PIL.Image): The model output postprocessed to a PIL Image.\n    \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.postprocess\")\n</code></pre>"},{"location":"inference/base/#restorers.inference.base.BaseInferer.preprocess","title":"<code>preprocess(image)</code>  <code>abstractmethod</code>","text":"<p>This is an abstract method that would hold the custom preprocessing logic that would preprocess a <code>PIL.Image</code> and add a batch dimension.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>PIL.Image</code> <p>A PIL Image.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, tf.Tensor]</code> <p>A numpy or Tensorflow tensor that would be fed to the model.</p> Source code in <code>restorers/inference/base.py</code> <pre><code>@abstractmethod\ndef preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]:\n\"\"\"This is an abstract method that would hold the custom preprocessing logic that would\n    preprocess a `PIL.Image` and add a batch dimension.\n    Args:\n        image (PIL.Image): A PIL Image.\n    Returns:\n        (Union[np.ndarray, tf.Tensor]): A numpy or Tensorflow tensor that would be fed to\n            the model.\n    \"\"\"\nraise NotImplementedError(f\"{self.__class__.__name__ }.preprocess\")\n</code></pre>"},{"location":"inference/low_light/","title":"Low-light Inferer","text":""},{"location":"inference/low_light/#restorers.inference.low_light.LowLightInferer","title":"<code>LowLightInferer</code>","text":"<p>         Bases: <code>BaseInferer</code></p> <p>Inferer for low-light enhancement models.</p> <p>Usage:</p> <pre><code>import os\nimport wandb\nfrom restorers.inference import LowLightInferer\n# initialize a wandb run for inference\nwandb.init(project=\"low-light-enhancement\", job_type=\"inference\")\n# initialize the inferer\ninferer = LowLightInferer(\nresize_factor=1, model_alias=\"Zero-DCE\"\n)\n# intialize the model from wandb artifacts\ninferer.initialize_model_from_wandb_artifact(\n# This artifact address corresponds to a Zero-DCE model trained on the LoL dataset\n\"ml-colabs/low-light-enhancement/run_oaa25znm_model:v99\"\n)\n# infer on a directory of images\n# inferer.infer(\"./dark_images\")\n# or infer on a single image\ninferer.infer(sample_image)\n</code></pre> Examples <ul> <li>Inferece on your own images for low-light enhancement.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>Optional[tf.keras.Model]</code> <p>The model that is to be perform inference. Note that passing the model during initializing the evaluator is not compulsory. The model can also be set using the function <code>initialize_model_from_wandb_artifact</code>.</p> <code>None</code> <code>resize_factor</code> <code>Optional[int]</code> <p>The factor by which the input image should be resized for inference.</p> <code>1</code> <code>model_alias</code> <code>Optional[str]</code> <p>The alias of the model that is to be logged to Weights &amp; Biases. This is useful for qualitative comparison of results of multiple models.</p> <code>None</code> Source code in <code>restorers/inference/low_light.py</code> <pre><code>class LowLightInferer(BaseInferer):\n\"\"\"Inferer for low-light enhancement models.\n    Usage:\n    ```py\n    import os\n    import wandb\n    from restorers.inference import LowLightInferer\n    # initialize a wandb run for inference\n    wandb.init(project=\"low-light-enhancement\", job_type=\"inference\")\n    # initialize the inferer\n    inferer = LowLightInferer(\n    resize_factor=1, model_alias=\"Zero-DCE\"\n    )\n    # intialize the model from wandb artifacts\n    inferer.initialize_model_from_wandb_artifact(\n    # This artifact address corresponds to a Zero-DCE model trained on the LoL dataset\n    \"ml-colabs/low-light-enhancement/run_oaa25znm_model:v99\"\n    )\n    # infer on a directory of images\n    # inferer.infer(\"./dark_images\")\n    # or infer on a single image\n    inferer.infer(sample_image)\n    ```\n    ??? example \"Examples\"\n        - [Inferece on your own images for low-light enhancement](../../examples/inference_low_light).\n    Args:\n        model (Optional[tf.keras.Model]): The model that is to be perform inference. Note that\n            passing the model during initializing the evaluator is not compulsory. The model can\n            also be set using the function `initialize_model_from_wandb_artifact`.\n        resize_factor (Optional[int]): The factor by which the input image should be resized\n            for inference.\n        model_alias (Optional[str]): The alias of the model that is to be logged to\n            Weights &amp; Biases. This is useful for qualitative comparison of results of multiple\n            models.\n    \"\"\"\ndef __init__(\nself,\nmodel: Optional[tf.keras.Model] = None,\nresize_factor: Optional[int] = 1,\nmodel_alias: Optional[str] = None,\n) -&gt; None:\nsuper().__init__(model, resize_factor, model_alias)\ndef preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]:\n\"\"\"Preprocessing logic for preprocessing a `PIL.Image` and adding a batch dimension.\n        Args:\n            image (PIL.Image): A PIL Image.\n        Returns:\n            (Union[np.ndarray, tf.Tensor]): A numpy or Tensorflow tensor that would be fed to\n                the model.\n        \"\"\"\nimage = tf.keras.preprocessing.image.img_to_array(image)\nimage = image.astype(\"float32\") / 255.0\nreturn np.expand_dims(image, axis=0)\ndef postprocess(self, model_output: np.ndarray) -&gt; Image:\n\"\"\"Postprocessing logic for converting the output of the model to a `PIL.Image`.\n        Args:\n            model_output (np.ndarray): Output of the model.\n        Returns:\n            (PIL.Image): The model output postprocessed to a PIL Image.\n        \"\"\"\nmodel_output = model_output * 255.0\nmodel_output = model_output.clip(0, 255)\nimage = model_output[0].reshape(\n(np.shape(model_output)[1], np.shape(model_output)[2], 3)\n)\nreturn Image.fromarray(np.uint8(image))\n</code></pre>"},{"location":"inference/low_light/#restorers.inference.low_light.LowLightInferer.postprocess","title":"<code>postprocess(model_output)</code>","text":"<p>Postprocessing logic for converting the output of the model to a <code>PIL.Image</code>.</p> <p>Parameters:</p> Name Type Description Default <code>model_output</code> <code>np.ndarray</code> <p>Output of the model.</p> required <p>Returns:</p> Type Description <code>PIL.Image</code> <p>The model output postprocessed to a PIL Image.</p> Source code in <code>restorers/inference/low_light.py</code> <pre><code>def postprocess(self, model_output: np.ndarray) -&gt; Image:\n\"\"\"Postprocessing logic for converting the output of the model to a `PIL.Image`.\n    Args:\n        model_output (np.ndarray): Output of the model.\n    Returns:\n        (PIL.Image): The model output postprocessed to a PIL Image.\n    \"\"\"\nmodel_output = model_output * 255.0\nmodel_output = model_output.clip(0, 255)\nimage = model_output[0].reshape(\n(np.shape(model_output)[1], np.shape(model_output)[2], 3)\n)\nreturn Image.fromarray(np.uint8(image))\n</code></pre>"},{"location":"inference/low_light/#restorers.inference.low_light.LowLightInferer.preprocess","title":"<code>preprocess(image)</code>","text":"<p>Preprocessing logic for preprocessing a <code>PIL.Image</code> and adding a batch dimension.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>PIL.Image</code> <p>A PIL Image.</p> required <p>Returns:</p> Type Description <code>Union[np.ndarray, tf.Tensor]</code> <p>A numpy or Tensorflow tensor that would be fed to the model.</p> Source code in <code>restorers/inference/low_light.py</code> <pre><code>def preprocess(self, image: Image) -&gt; Union[np.ndarray, tf.Tensor]:\n\"\"\"Preprocessing logic for preprocessing a `PIL.Image` and adding a batch dimension.\n    Args:\n        image (PIL.Image): A PIL Image.\n    Returns:\n        (Union[np.ndarray, tf.Tensor]): A numpy or Tensorflow tensor that would be fed to\n            the model.\n    \"\"\"\nimage = tf.keras.preprocessing.image.img_to_array(image)\nimage = image.astype(\"float32\") / 255.0\nreturn np.expand_dims(image, axis=0)\n</code></pre>"},{"location":"losses/charbonnier_loss/","title":"Charbonnier Loss","text":""},{"location":"losses/charbonnier_loss/#restorers.losses.charbonnier_loss.CharbonnierLoss","title":"<code>CharbonnierLoss</code>","text":"<p>         Bases: <code>tf.keras.losses.Loss</code></p> <p>The Charbonnier implemented as a <code>tf.keras.losses.Loss</code>.</p> <p>The Charbonnier loss, also known as the \"smooth L1 loss,\" is a loss function that is used in image processing and computer vision tasks to balance the trade-off between the Mean Squared Error (MSE) and the Mean Absolute Error (MAE). It is defined as</p> \\[L=\\sqrt{\\left(\\left(x^{\\wedge} 2+\\varepsilon^{\\wedge} 2\\right)\\right)}\\] <p>where x is the error and \u03b5 is a small positive constant (typically on the order of 0.001). It is less sensitive to outliers than the mean squared error and less computationally expensive than the mean absolute error.</p> Examples <ul> <li>Training a supervised low-light enhancement model using MirNetv2..</li> </ul> <p>Parameters:</p> Name Type Description Default <code>epsilon</code> <code>float</code> <p>a small positive constant.</p> required Source code in <code>restorers/losses/charbonnier_loss.py</code> <pre><code>class CharbonnierLoss(tf.keras.losses.Loss):\n\"\"\"The Charbonnier implemented as a `tf.keras.losses.Loss`.\n    The Charbonnier loss, also known as the \"smooth L1 loss,\" is a loss function that is used in\n    image processing and computer vision tasks to balance the trade-off between the Mean Squared\n    Error (MSE) and the Mean Absolute Error (MAE). It is defined as\n    $$L=\\\\sqrt{\\\\left(\\\\left(x^{\\\\wedge} 2+\\\\varepsilon^{\\\\wedge} 2\\\\right)\\\\right)}$$\n    where x is the error and \u03b5 is a small positive constant (typically on the order of 0.001). It\n    is less sensitive to outliers than the mean squared error and less computationally expensive\n    than the mean absolute error.\n    ??? example \"Examples\"\n        - [Training a supervised low-light enhancement model using MirNetv2.](../../examples/train_mirnetv2).\n    Args:\n        epsilon (float): a small positive constant.\n    \"\"\"\ndef __init__(self, epsilon: float, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.epsilon = tf.convert_to_tensor(epsilon)\ndef call(self, y_true, y_pred):\nsquared_difference = tf.square(y_true - y_pred)\nreturn tf.reduce_mean(tf.sqrt(squared_difference + tf.square(self.epsilon)))\n</code></pre>"},{"location":"losses/psnr_loss/","title":"PSNR Loss","text":""},{"location":"losses/psnr_loss/#restorers.losses.psnr_loss.PSNRLoss","title":"<code>PSNRLoss</code>","text":"<p>         Bases: <code>tf.keras.losses.Loss</code></p> <p>Implementation of Negative Peak Signal-to-Noise Ratio Loss defined as follows:</p> \\[\\text { Loss }=-\\sum_{i=1}^2 \\operatorname{PSNR}\\left(\\left(R_i+X_i\\right), Y\\right)\\] <p>where...</p> <ul> <li>\\(X_i \\in \\mathbb{R}^{N \\times H \\times W \\times C}\\) is the input image</li> <li>\\(X_i \\in \\mathbb{R}^{N \\times H \\times W \\times C}\\) is the prediction</li> </ul> References <ol> <li>HINet: Half Instance Normalization Network for Image Restoration</li> </ol> Source code in <code>restorers/losses/psnr_loss.py</code> <pre><code>class PSNRLoss(tf.keras.losses.Loss):\n\"\"\"Implementation of Negative Peak Signal-to-Noise Ratio Loss defined as follows:\n    $$\\\\text { Loss }=-\\\\sum_{i=1}^2 \\\\operatorname{PSNR}\\\\left(\\\\left(R_i+X_i\\\\right), Y\\\\right)$$\n    where...\n    * $X_i \\\\in \\\\mathbb{R}^{N \\\\times H \\\\times W \\\\times C}$ is the input image\n    * $X_i \\\\in \\\\mathbb{R}^{N \\\\times H \\\\times W \\\\times C}$ is the prediction\n    ??? info \"References\"\n        1. [HINet: Half Instance Normalization Network for Image Restoration](https://arxiv.org/abs/2105.06086)\n    \"\"\"\ndef __init__(self, max_val: float = 1.0, *args, **kwargs):\nsuper().__init__(*args, **kwargs)\nself.max_val = max_val\ndef call(self, y_true, y_pred):\nreturn -tf.image.psnr(y_true, y_pred, max_val=self.max_val)\n</code></pre>"},{"location":"losses/spatial_consistency_loss/","title":"Spatial Consistency Loss","text":""},{"location":"losses/spatial_consistency_loss/#restorers.losses.spatial_consistency_loss.SpatialConsistencyLoss","title":"<code>SpatialConsistencyLoss</code>","text":"<p>         Bases: <code>tf.keras.losses.Loss</code></p> <p>The Spatial Consistency Loss implemented as a <code>tf.keras.losses.Loss</code>.</p> <p>The spatial consistency loss encourages spatial coherence of the enhanced image through preserving the difference of neighboring regions between the input image and its enhanced version. It is given by</p> \\[L_{s p a}=\\frac{1}{K} \\sum_{i=1}^K \\sum_{j \\in \\Omega(i)}\\left(\\left|\\left(Y_i-Y_j\\right)\\right|-\\left|\\left(I_i-I_j\\right)\\right|\\right)^2\\] <p>where...</p> <ul> <li>K is the number of local regions</li> <li>\\(\\Omega(i)\\) is the four neighboring regions (top, down, left, right) centered at the region i</li> <li>Y and I are denoted as the average intensity value of the local region in the enhanced version and input image respectively</li> </ul> Examples <ul> <li>Training an unsupervised low-light enhancement model using Zero-DCE..</li> <li>Training an unsupervised low-light enhancement model using Fast Zero-DCE..</li> </ul> References <ol> <li>Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement</li> <li>Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)</li> <li>Official PyTorch implementation of Zero-DCE</li> <li>Unofficial PyTorch implementation of Zero-DCE</li> <li>Tensorflow implementation of Zero-DCE</li> <li>Keras tutorial for implementing Zero-DCE</li> </ol> Source code in <code>restorers/losses/spatial_consistency_loss.py</code> <pre><code>class SpatialConsistencyLoss(tf.keras.losses.Loss):\n\"\"\"The Spatial Consistency Loss implemented as a `tf.keras.losses.Loss`.\n    The spatial consistency loss encourages spatial coherence of the enhanced image through\n    preserving the difference of neighboring regions between the input image and its enhanced\n    version. It is given by\n    $$L_{s p a}=\\\\frac{1}{K} \\\\sum_{i=1}^K \\\\sum_{j \\\\in \\\\Omega(i)}\\\\left(\\\\left|\\\\left(Y_i-Y_j\\\\right)\\\\right|-\\\\left|\\\\left(I_i-I_j\\\\right)\\\\right|\\\\right)^2$$\n    where...\n    * K is the number of local regions\n    * $\\\\Omega(i)$ is the four neighboring regions (top, down, left, right) centered at the region i\n    * Y and I are denoted as the average intensity value of the local region in the enhanced version and input image respectively\n    ??? example \"Examples\"\n        - [Training an unsupervised low-light enhancement model using Zero-DCE.](../../examples/train_zero_dce).\n        - [Training an unsupervised low-light enhancement model using Fast Zero-DCE.](../../examples/train_fast_zero_dce).\n    ??? info \"References\"\n        1. [Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement](https://arxiv.org/abs/2001.06826)\n        2. [Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Guo_Zero-Reference_Deep_Curve_CVPR_2020_supplemental.pdf)\n        3. [Official PyTorch implementation of Zero-DCE](https://github.com/Li-Chongyi/Zero-DCE/blob/master/Zero-DCE_code/Myloss.py#L29)\n        4. [Unofficial PyTorch implementation of Zero-DCE](https://github.com/bsun0802/Zero-DCE/blob/master/code/utils.py#L79-L109)\n        5. [Tensorflow implementation of Zero-DCE](https://github.com/tuvovan/Zero_DCE_TF/blob/master/src/loss.py#L38-L86)\n        6. [Keras tutorial for implementing Zero-DCE](https://keras.io/examples/vision/zero_dce/#spatial-consistency-loss)\n    \"\"\"\ndef __init__(self, **kwargs) -&gt; None:\nsuper().__init__(reduction=\"none\")\nself.left_kernel = tf.constant(\n[[[[0, 0, 0]], [[-1, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n)\nself.right_kernel = tf.constant(\n[[[[0, 0, 0]], [[0, 1, -1]], [[0, 0, 0]]]], dtype=tf.float32\n)\nself.up_kernel = tf.constant(\n[[[[0, -1, 0]], [[0, 1, 0]], [[0, 0, 0]]]], dtype=tf.float32\n)\nself.down_kernel = tf.constant(\n[[[[0, 0, 0]], [[0, 1, 0]], [[0, -1, 0]]]], dtype=tf.float32\n)\ndef call(self, y_true: tf.Tensor, y_pred: tf.Tensor) -&gt; tf.Tensor:\noriginal_mean = tf.reduce_mean(y_true, 3, keepdims=True)\nenhanced_mean = tf.reduce_mean(y_pred, 3, keepdims=True)\noriginal_pool = tf.nn.avg_pool2d(\noriginal_mean, ksize=4, strides=4, padding=\"VALID\"\n)\nenhanced_pool = tf.nn.avg_pool2d(\nenhanced_mean, ksize=4, strides=4, padding=\"VALID\"\n)\nd_original_left = tf.nn.conv2d(\noriginal_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n)\nd_original_right = tf.nn.conv2d(\noriginal_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n)\nd_original_up = tf.nn.conv2d(\noriginal_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n)\nd_original_down = tf.nn.conv2d(\noriginal_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n)\nd_enhanced_left = tf.nn.conv2d(\nenhanced_pool, self.left_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n)\nd_enhanced_right = tf.nn.conv2d(\nenhanced_pool, self.right_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n)\nd_enhanced_up = tf.nn.conv2d(\nenhanced_pool, self.up_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n)\nd_enhanced_down = tf.nn.conv2d(\nenhanced_pool, self.down_kernel, strides=[1, 1, 1, 1], padding=\"SAME\"\n)\nd_left = tf.square(d_original_left - d_enhanced_left)\nd_right = tf.square(d_original_right - d_enhanced_right)\nd_up = tf.square(d_original_up - d_enhanced_up)\nd_down = tf.square(d_original_down - d_enhanced_down)\nspatial_constancy_loss = tf.reduce_mean(d_left + d_right + d_up + d_down)\nreturn spatial_constancy_loss\n</code></pre>"},{"location":"losses/zero_reference/","title":"Zero Reference Losses","text":"Examples <ul> <li>Training an unsupervised low-light enhancement model using Zero-DCE..</li> <li>Training an unsupervised low-light enhancement model using Fast Zero-DCE..</li> </ul>"},{"location":"losses/zero_reference/#restorers.losses.zero_reference.color_constancy","title":"<code>color_constancy(x)</code>","text":"<p>An implementation of the Color Constancy Loss.</p> <p>The purpose of the Color Constancy Loss is to correct the potential color deviations in the enhanced image and also build the relations among the three adjusted channels. It is given by</p> \\[L_{c o l}=\\sum_{\\forall(p, q) \\in \\varepsilon}\\left(J^p-J^q\\right)^2, \\varepsilon=\\{(R, G),(R, B),(G, B)\\}\\] References <ol> <li>Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement</li> <li>Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)</li> <li>Official PyTorch implementation of Zero-DCE</li> <li>Tensorflow implementation of Zero-DCE</li> <li>Keras tutorial for implementing Zero-DCE</li> </ol> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>tf.Tensor</code> <p>image.</p> required <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>color constancy loss.</p> Source code in <code>restorers/losses/zero_reference.py</code> <pre><code>def color_constancy(x: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"An implementation of the Color Constancy Loss.\n    The purpose of the Color Constancy Loss is to correct the potential color deviations in the\n    enhanced image and also build the relations among the three adjusted channels. It is given by\n    $$L_{c o l}=\\\\sum_{\\\\forall(p, q) \\\\in \\\\varepsilon}\\\\left(J^p-J^q\\\\right)^2, \\\\varepsilon=\\\\{(R, G),(R, B),(G, B)\\\\}$$\n    ??? info \"References\"\n        1. [Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement](https://arxiv.org/abs/2001.06826)\n        2. [Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Guo_Zero-Reference_Deep_Curve_CVPR_2020_supplemental.pdf)\n        3. [Official PyTorch implementation of Zero-DCE](https://github.com/Li-Chongyi/Zero-DCE/blob/master/Zero-DCE_code/Myloss.py#L9)\n        4. [Tensorflow implementation of Zero-DCE](https://github.com/tuvovan/Zero_DCE_TF/blob/master/src/loss.py#L10)\n        5. [Keras tutorial for implementing Zero-DCE](https://keras.io/examples/vision/zero_dce/#color-constancy-loss)\n    Args:\n        x (tf.Tensor): image.\n    Returns:\n        (tf.Tensor): color constancy loss.\n    \"\"\"\nmean_rgb = tf.reduce_mean(x, axis=(1, 2), keepdims=True)\nmean_red, mean_green, mean_blue = tf.split(mean_rgb, 3, axis=3)\ndifference_red_green = tf.square(mean_red - mean_green)\ndifference_red_blue = tf.square(mean_red - mean_blue)\ndifference_green_blue = tf.square(mean_blue - mean_green)\nreturn tf.sqrt(\ntf.square(difference_red_green)\n+ tf.square(difference_red_blue)\n+ tf.square(difference_green_blue)\n)\n</code></pre>"},{"location":"losses/zero_reference/#restorers.losses.zero_reference.exposure_control_loss","title":"<code>exposure_control_loss(x, window_size=16, mean_val=0.6)</code>","text":"<p>An implementation of the Exposure Constancy Loss.</p> <p>The exposure control loss measures the distance between the average intensity value of a local region to the well-exposedness level E which is set within [0.4, 0.7]. It is given by</p> \\[L_{e x p}=\\frac{1}{M} \\sum_{k=1}^M\\left|Y_k-E\\right|\\] <p>Reference:</p> <ol> <li>Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement</li> <li>Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)</li> <li>Official PyTorch implementation of Zero-DCE</li> <li>Tensorflow implementation of Zero-DCE</li> <li>Keras tutorial for implementing Zero-DCE</li> </ol> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>tf.Tensor</code> <p>image.</p> required <code>window_size</code> <code>int</code> <p>The size of the window for each dimension of the input tensor for average pooling.</p> <code>16</code> <code>mean_val</code> <code>int</code> <p>The average intensity value of a local region to the well-exposedness level.</p> <code>0.6</code> <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>exposure control loss.</p> Source code in <code>restorers/losses/zero_reference.py</code> <pre><code>def exposure_control_loss(\nx: tf.Tensor, window_size: int = 16, mean_val: float = 0.6\n) -&gt; tf.Tensor:\n\"\"\"An implementation of the Exposure Constancy Loss.\n    The exposure control loss measures the distance between the average intensity value of a local\n    region to the well-exposedness level E which is set within [0.4, 0.7]. It is given by\n    $$L_{e x p}=\\\\frac{1}{M} \\\\sum_{k=1}^M\\\\left|Y_k-E\\\\right|$$\n    Reference:\n    1. [Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement](https://arxiv.org/abs/2001.06826)\n    2. [Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Guo_Zero-Reference_Deep_Curve_CVPR_2020_supplemental.pdf)\n    3. [Official PyTorch implementation of Zero-DCE](https://github.com/Li-Chongyi/Zero-DCE/blob/master/Zero-DCE_code/Myloss.py#L74)\n    4. [Tensorflow implementation of Zero-DCE](https://github.com/tuvovan/Zero_DCE_TF/blob/master/src/loss.py#L21)\n    5. [Keras tutorial for implementing Zero-DCE](https://keras.io/examples/vision/zero_dce/#exposure-loss)\n    Args:\n        x (tf.Tensor): image.\n        window_size (int): The size of the window for each dimension of the input tensor for average pooling.\n        mean_val (int): The average intensity value of a local region to the well-exposedness level.\n    Returns:\n        (tf.Tensor): exposure control loss.\n    \"\"\"\nx = tf.reduce_mean(x, axis=-1, keepdims=True)\nmean = tf.nn.avg_pool2d(x, ksize=window_size, strides=window_size, padding=\"VALID\")\nreturn tf.reduce_mean(tf.square(mean - mean_val))\n</code></pre>"},{"location":"losses/zero_reference/#restorers.losses.zero_reference.illumination_smoothness_loss","title":"<code>illumination_smoothness_loss(x)</code>","text":"<p>An implementation of the Illumination Smoothness Loss.</p> <p>The purpose of the illumination smoothness loss is to preserve the monotonicity relations between neighboring pixels and it is applied to each curve parameter map. It is given by</p> \\[L_{t v_{\\mathcal{A}}}=\\frac{1}{N} \\sum_{n=1}^N \\sum_{c \\in \\xi}\\left(\\left|\\nabla_x \\mathcal{A}_n^c\\right|+\\nabla_y \\mathcal{A}_n^c \\mid\\right)^2, \\xi=\\{R, G, B\\}\\] <p>Reference:</p> <ol> <li>Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement</li> <li>Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)</li> <li>Official PyTorch implementation of Zero-DCE</li> <li>Tensorflow implementation of Zero-DCE</li> <li>Keras tutorial for implementing Zero-DCE</li> </ol> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>tf.Tensor</code> <p>image.</p> required <p>Returns:</p> Type Description <code>tf.Tensor</code> <p>illumination smoothness loss.</p> Source code in <code>restorers/losses/zero_reference.py</code> <pre><code>def illumination_smoothness_loss(x: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"An implementation of the Illumination Smoothness Loss.\n    The purpose of the illumination smoothness loss is to preserve the monotonicity relations between\n    neighboring pixels and it is applied to each curve parameter map. It is given by\n    $$L_{t v_{\\\\mathcal{A}}}=\\\\frac{1}{N} \\\\sum_{n=1}^N \\\\sum_{c \\\\in \\\\xi}\\\\left(\\\\left|\\\\nabla_x \\\\mathcal{A}_n^c\\\\right|+\\\\nabla_y \\\\mathcal{A}_n^c \\\\mid\\\\right)^2, \\\\xi=\\\\{R, G, B\\\\}$$\n    Reference:\n    1. [Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement](https://arxiv.org/abs/2001.06826)\n    2. [Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Guo_Zero-Reference_Deep_Curve_CVPR_2020_supplemental.pdf)\n    3. [Official PyTorch implementation of Zero-DCE](https://github.com/Li-Chongyi/Zero-DCE/blob/master/Zero-DCE_code/Myloss.py#L90)\n    4. [Tensorflow implementation of Zero-DCE](https://github.com/tuvovan/Zero_DCE_TF/blob/master/src/loss.py#L28)\n    5. [Keras tutorial for implementing Zero-DCE](https://keras.io/examples/vision/zero_dce/#illumination-smoothness-loss)\n    Args:\n        x (tf.Tensor): image.\n    Returns:\n        (tf.Tensor): illumination smoothness loss.\n    \"\"\"\nbatch_size = tf.shape(x)[0]\nh_x = tf.shape(x)[1]\nw_x = tf.shape(x)[2]\ncount_h = (tf.shape(x)[2] - 1) * tf.shape(x)[3]\ncount_w = tf.shape(x)[2] * (tf.shape(x)[3] - 1)\nh_tv = tf.reduce_sum(tf.square((x[:, 1:, :, :] - x[:, : h_x - 1, :, :])))\nw_tv = tf.reduce_sum(tf.square((x[:, :, 1:, :] - x[:, :, : w_x - 1, :])))\nbatch_size = tf.cast(batch_size, dtype=tf.float32)\ncount_h = tf.cast(count_h, dtype=tf.float32)\ncount_w = tf.cast(count_w, dtype=tf.float32)\nreturn 2 * (h_tv / count_h + w_tv / count_w) / batch_size\n</code></pre>"},{"location":"models/mirnetv2/","title":"MirNetV2","text":""},{"location":"models/mirnetv2/#restorers.model.mirnetv2.mirnet.MirNetv2","title":"<code>MirNetv2</code>","text":"<p>         Bases: <code>tf.keras.Model</code></p> <p>Implementation of the MirNetv2 model.</p> <p>MirNetv2 is a fully convolutional architecture that learns enriched feature representations for image restoration and enhancement. It is based on a recursive residual design with the multi-scale residual block or MRB at its core. The main branch of the MRB is dedicated to maintaining spatially-precise high-resolution representations through the entire network and the complimentary set of parallel branches provide better contextualized features.</p> <p></p> Examples <ul> <li>Training a supervised low-light enhancement model using MirNetv2.</li> </ul> <p>References</p> <ol> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of channels in the feature map.</p> required <code>channel_factor</code> <code>float</code> <p>factor by which number of the number of output channels vary.</p> required <code>num_mrb_blocks</code> <code>int</code> <p>number of multi-scale residual blocks.</p> required <code>add_residual_connection</code> <code>bool</code> <p>add a residual connection between the inputs and the outputs or not.</p> required Source code in <code>restorers/model/mirnetv2/mirnet.py</code> <pre><code>class MirNetv2(tf.keras.Model):\n\"\"\"Implementation of the MirNetv2 model.\n    MirNetv2 is a fully convolutional architecture that learns enriched feature\n    representations for image restoration and enhancement. It is based on a\n    **recursive residual design** with the **multi-scale residual block** or **MRB**\n    at its core. The main branch of the MRB is dedicated to maintaining spatially-precise\n    high-resolution representations through the entire network and the complimentary set\n    of parallel branches provide better contextualized features.\n    ![The MirNetv2 Architecture](https://i.imgur.com/oCIo69j.png){ loading=lazy }\n    ??? example \"Examples\"\n        - [Training a supervised low-light enhancement model using MirNetv2](../../examples/train_mirnetv2).\n    !!! info \"References\"\n        1. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n        2. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L242)\n    Args:\n        channels (int): number of channels in the feature map.\n        channel_factor (float): factor by which number of the number of output channels vary.\n        num_mrb_blocks (int): number of multi-scale residual blocks.\n        add_residual_connection (bool): add a residual connection between the inputs and the\n            outputs or not.\n    \"\"\"\ndef __init__(\nself,\nchannels: int,\nchannel_factor: float,\nnum_mrb_blocks: int,\nadd_residual_connection: bool,\n*args,\n**kwargs\n) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.channels = channels\nself.channel_factor = channel_factor\nself.num_mrb_blocks = num_mrb_blocks\nself.add_residual_connection = add_residual_connection\nself.conv_in = tf.keras.layers.Conv2D(channels, kernel_size=3, padding=\"same\")\nself.rrg_block_1 = RecursiveResidualGroup(\nchannels, num_mrb_blocks, channel_factor, groups=1\n)\nself.rrg_block_2 = RecursiveResidualGroup(\nchannels, num_mrb_blocks, channel_factor, groups=2\n)\nself.rrg_block_3 = RecursiveResidualGroup(\nchannels, num_mrb_blocks, channel_factor, groups=4\n)\nself.rrg_block_4 = RecursiveResidualGroup(\nchannels, num_mrb_blocks, channel_factor, groups=4\n)\nself.conv_out = tf.keras.layers.Conv2D(3, kernel_size=3, padding=\"same\")\ndef call(self, inputs: tf.Tensor, training=None, mask=None) -&gt; tf.Tensor:\nshallow_features = self.conv_in(inputs)\ndeep_features = self.rrg_block_1(shallow_features)\ndeep_features = self.rrg_block_2(deep_features)\ndeep_features = self.rrg_block_3(deep_features)\ndeep_features = self.rrg_block_4(deep_features)\noutput = self.conv_out(deep_features)\noutput = output + inputs if self.add_residual_connection else output\nreturn output\ndef save(self, filepath: str, *args, **kwargs) -&gt; None:\ninput_tensor = tf.keras.Input(shape=[None, None, 3])\nsaved_model = tf.keras.Model(\ninputs=input_tensor, outputs=self.call(input_tensor)\n)\nsaved_model.save(filepath, *args, **kwargs)\ndef get_config(self) -&gt; Dict:\nreturn {\n\"channels\": self.channels,\n\"num_mrb_blocks\": self.num_mrb_blocks,\n\"channel_factor\": self.channel_factor,\n\"add_residual_connection\": self.add_residual_connection,\n}\n</code></pre>"},{"location":"models/mirnetv2/#mirnetv2-blocks","title":"MirNetV2 Blocks","text":""},{"location":"models/mirnetv2/#restorers.model.mirnetv2.downsample.DownBlock","title":"<code>DownBlock</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Submodule of <code>DownSampleBlock</code>.</p> <p>References</p> <ol> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of input channels.</p> required <code>channel_factor</code> <code>float</code> <p>factor by which number of the number of output channels vary.</p> required Source code in <code>restorers/model/mirnetv2/downsample.py</code> <pre><code>class DownBlock(tf.keras.layers.Layer):\n\"\"\"Submodule of `DownSampleBlock`.\n    !!! info \"References\"\n        1. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n        2. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L130)\n    Args:\n        channels (int): number of input channels.\n        channel_factor (float): factor by which number of the number of output channels vary.\n    \"\"\"\ndef __init__(self, channels: int, channel_factor: float, *args, **kwargs) -&gt; None:\nsuper(DownBlock, self).__init__(*args, **kwargs)\nself.channels = channels\nself.channel_factor = channel_factor\nself.average_pool = tf.keras.layers.AveragePooling2D(pool_size=2, strides=2)\nself.conv = tf.keras.layers.Conv2D(\nint(channels * channel_factor), kernel_size=1, strides=1, padding=\"same\"\n)\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\nreturn self.conv(self.average_pool(inputs))\ndef get_config(self) -&gt; Dict:\nreturn {\"channels\": self.channels, \"channel_factor\": self.channel_factor}\n</code></pre>"},{"location":"models/mirnetv2/#restorers.model.mirnetv2.downsample.DownSampleBlock","title":"<code>DownSampleBlock</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Layer for downsampling feature map for the Multi-scale Residual Block.</p> References <ol> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of input channels.</p> required <code>scale_factor</code> <code>int</code> <p>number of downsample operations.</p> required <code>channel_factor</code> <code>float</code> <p>factor by which number of the number of output channels vary.</p> required Source code in <code>restorers/model/mirnetv2/downsample.py</code> <pre><code>class DownSampleBlock(tf.keras.layers.Layer):\n\"\"\"Layer for downsampling feature map for the Multi-scale Residual Block.\n    ??? info \"References\"\n        1. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n        2. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L142)\n    Args:\n        channels (int): number of input channels.\n        scale_factor (int): number of downsample operations.\n        channel_factor (float): factor by which number of the number of output channels vary.\n    \"\"\"\ndef __init__(\nself, channels: int, scale_factor: int, channel_factor: float, *args, **kwargs\n) -&gt; None:\nsuper(DownSampleBlock, self).__init__(*args, **kwargs)\nself.channels = channels\nself.scale_factor = scale_factor\nself.channel_factor = channel_factor\nself.layers = []\nfor _ in range(int(np.log2(scale_factor))):\nself.layers.append(DownBlock(channels, channel_factor))\nchannels = int(channels * channel_factor)\ndef call(self, x: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\nfor layer in self.layers:\nx = layer(x)\nreturn x\ndef get_config(self) -&gt; Dict:\nreturn {\n\"channels\": self.channels,\n\"channel_factor\": self.channel_factor,\n\"scale_factor\": self.scale_factor,\n}\n</code></pre>"},{"location":"models/mirnetv2/#restorers.model.mirnetv2.upsample.UpBlock","title":"<code>UpBlock</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Submodule of <code>UpSampleBlock</code>.</p> <p>References</p> <ol> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of input channels.</p> required <code>channel_factor</code> <code>float</code> <p>factor by which number of the number of output channels vary.</p> required Source code in <code>restorers/model/mirnetv2/upsample.py</code> <pre><code>class UpBlock(tf.keras.layers.Layer):\n\"\"\"Submodule of `UpSampleBlock`.\n    !!! info \"References\"\n        1. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n        2. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L158)\n    Args:\n        channels (int): number of input channels.\n        channel_factor (float): factor by which number of the number of output channels vary.\n    \"\"\"\ndef __init__(self, channels: int, channel_factor: float, *args, **kwargs) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.channels = channels\nself.channel_factor = channel_factor\nself.conv = tf.keras.layers.Conv2D(\nint(channels // channel_factor), kernel_size=1, strides=1, padding=\"same\"\n)\nself.upsample = tf.keras.layers.UpSampling2D(size=2, interpolation=\"bilinear\")\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\nreturn self.upsample(self.conv(inputs))\ndef get_config(self) -&gt; Dict:\nreturn {\"channels\": self.channels, \"channel_factor\": self.channel_factor}\n</code></pre>"},{"location":"models/mirnetv2/#restorers.model.mirnetv2.upsample.UpSampleBlock","title":"<code>UpSampleBlock</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Layer for upsampling feature map for the Multi-scale Residual Block.</p> <p>Reference:</p> <ol> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of input channels.</p> required <code>scale_factor</code> <code>int</code> <p>number of downsample operations.</p> required <code>channel_factor</code> <code>float</code> <p>factor by which number of the number of output channels vary.</p> required Source code in <code>restorers/model/mirnetv2/upsample.py</code> <pre><code>class UpSampleBlock(tf.keras.layers.Layer):\n\"\"\"Layer for upsampling feature map for the Multi-scale Residual Block.\n    Reference:\n    1. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n    2. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L170)\n    Args:\n        channels (int): number of input channels.\n        scale_factor (int): number of downsample operations.\n        channel_factor (float): factor by which number of the number of output channels vary.\n    \"\"\"\ndef __init__(\nself, channels: int, scale_factor: int, channel_factor: float, *args, **kwargs\n) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.channels = channels\nself.scale_factor = scale_factor\nself.channel_factor = channel_factor\nself.layers = []\nfor _ in range(int(np.log2(scale_factor))):\nself.layers.append(UpBlock(channels, channel_factor))\nchannels = int(channels // channel_factor)\ndef call(self, x, *args, **kwargs):\nfor layer in self.layers:\nx = layer(x)\nreturn x\ndef get_config(self) -&gt; Dict:\nreturn {\n\"channels\": self.channels,\n\"scale_factor\": self.scale_factor,\n\"channel_factor\": self.channel_factor,\n}\n</code></pre>"},{"location":"models/mirnetv2/#restorers.model.mirnetv2.skff.SelectiveKernelFeatureFusion","title":"<code>SelectiveKernelFeatureFusion</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Implementation of the Selective Kernel Feature Fusion Layer.</p> <p>This layer adaptively adjusts the input receptive fields by using multi-scale feature generation (in the same layer) followed by feature aggregation and selection. This is done using two distinct operations:</p> <ul> <li>Fuse Operation: The fuse operator generates global feature descriptors by     combining the information from multiresolution streams.</li> <li>Select Operation: The select operator uses the feature descriptors     generated by the fuse operator to recalibrate the feature maps     (of different streams) followed by their aggregation.</li> </ul> <p></p> <p>References</p> <ol> <li>Selective Kernel Networks</li> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of channels in the feature map.</p> required Source code in <code>restorers/model/mirnetv2/skff.py</code> <pre><code>class SelectiveKernelFeatureFusion(tf.keras.layers.Layer):\n\"\"\"Implementation of the Selective Kernel Feature Fusion Layer.\n    This layer adaptively adjusts the input receptive fields by using multi-scale\n    feature generation (in the same layer) followed by feature aggregation and\n    selection. This is done using two distinct operations:\n    - **Fuse Operation:** The fuse operator generates global feature descriptors by\n        combining the information from multiresolution streams.\n    - **Select Operation:** The select operator uses the feature descriptors\n        generated by the fuse operator to recalibrate the feature maps\n        (of different streams) followed by their aggregation.\n    ![Selective Kernel Feature Fusion](https://i.imgur.com/kVn5N8t.png){ loading=lazy }\n    !!! info \"References\"\n        1. [Selective Kernel Networks](https://arxiv.org/abs/1903.06586)\n        2. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n        3. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L17)\n    Args:\n        channels (int): number of channels in the feature map.\n    \"\"\"\ndef __init__(self, channels: int, *args, **kwargs) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.channels = channels\nself.hidden_channels = max(int(self.channels / 8), 4)\nself.average_pooling = tf.keras.layers.GlobalAveragePooling2D(keepdims=True)\nself.conv_channel_downscale = tf.keras.layers.Conv2D(\nself.hidden_channels, kernel_size=1, padding=\"same\"\n)\nself.conv_attention_1 = tf.keras.layers.Conv2D(\nself.channels, kernel_size=1, strides=1, padding=\"same\"\n)\nself.conv_attention_2 = tf.keras.layers.Conv2D(\nself.channels, kernel_size=1, strides=1, padding=\"same\"\n)\nself.softmax = tf.keras.layers.Softmax(axis=-1)\ndef call(\nself, inputs: Tuple[tf.Tensor], training: Optional[bool] = None\n) -&gt; tf.Tensor:\n# Fuse operation\ncombined_input_features = inputs[0] + inputs[1]\nchannel_wise_statistics = self.average_pooling(combined_input_features)\ndownscaled_channel_wise_statistics = self.conv_channel_downscale(\nchannel_wise_statistics\n)\nattention_vector_1 = self.softmax(\nself.conv_attention_1(downscaled_channel_wise_statistics)\n)\nattention_vector_2 = self.softmax(\nself.conv_attention_2(downscaled_channel_wise_statistics)\n)\n# Select operation\nselected_features = (\ninputs[0] * attention_vector_1 + inputs[1] * attention_vector_2\n)\nreturn selected_features\ndef get_config(self) -&gt; Dict:\nreturn {\"channels\": self.channels}\n</code></pre>"},{"location":"models/mirnetv2/#restorers.model.mirnetv2.rcb.ContextBlock","title":"<code>ContextBlock</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Submodule of the Residual Contextual Block.</p> <p>References</p> <ol> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of channels in the feature map.</p> required Source code in <code>restorers/model/mirnetv2/rcb.py</code> <pre><code>class ContextBlock(tf.keras.layers.Layer):\n\"\"\"Submodule of the Residual Contextual Block.\n    !!! info \"References\"\n        1. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n        2. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L57)\n    Args:\n        channels (int): number of channels in the feature map.\n    \"\"\"\ndef __init__(self, channels: int, *args, **kwargs) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.channels = channels\nself.mask_conv = tf.keras.layers.Conv2D(1, kernel_size=1, padding=\"same\")\nself.channel_add_conv_1 = tf.keras.layers.Conv2D(\nchannels, kernel_size=1, padding=\"same\"\n)\nself.channel_add_conv_2 = tf.keras.layers.Conv2D(\nchannels, kernel_size=1, padding=\"same\"\n)\nself.softmax = tf.keras.layers.Softmax(axis=1)\nself.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\ndef modeling(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n_, height, width, channels = [\ntf.shape(inputs)[_shape_idx] if _shape is None else _shape\nfor _shape_idx, _shape in enumerate(inputs.shape.as_list())\n]\nreshaped_inputs = tf.expand_dims(\ntf.reshape(inputs, (-1, channels, height * width)), axis=1\n)\ncontext_mask = self.mask_conv(inputs)\ncontext_mask = tf.reshape(context_mask, (-1, height * width, 1))\ncontext_mask = self.softmax(context_mask)\ncontext_mask = tf.expand_dims(context_mask, axis=1)\ncontext = tf.reshape(\ntf.matmul(reshaped_inputs, context_mask), (-1, 1, 1, channels)\n)\nreturn context\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\ncontext = self.modeling(inputs)\nchannel_add_term = self.channel_add_conv_1(context)\nchannel_add_term = self.leaky_relu(channel_add_term)\nchannel_add_term = self.channel_add_conv_2(channel_add_term)\nreturn inputs + channel_add_term\ndef get_config(self) -&gt; Dict:\nreturn {\"channels\": self.channels}\n</code></pre>"},{"location":"models/mirnetv2/#restorers.model.mirnetv2.rcb.ResidualContextBlock","title":"<code>ResidualContextBlock</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Implementation of the Residual Contextual Block.</p> <p>The Residual Contextual Block is used to extract features in the convolutional streams and suppress less useful features. The overall process of RCB is summarized as:</p> \\[F_{RCB} = F_{a} + W(CM(F_{b}))\\] <p>where...</p> <ul> <li> <p>\\(F_{a}\\) are the input feature maps.</p> </li> <li> <p>\\(F_{b}\\) represents feature maps that are obtained by applying two 3x3 group     convolution layers to the input features.</p> </li> <li> <p>\\(CM\\) respresents a contextual modules.</p> </li> <li> <p>\\(W\\) denotes the last convolutional layer with filter size \\(1      imes 1\\).</p> </li> </ul> <p></p> <p>Reference:</p> <ol> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of channels in the feature map.</p> required <code>groups</code> <code>int</code> <p>number of groups in which the input is split along the channel axis in the convolution layers.</p> required Source code in <code>restorers/model/mirnetv2/rcb.py</code> <pre><code>class ResidualContextBlock(tf.keras.layers.Layer):\n\"\"\"Implementation of the Residual Contextual Block.\n    The Residual Contextual Block is used to extract features in the convolutional\n    streams and suppress less useful features. The overall process of RCB is\n    summarized as:\n    $$F_{RCB} = F_{a} + W(CM(F_{b}))$$\n    where...\n    - $F_{a}$ are the input feature maps.\n    - $F_{b}$ represents feature maps that are obtained by applying two 3x3 group\n        convolution layers to the input features.\n    - $CM$ respresents a **contextual modules**.\n    - $W$ denotes the last convolutional layer with filter size $1 \\times 1$.\n    ![Residual Context Block](https://i.imgur.com/WM1s4IV.png){ loading=lazy }\n    Reference:\n    1. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n    2. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L105)\n    Args:\n        channels (int): number of channels in the feature map.\n        groups (int): number of groups in which the input is split along the\n            channel axis in the convolution layers.\n    \"\"\"\ndef __init__(self, channels: int, groups: int, *args, **kwargs) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.channels = channels\nself.groups = groups\nself.conv_1 = tf.keras.layers.Conv2D(\nchannels, kernel_size=3, padding=\"same\", groups=groups\n)\nself.conv_2 = tf.keras.layers.Conv2D(\nchannels, kernel_size=3, padding=\"same\", groups=groups\n)\nself.leaky_relu = tf.keras.layers.LeakyReLU(alpha=0.2)\nself.context_block = ContextBlock(channels=channels)\ndef call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\nx = self.conv_1(inputs)\nx = self.leaky_relu(x)\nx = self.conv_2(x)\nx = self.context_block(x)\nx = self.leaky_relu(x)\nx = x + inputs\nreturn x\ndef get_config(self) -&gt; Dict:\nreturn {\"channels\": self.channels, \"groups\": self.groups}\n</code></pre>"},{"location":"models/mirnetv2/#restorers.model.mirnetv2.mrb.MultiScaleResidualBlock","title":"<code>MultiScaleResidualBlock</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Implementation of the Multi-scale Residual Block.</p> <p>The Multi-scale Residual Block mechanism of collecting multiscale spatial information. This block forms the core component of the recursive residual design of MIRNet-v2. The key advantages of MRB are:</p> <ul> <li> <p>It is capable of generating a spatially-precise output by maintaining high-resolution representations, while receiving rich contextual information from low-resolutions.</p> </li> <li> <p>It allows contextualized-information transfer from the low-resolution streams to consolidate the high-resolution features.</p> </li> </ul> <p>References</p> <ol> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of channels in the feature map.</p> required <code>channel_factor</code> <code>float</code> <p>factor by which number of the number of output channels vary.</p> required <code>groups</code> <code>int</code> <p>number of groups in which the input is split along the channel axis in the convolution layers.</p> required Source code in <code>restorers/model/mirnetv2/mrb.py</code> <pre><code>class MultiScaleResidualBlock(tf.keras.layers.Layer):\n\"\"\"Implementation of the Multi-scale Residual Block.\n    The Multi-scale Residual Block mechanism of collecting multiscale spatial information.\n    This block forms the core component of the recursive residual design of MIRNet-v2.\n    The key advantages of MRB are:\n    - It is capable of generating a spatially-precise output by maintaining high-resolution representations, while receiving rich contextual information from low-resolutions.\n    - It allows contextualized-information transfer from the low-resolution streams to consolidate the high-resolution features.\n    !!! info \"References\"\n        1. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n        2. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L189)\n    Args:\n        channels (int): number of channels in the feature map.\n        channel_factor (float): factor by which number of the number of output channels vary.\n        groups (int): number of groups in which the input is split along the\n            channel axis in the convolution layers.\n    \"\"\"\ndef __init__(\nself, channels: int, channel_factor: float, groups: int, *args, **kwargs\n):\nsuper().__init__(*args, **kwargs)\nself.channels = channels\nself.channel_factor = channel_factor\nself.groups = groups\n# Residual Context Blocks\nself.rcb_top = ResidualContextBlock(\nint(channels * channel_factor**0), groups=groups\n)\nself.rcb_middle = ResidualContextBlock(\nint(channels * channel_factor**1), groups=groups\n)\nself.rcb_bottom = ResidualContextBlock(\nint(channels * channel_factor**2), groups=groups\n)\n# Downsample Blocks\nself.down_2 = DownSampleBlock(\nchannels=int((channel_factor**0) * channels),\nscale_factor=2,\nchannel_factor=channel_factor,\n)\nself.down_4_1 = DownSampleBlock(\nchannels=int((channel_factor**0) * channels),\nscale_factor=2,\nchannel_factor=channel_factor,\n)\nself.down_4_2 = DownSampleBlock(\nchannels=int((channel_factor**1) * channels),\nscale_factor=2,\nchannel_factor=channel_factor,\n)\n# UpSample Blocks\nself.up21_1 = UpSampleBlock(\nchannels=int((channel_factor**1) * channels),\nscale_factor=2,\nchannel_factor=channel_factor,\n)\nself.up21_2 = UpSampleBlock(\nchannels=int((channel_factor**1) * channels),\nscale_factor=2,\nchannel_factor=channel_factor,\n)\nself.up32_1 = UpSampleBlock(\nchannels=int((channel_factor**2) * channels),\nscale_factor=2,\nchannel_factor=channel_factor,\n)\nself.up32_2 = UpSampleBlock(\nchannels=int((channel_factor**2) * channels),\nscale_factor=2,\nchannel_factor=channel_factor,\n)\n# SKFF Blocks\nself.skff_top = SelectiveKernelFeatureFusion(\nchannels=int(channels * channel_factor**0)\n)\nself.skff_middle = SelectiveKernelFeatureFusion(\nchannels=int(channels * channel_factor**1)\n)\n# Convolution\nself.conv_out = tf.keras.layers.Conv2D(channels, kernel_size=1, padding=\"same\")\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\nx_top = inputs\nx_middle = self.down_2(x_top)\nx_bottom = self.down_4_2(self.down_4_1(x_top))\nx_top = self.rcb_top(x_top)\nx_middle = self.rcb_middle(x_middle)\nx_bottom = self.rcb_bottom(x_bottom)\nx_middle = self.skff_middle([x_middle, self.up32_1(x_bottom)])\nx_top = self.skff_top([x_top, self.up21_1(x_middle)])\nx_top = self.rcb_top(x_top)\nx_middle = self.rcb_middle(x_middle)\nx_bottom = self.rcb_bottom(x_bottom)\nx_middle = self.skff_middle([x_middle, self.up32_2(x_bottom)])\nx_top = self.skff_top([x_top, self.up21_2(x_middle)])\noutput = self.conv_out(x_top)\noutput = output + inputs\nreturn output\ndef get_config(self) -&gt; Dict:\nreturn {\n\"channels\": self.channels,\n\"channel_factor\": self.channel_factor,\n\"groups\": self.groups,\n}\n</code></pre>"},{"location":"models/mirnetv2/#restorers.model.mirnetv2.recursive_residual_group.RecursiveResidualGroup","title":"<code>RecursiveResidualGroup</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Implementation of the Recursive Residual Group.</p> <p>The Recursive Residual Group forms the basic building block on MirNetV2. It progressively breaks down the input signal in order to simplify the overall learning process, and allows the construction of very deep networks.</p> <p>References</p> <ol> <li>Learning Enriched Features for Fast Image Restoration and Enhancement</li> <li>Official PyTorch implementation of MirNetv2</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of channels in the feature map.</p> required <code>num_mrb_blocks</code> <code>int</code> <p>number of multi-scale residual blocks.</p> required <code>channel_factor</code> <code>float</code> <p>factor by which number of the number of output channels vary.</p> required <code>groups</code> <code>int</code> <p>number of groups in which the input is split along the channel axis in the convolution layers.</p> required Source code in <code>restorers/model/mirnetv2/recursive_residual_group.py</code> <pre><code>class RecursiveResidualGroup(tf.keras.layers.Layer):\n\"\"\"Implementation of the Recursive Residual Group.\n    The Recursive Residual Group forms the basic building block on MirNetV2.\n    It progressively breaks down the input signal in order to simplify the overall\n    learning process, and allows the construction of very deep networks.\n    !!! info \"References\"\n        1. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)\n        2. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L242)\n    Args:\n        channels (int): number of channels in the feature map.\n        num_mrb_blocks (int): number of multi-scale residual blocks.\n        channel_factor (float): factor by which number of the number of output channels vary.\n        groups (int): number of groups in which the input is split along the\n            channel axis in the convolution layers.\n    \"\"\"\ndef __init__(\nself,\nchannels: int,\nnum_mrb_blocks: int,\nchannel_factor: float,\ngroups: int,\n*args,\n**kwargs\n) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.channels = channels\nself.num_mrb_blocks = num_mrb_blocks\nself.channel_factor = channel_factor\nself.groups = groups\nself.layers = [\nMultiScaleResidualBlock(channels, channel_factor, groups)\nfor _ in range(num_mrb_blocks)\n]\nself.layers.append(\ntf.keras.layers.Conv2D(channels, kernel_size=3, strides=1, padding=\"same\")\n)\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\nresidual = inputs\nfor layer in self.layers:\nresidual = layer(residual)\nresidual = residual + inputs\nreturn residual\ndef get_config(self) -&gt; Dict:\nreturn {\n\"channels\": self.channels,\n\"num_mrb_blocks\": self.num_mrb_blocks,\n\"channel_factor\": self.channel_factor,\n\"groups\": self.groups,\n}\n</code></pre>"},{"location":"models/nafnet/","title":"NAFNet","text":""},{"location":"models/nafnet/#restorers.model.nafnet.nafnet.NAFNet","title":"<code>NAFNet</code>","text":"<p>         Bases: <code>keras.models.Model</code></p> <p>NAFNet</p> <p>The input channels will be mapped to the number of filters passed. After each down block, the number of filters will increase by a factor of 2. After each up block, the number of filters will decrease by a factor of 2. And finally the filters will be mapped back to the initial input size.</p> <p>Overwrite <code>create_encoder_and_down_blocks</code>, <code>create_decoder_and_up_blocks</code>, and <code>create_middle_blocks</code> to add your own implementation for these blocks. Overwrite get_blocks to use your custom block in NAFNet. But make sure to follow the restrictions on these methods and blocks.</p> <p></p> Examples <ul> <li>Training a supervised low-light enhancement model using NAFNet.</li> </ul> <p>References</p> <ol> <li>Simple Baselines for Image Restoration</li> <li>Official PyTorch implementation of NAFNet</li> </ol> <p>Parameters:</p> Name Type Description Default <code>filters</code> <code>Optional[int]</code> <p>denotes the starting filter size. Default filters' size is 16 .</p> <code>16</code> <code>middle_block_num</code> <code>Optional[int]</code> <p>denotes the number of middle blocks. Each middle block is a single NAFBlock unit. Default value is 1.</p> <code>1</code> <code>encoder_block_nums</code> <code>tuple</code> <p>the tuple size denotes the number of encoder blocks. Each tuple entry denotes the number of NAFBlocks in the corresponding encoder block. len(encoder_block_nums) should be the same as the len(decoder_block_nums) Default value is (1,1,1,1).</p> <code>(1, 1, 1, 1)</code> <code>decoder_block_nums</code> <code>tuple</code> <p>the tuple size denotes the number of decoder blocks. Each tuple entry denotes the number of NAFBlocks in the corresponding decoder block. len(decoder_block_nums) should be the same as the len(encoder_block_nums) Default value is (1,1,1,1).</p> <code>(1, 1, 1, 1)</code> <code>block_type</code> <code>str</code> <p>denotes what block to use in NAFNet Default block is 'nafblock'</p> <code>NAFBLOCK</code> Source code in <code>restorers/model/nafnet/nafnet.py</code> <pre><code>class NAFNet(keras.models.Model):\n\"\"\"\n    NAFNet\n    The input channels will be mapped to the number of filters passed.\n    After each down block, the number of filters will increase by a factor of 2.\n    After each up block, the number of filters will decrease by a factor of 2.\n    And finally the filters will be mapped back to the initial input size.\n    Overwrite `create_encoder_and_down_blocks`, `create_decoder_and_up_blocks`,\n    and `create_middle_blocks` to add your own implementation for these blocks.\n    Overwrite get_blocks to use your custom block in NAFNet. But make sure to follow\n    the restrictions on these methods and blocks.\n    ![](https://i.imgur.com/Ll017JJ.png)\n    ??? example \"Examples\"\n        - [Training a supervised low-light enhancement model using NAFNet](../../examples/train_nafnet).\n    !!! info \"References\"\n        1. [Simple Baselines for Image Restoration](https://arxiv.org/abs/2204.04676)\n        2. [Official PyTorch implementation of NAFNet](https://github.com/megvii-research/NAFNet)\n    Args:\n        filters (Optional[int]): denotes the starting filter size.\n            Default filters' size is 16 .\n        middle_block_num (Optional[int]): denotes the number of middle blocks.\n            Each middle block is a single NAFBlock unit. Default value is 1.\n        encoder_block_nums (tuple): the tuple size denotes the number of encoder blocks.\n            Each tuple entry denotes the number of NAFBlocks in the corresponding encoder block.\n            len(encoder_block_nums) should be the same as the len(decoder_block_nums)\n            Default value is (1,1,1,1).\n        decoder_block_nums (tuple): the tuple size denotes the number of decoder blocks.\n            Each tuple entry denotes the number of NAFBlocks in the corresponding decoder block.\n            len(decoder_block_nums) should be the same as the len(encoder_block_nums)\n            Default value is (1,1,1,1).\n        block_type (str): denotes what block to use in NAFNet\n            Default block is 'nafblock'\n    \"\"\"\ndef __init__(\nself,\nfilters: int = 16,\nmiddle_block_num: int = 1,\nencoder_block_nums: Tuple[int] = (1, 1, 1, 1),\ndecoder_block_nums: Tuple[int] = (1, 1, 1, 1),\nblock_type: str = NAFBLOCK,\n**kwargs,\n) -&gt; None:\nsuper().__init__(**kwargs)\nself.filters = filters\nself.middle_block_num = middle_block_num\nself.encoder_block_nums = encoder_block_nums\nself.decoder_block_nums = decoder_block_nums\nself.block_type = block_type\nself.intro = keras.layers.Conv2D(filters=filters, kernel_size=3, padding=\"same\")\nself.encoders = []\nself.decoders = []\nself.ups = []\nself.downs = []\nif len(encoder_block_nums) != len(decoder_block_nums):\nraise ValueError(\n\"The number of encoder blocks should match the number of decoder blocks\"\nf\"In the constructor {len(encoder_block_nums)} encoder blocks\"\nf\" and {len(decoder_block_nums)} were passed.\"\n)\nchannels = filters\nchannels = self.create_encoder_and_down_blocks(channels, encoder_block_nums)\nif len(self.encoders) != len(self.downs):\nraise ValueError(\n\"The number of encoder blocks should match the number of down blocks\"\nf\"In `create_encoder_and_down_blocks` {len(self.encoders)} encoder blocks\"\nf\" and {len(self.downs)} down blocks were created.\"\n)\nself.create_middle_blocks(middle_block_num)\nself.create_decoder_and_up_blocks(channels, decoder_block_nums)\nif len(self.decoders) != len(self.ups):\nraise ValueError(\n\"The number of decoder blocks should match the number of up blocks\"\nf\"In `create_decoder_and_up_blocks` {len(self.decoders)} decoder blocks\"\nf\" and {len(self.ups)} up blocks were created.\"\n)\nif len(encoder_block_nums) != len(decoder_block_nums):\nraise ValueError(\n\"The number of encoder blocks should match the number of decoder blocks\"\nf\"In `create_encoder_and_down_blocks` {len(self.encoders)} encoder blocks were created.\"\nf\"In `create_decoder_and_up_blocks` {len(self.decoders)} decoder blocks were created.\"\n)\n# The height and width of the image should be a\n#  multiple of self.expected_image_scale\n# If that is not the case, it will be fixed in the call(...) method.\nself.expected_image_scale = 2 ** len(self.encoders)\ndef build(self, input_shape: tf.TensorShape) -&gt; None:\ninput_channels = input_shape[-1]\nself.ending = keras.layers.Conv2DTranspose(\nfilters=input_channels, kernel_size=3, padding=\"same\"\n)\ndef get_block(self) -&gt; keras.layers.Layer:\n\"\"\"Returns the block to be used in NAFNet. This function can be overriden to use custom blocks\n        in NAFNet.\n        \"\"\"\nreturn NAFBlock(mode=self.block_type)\ndef create_encoder_and_down_blocks(\nself,\nchannels: int,\nencoder_block_nums: Tuple[int],\n) -&gt; int:\n\"\"\"Creates equal number of encoder blocks and down blocks.\"\"\"\nfor num in encoder_block_nums:\nself.encoders.append(\nkeras.models.Sequential([self.get_block() for _ in range(num)])\n)\nself.downs.append(\nkeras.layers.Conv2D(2 * channels, kernel_size=2, strides=2)\n)\nchannels *= 2\nreturn channels\ndef create_middle_blocks(self, middle_block_num: int) -&gt; None:\n\"\"\"Creates middle blocks in NAFNet\"\"\"\nself.middle_blocks = keras.models.Sequential(\n[self.get_block() for _ in range(middle_block_num)]\n)\ndef create_decoder_and_up_blocks(\nself,\nchannels: int,\ndecoder_block_nums: Tuple[int],\n) -&gt; int:\n\"\"\"Creates equal number of decoder blocks and up blocks.\"\"\"\nfor num in decoder_block_nums:\nself.ups.append(UpScale(2 * channels, pixel_shuffle_factor=2))\nchannels = channels // 2\nself.decoders.append(\nkeras.models.Sequential([self.get_block() for _ in range(num)])\n)\nreturn channels\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\n_, H, W, _ = inputs.shape\n# Scale the image to the next nearest multiple of self.expected_image_scale\ninputs = self.fix_input_shape(inputs)\nx = self.intro(inputs)\nencoder_outputs = []\nfor encoder, down in zip(self.encoders, self.downs):\nx = encoder(x)\nencoder_outputs.append(x)\nx = down(x)\nx = self.middle_blocks(x)\nfor decoder, up, encoder_output in zip(\nself.decoders, self.ups, encoder_outputs[::-1]\n):\nx = up(x)\n# Residual connection of encoder blocks with decoder blocks\nx = x + encoder_output\nx = decoder(x)\nx = self.ending(x)\n# Residual connection of inputs with output\nx = x + inputs\n# Crop back to the original size\nreturn x[:, :H, :W, :]\ndef fix_input_shape(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"Fixes input shape for NAFNet.\n        This is because NAFNet can only work with images whose shape is multiple of\n        2**(no. of encoder blocks). Hence the image is padded to match that shape.\n        \"\"\"\n_, H, W, _ = inputs.shape\n# Calculating how much padding is required\nheight_padding, width_padding = 0, 0\nif H % self.expected_image_scale != 0:\nheight_padding = self.expected_image_scale - H % self.expected_image_scale\nif W % self.expected_image_scale != 0:\nwidth_padding = self.expected_image_scale - W % self.expected_image_scale\npaddings = tf.constant(\n[[0, 0], [0, height_padding], [0, width_padding], [0, 0]]\n)\nreturn tf.pad(inputs, paddings)\ndef save(self, filepath: str, *args, **kwargs) -&gt; None:\ninput_tensor = tf.keras.Input(shape=[None, None, 3])\nsaved_model = tf.keras.Model(\ninputs=input_tensor, outputs=self.call(input_tensor)\n)\nsaved_model.save(filepath, *args, **kwargs)\ndef get_config(self) -&gt; dict:\nconfig = super().get_config()\nconfig.update(\n{\n\"filters\": self.filters,\n\"middle_block_num\": self.middle_block_num,\n\"encoder_block_nums\": self.encoder_block_nums,\n\"decoder_block_nums\": self.decoder_block_nums,\n\"block_type\": self.block_type,\n}\n)\nreturn config\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.nafnet.NAFNet.create_decoder_and_up_blocks","title":"<code>create_decoder_and_up_blocks(channels, decoder_block_nums)</code>","text":"<p>Creates equal number of decoder blocks and up blocks.</p> Source code in <code>restorers/model/nafnet/nafnet.py</code> <pre><code>def create_decoder_and_up_blocks(\nself,\nchannels: int,\ndecoder_block_nums: Tuple[int],\n) -&gt; int:\n\"\"\"Creates equal number of decoder blocks and up blocks.\"\"\"\nfor num in decoder_block_nums:\nself.ups.append(UpScale(2 * channels, pixel_shuffle_factor=2))\nchannels = channels // 2\nself.decoders.append(\nkeras.models.Sequential([self.get_block() for _ in range(num)])\n)\nreturn channels\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.nafnet.NAFNet.create_encoder_and_down_blocks","title":"<code>create_encoder_and_down_blocks(channels, encoder_block_nums)</code>","text":"<p>Creates equal number of encoder blocks and down blocks.</p> Source code in <code>restorers/model/nafnet/nafnet.py</code> <pre><code>def create_encoder_and_down_blocks(\nself,\nchannels: int,\nencoder_block_nums: Tuple[int],\n) -&gt; int:\n\"\"\"Creates equal number of encoder blocks and down blocks.\"\"\"\nfor num in encoder_block_nums:\nself.encoders.append(\nkeras.models.Sequential([self.get_block() for _ in range(num)])\n)\nself.downs.append(\nkeras.layers.Conv2D(2 * channels, kernel_size=2, strides=2)\n)\nchannels *= 2\nreturn channels\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.nafnet.NAFNet.create_middle_blocks","title":"<code>create_middle_blocks(middle_block_num)</code>","text":"<p>Creates middle blocks in NAFNet</p> Source code in <code>restorers/model/nafnet/nafnet.py</code> <pre><code>def create_middle_blocks(self, middle_block_num: int) -&gt; None:\n\"\"\"Creates middle blocks in NAFNet\"\"\"\nself.middle_blocks = keras.models.Sequential(\n[self.get_block() for _ in range(middle_block_num)]\n)\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.nafnet.NAFNet.fix_input_shape","title":"<code>fix_input_shape(inputs)</code>","text":"<p>Fixes input shape for NAFNet. This is because NAFNet can only work with images whose shape is multiple of 2**(no. of encoder blocks). Hence the image is padded to match that shape.</p> Source code in <code>restorers/model/nafnet/nafnet.py</code> <pre><code>def fix_input_shape(self, inputs: tf.Tensor) -&gt; tf.Tensor:\n\"\"\"Fixes input shape for NAFNet.\n    This is because NAFNet can only work with images whose shape is multiple of\n    2**(no. of encoder blocks). Hence the image is padded to match that shape.\n    \"\"\"\n_, H, W, _ = inputs.shape\n# Calculating how much padding is required\nheight_padding, width_padding = 0, 0\nif H % self.expected_image_scale != 0:\nheight_padding = self.expected_image_scale - H % self.expected_image_scale\nif W % self.expected_image_scale != 0:\nwidth_padding = self.expected_image_scale - W % self.expected_image_scale\npaddings = tf.constant(\n[[0, 0], [0, height_padding], [0, width_padding], [0, 0]]\n)\nreturn tf.pad(inputs, paddings)\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.nafnet.NAFNet.get_block","title":"<code>get_block()</code>","text":"<p>Returns the block to be used in NAFNet. This function can be overriden to use custom blocks in NAFNet.</p> Source code in <code>restorers/model/nafnet/nafnet.py</code> <pre><code>def get_block(self) -&gt; keras.layers.Layer:\n\"\"\"Returns the block to be used in NAFNet. This function can be overriden to use custom blocks\n    in NAFNet.\n    \"\"\"\nreturn NAFBlock(mode=self.block_type)\n</code></pre>"},{"location":"models/nafnet/#nafnet-blocks","title":"NAFNet Blocks","text":""},{"location":"models/nafnet/#restorers.model.nafnet.blocks.PixelShuffle","title":"<code>PixelShuffle</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>PixelShuffle Layer</p> <p>Given input of size (H,W,C), it will generate an output of size</p> \\[(H \\cdot f, W \\cdot f, \\frac{c}{f^2})\\] <p>Where \\(c\\) is channels and \\(f\\) is pixel_shuffle_factor</p> <p>While giving input, make sure that \\(f^2\\) divides \\(c\\).</p> <p>Wrapper Class for tf.nn.depth_to_space Reference: https://www.tensorflow.org/api_docs/python/tf/nn/depth_to_space</p> <p>Parameters:</p> Name Type Description Default <code>upscale_factor</code> <code>int</code> <p>the factor by which the input's spatial dimensions will be scaled.</p> required Source code in <code>restorers/model/nafnet/blocks.py</code> <pre><code>class PixelShuffle(tf.keras.layers.Layer):\n\"\"\"\n    PixelShuffle Layer\n    Given input of size (H,W,C), it will generate an output\n    of size\n    $$(H \\cdot f, W \\cdot f, \\\\frac{c}{f^2})$$\n    Where $c$ is channels and $f$ is pixel_shuffle_factor\n    While giving input, make sure that $f^2$ divides $c$.\n    Wrapper Class for tf.nn.depth_to_space\n    Reference: https://www.tensorflow.org/api_docs/python/tf/nn/depth_to_space\n    Args:\n        upscale_factor (int): the factor by which the input's spatial dimensions will be scaled.\n    \"\"\"\ndef __init__(self, upscale_factor: int, **kwargs) -&gt; None:\nsuper().__init__(**kwargs)\nself.upscale_factor = upscale_factor\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\nreturn tf.nn.depth_to_space(inputs, self.upscale_factor)\ndef get_config(self) -&gt; dict:\nconfig = super().get_config()\nconfig.update({\"upscale_factor\": self.upscale_factor})\nreturn config\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.blocks.UpScale","title":"<code>UpScale</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>UpScale Layer</p> <p>Given channels and pixel_shuffle_factor as input, it will generate an output of size</p> \\[(H \\cdot f, W \\cdot f, \\frac{c}{f^2})\\] <p>Where \\(c\\) is channels and \\(f\\) is pixel_shuffle_factor</p> <p>While giving input, make sure that \\(f^2\\) divides \\(c\\).</p> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of channels in the input.</p> required <code>pixel_shuffle_factor</code> <code>int</code> <p>the factor by which the input's spatial dimensions will be scaled.</p> required Source code in <code>restorers/model/nafnet/blocks.py</code> <pre><code>class UpScale(tf.keras.layers.Layer):\n\"\"\"\n    UpScale Layer\n    Given channels and pixel_shuffle_factor as input, it will generate an output\n    of size\n    $$(H \\cdot f, W \\cdot f, \\\\frac{c}{f^2})$$\n    Where $c$ is channels and $f$ is pixel_shuffle_factor\n    While giving input, make sure that $f^2$ divides $c$.\n    Args:\n        channels (int): number of channels in the input.\n        pixel_shuffle_factor (int): the factor by which the input's spatial dimensions will be scaled.\n    \"\"\"\ndef __init__(self, channels: int, pixel_shuffle_factor: int, **kwargs) -&gt; None:\nsuper().__init__(**kwargs)\nself.channels = channels\nself.pixel_shuffle_factor = pixel_shuffle_factor\nif channels % (pixel_shuffle_factor**2) != 0:\nraise ValueError(\nf\"Number of channels must divide square of pixel_shuffle_factor\"\nf\"In the constructor {channels} channels and \"\nf\"{pixel_shuffle_factor} pixel_shuffle_factor was passed\"\n)\nself.conv = tf.keras.layers.Conv2D(\nchannels, kernel_size=1, strides=1, use_bias=False\n)\nself.pixel_shuffle = PixelShuffle(pixel_shuffle_factor)\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\nreturn self.pixel_shuffle(self.conv(inputs))\ndef get_config(self) -&gt; dict:\nconfig = super().get_config()\nconfig.update(\n{\n\"channels\": self.channels,\n\"pixel_shuffle_factor\": self.pixel_shuffle_factor,\n}\n)\nreturn config\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.nafblock.ChannelAttention","title":"<code>ChannelAttention</code>","text":"<p>         Bases: <code>keras.layers.Layer</code></p> <p>Channel Attention layer</p> <p>The block is named Squeeze-and-Excitation block (SE Block) in the original paper.</p> <ol> <li> <p>First the input is 'squeezed' across the spatial dimension to generate a     channel-wise descriptor.</p> </li> <li> <p>Following that the inter channel dependency is learnt by applying     two convolution layers.</p> </li> <li> <p>Finally, the input is rescaled by a channel-wise multiplication with the     output of the excitation operation.</p> </li> </ol> <p>References</p> <ol> <li>Squeeze-and-Excitation Networks</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of channels in input</p> required Source code in <code>restorers/model/nafnet/nafblock.py</code> <pre><code>class ChannelAttention(keras.layers.Layer):\n\"\"\"\n    Channel Attention layer\n    The block is named Squeeze-and-Excitation block (SE Block) in the original paper.\n    1. First the input is 'squeezed' across the spatial dimension to generate a\n        channel-wise descriptor.\n    2. Following that the inter channel dependency is learnt by applying\n        two convolution layers.\n    3. Finally, the input is rescaled by a channel-wise multiplication with the\n        output of the excitation operation.\n    !!! info \"References\"\n        1. [Squeeze-and-Excitation Networks](https://arxiv.org/abs/1709.01507)\n    Args:\n        channels: number of channels in input\n    \"\"\"\ndef __init__(self, channels: int, **kwargs) -&gt; None:\nsuper().__init__(**kwargs)\nself.channels = channels\nself.avg_pool = keras.layers.GlobalAveragePooling2D()\nself.conv1 = keras.layers.Conv2D(\nfilters=channels // 2, kernel_size=1, activation=keras.activations.relu\n)\nself.conv2 = keras.layers.Conv2D(\nfilters=channels, kernel_size=1, activation=keras.activations.sigmoid\n)\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\naverage_pooling = self.avg_pool(inputs)\nfeature_descriptor = tf.reshape(\naverage_pooling, shape=(-1, 1, 1, self.channels)\n)\nx = self.conv1(feature_descriptor)\nreturn inputs * self.conv2(x)\ndef get_config(self) -&gt; dict:\nconfig = super().get_config()\nconfig.update({\"channels\": self.channels})\nreturn config\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.nafblock.NAFBlock","title":"<code>NAFBlock</code>","text":"<p>         Bases: <code>keras.layers.Layer</code></p> <p>NAFBlock (Nonlinear Activation Free Block)</p> <ul> <li>The authors first define a plain block by retaining the most used operations from the restormer block.</li> <li>In the plain block layer normalization and channel attention is added to make the baseline block.</li> <li>NAFBlock is constructed by removing all the non-linear activations from the baseline block.</li> </ul> <p>The authors have the idea that any operations of the form,</p> \\[f(X) \\cdot \\sigma(g(Y))\\] <p>(where f and g are feature maps and \\(\\sigma\\) is activation function) can be simplified to the form</p> \\[X \\cdot g(Y)\\] <p>Using this idea, all the nonlinear activations are replaced by     a series of Hadamard produces</p> <p>Reference:</p> <ol> <li>Simple Baselines for Image Restoration</li> </ol> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>Optional[float]</code> <p>factor by which the channels must be increased before being reduced by simple gate. (Higher factor denotes higher order polynomial in multiplication. Default factor is 2)</p> <code>2</code> <code>drop_out_rate</code> <code>Optional[float]</code> <p>dropout rate Default value is 0.0</p> <code>0.0</code> <code>balanced_skip_connection</code> <code>Optional[bool]</code> <p>adds additional trainable parameters to the skip connections. The parameter denotes how much importance should be given to the sub block in the skip connection. Default value is False</p> <code>False</code> <code>mode</code> <code>Optional[str]</code> <p>NAFBlock has 3 mode. 'plain' mode uses the PlainBlock.     It is derived from the restormer block, keeping the most common components 'baseline' mode used the BaselineBlock     It is derived by adding layer normalization, channel attention to PlainBlock.     It also replaces ReLU activation with GeLU in PlainBlock. 'nafblock' mode uses the NAFBlock     It derived from BaselineBlock by removing all the non-linear activation.     Non-linear activations are replaced by equivalent matrix multiplication operations. Default mode is 'nafblock'</p> <code>NAFBLOCK</code> Source code in <code>restorers/model/nafnet/nafblock.py</code> <pre><code>class NAFBlock(keras.layers.Layer):\n\"\"\"\n    NAFBlock (Nonlinear Activation Free Block)\n    - The authors first define a plain block by retaining the most used operations from the restormer block.\n    - In the plain block layer normalization and channel attention is added to make the baseline block.\n    - NAFBlock is constructed by removing all the non-linear activations from the baseline block.\n    The authors have the idea that any operations of the form,\n    $$f(X) \\cdot \\sigma(g(Y))$$\n    (where f and g are feature maps and $\\\\sigma$ is activation function)\n    can be simplified to the form\n    $$X \\cdot g(Y)$$\n    Using this idea, all the nonlinear activations are replaced by\n        a series of Hadamard produces\n    Reference:\n    1. [Simple Baselines for Image Restoration](https://arxiv.org/abs/2204.04676)\n    Args:\n        factor (Optional[float]): factor by which the channels must be increased before being reduced by simple gate.\n            (Higher factor denotes higher order polynomial in multiplication. Default factor is 2)\n        drop_out_rate (Optional[float]): dropout rate\n            Default value is 0.0\n        balanced_skip_connection (Optional[bool]): adds additional trainable parameters to the skip connections.\n            The parameter denotes how much importance should be given to the sub block in the skip connection.\n            Default value is False\n        mode (Optional[str]): NAFBlock has 3 mode.\n            'plain' mode uses the PlainBlock.\n                It is derived from the restormer block, keeping the most common components\n            'baseline' mode used the BaselineBlock\n                It is derived by adding layer normalization, channel attention to PlainBlock.\n                It also replaces ReLU activation with GeLU in PlainBlock.\n            'nafblock' mode uses the NAFBlock\n                It derived from BaselineBlock by removing all the non-linear activation.\n                Non-linear activations are replaced by equivalent matrix multiplication operations.\n            Default mode is 'nafblock'\n    \"\"\"\ndef __init__(\nself,\nfactor: int = 2,\ndrop_out_rate: float = 0.0,\nbalanced_skip_connection: bool = False,\nmode: str = NAFBLOCK,\n**kwargs\n) -&gt; None:\nsuper().__init__(**kwargs)\nself.factor = factor\nself.drop_out_rate = drop_out_rate\nself.balanced_skip_connection = balanced_skip_connection\nvalid_mode = {PLAIN, BASELINE, NAFBLOCK}\nif mode not in valid_mode:\nraise ValueError(\"Mode must be one of %r.\" % valid_mode)\nself.mode = mode\nif self.mode == PLAIN:\nself.activation = keras.layers.Activation(\"relu\")\nelif self.mode == BASELINE:\nself.activation = keras.layers.Activation(\"gelu\")\nelse:\nself.activation = SimpleGate(factor)\nself.dropout1 = keras.layers.Dropout(drop_out_rate)\nself.dropout2 = keras.layers.Dropout(drop_out_rate)\nself.layer_norm1 = None\nself.layer_norm2 = None\nif self.mode in [NAFBLOCK, BASELINE]:\nself.layer_norm1 = keras.layers.LayerNormalization()\nself.layer_norm2 = keras.layers.LayerNormalization()\ndef get_dw_channel(self, input_channels: int) -&gt; int:\nif self.mode == NAFBLOCK:\nreturn input_channels * self.factor\nelse:\nreturn input_channels\ndef get_ffn_channel(self, input_channels: int) -&gt; int:\nreturn input_channels * self.factor\ndef get_attention_layer(\nself, input_shape: tf.TensorShape\n) -&gt; Optional[keras.layers.Layer]:\ninput_channels = input_shape[-1]\nif self.mode == NAFBLOCK:\nreturn SimplifiedChannelAttention(input_channels)\nelif self.mode == BASELINE:\nreturn ChannelAttention(input_channels)\nelse:\nreturn None\ndef build(self, input_shape: tf.TensorShape) -&gt; None:\ninput_channels = input_shape[-1]\ndw_channel = self.get_dw_channel(input_channels)\nself.conv1 = keras.layers.Conv2D(filters=dw_channel, kernel_size=1, strides=1)\nself.dconv2 = keras.layers.Conv2D(\nfilters=dw_channel,\nkernel_size=3,\npadding=\"same\",\nstrides=1,\ngroups=dw_channel,\n)\nself.attention = self.get_attention_layer(input_shape)\nself.conv3 = keras.layers.Conv2D(\nfilters=input_channels, kernel_size=1, strides=1\n)\nffn_channel = self.get_ffn_channel(input_channels)\nself.conv4 = keras.layers.Conv2D(filters=ffn_channel, kernel_size=1, strides=1)\nself.conv5 = keras.layers.Conv2D(\nfilters=input_channels, kernel_size=1, strides=1\n)\nself.beta = tf.Variable(\ntf.ones((1, 1, 1, input_channels)), trainable=self.balanced_skip_connection\n)\nself.gamma = tf.Variable(\ntf.ones((1, 1, 1, input_channels)), trainable=self.balanced_skip_connection\n)\ndef call_block1(self, inputs: tf.Tensor) -&gt; tf.Tensor:\nx = inputs\nif self.layer_norm1 != None:\nx = self.layer_norm1(x)\nx = self.conv1(x)\nx = self.dconv2(x)\nx = self.activation(x)\nif self.attention != None:\nx = self.attention(x)\nx = self.conv3(x)\nx = self.dropout1(x)\nreturn x\ndef call_block2(self, inputs: tf.Tensor) -&gt; tf.Tensor:\ny = inputs\nif self.layer_norm2 != None:\ny = self.layer_norm2(y)\ny = self.conv4(y)\ny = self.activation(y)\ny = self.conv5(y)\ny = self.dropout2(y)\nreturn y\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\n# Block 1\nx = self.call_block1(inputs)\n# Residual connection\nx = inputs + self.beta * x\n# Block 2\ny = self.call_block2(x)\n# Residual connection\ny = x + self.gamma * y\nreturn y\ndef get_config(self) -&gt; dict:\nconfig = super().get_config()\nconfig.update(\n{\n\"factor\": self.factor,\n\"drop_out_rate\": self.drop_out_rate,\n\"balanced_skip_connection\": self.balanced_skip_connection,\n\"mode\": self.mode,\n}\n)\nreturn config\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.nafblock.SimpleGate","title":"<code>SimpleGate</code>","text":"<p>         Bases: <code>keras.layers.Layer</code></p> <p>Simple Gate It splits the input of size (b,h,w,c) into tensors of size (b,h,w,c//factor) and returns their Hadamard product</p> <p>References</p> <ol> <li>Simple Baselines for Image Restoration</li> </ol> <p>Parameters:</p> Name Type Description Default <code>factor</code> <code>Optional[int]</code> <p>the amount by which the channels are scaled down Default factor is 2.</p> <code>2</code> Source code in <code>restorers/model/nafnet/nafblock.py</code> <pre><code>class SimpleGate(keras.layers.Layer):\n\"\"\"\n    Simple Gate\n    It splits the input of size (b,h,w,c) into tensors of size (b,h,w,c//factor) and returns their Hadamard product\n    !!! info \"References\"\n        1. [Simple Baselines for Image Restoration](https://arxiv.org/abs/2204.04676)\n    Args:\n        factor (Optional[int]): the amount by which the channels are scaled down\n            Default factor is 2.\n    \"\"\"\ndef __init__(self, factor: int = 2, **kwargs) -&gt; None:\nsuper().__init__(**kwargs)\nself.factor = factor\ndef call(self, x: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\nx = tf.expand_dims(x, axis=-1)\nreturn tf.reduce_prod(\ntf.concat(tf.split(x, num_or_size_splits=self.factor, axis=-2), axis=-1),\naxis=-1,\n)\ndef get_config(self) -&gt; dict:\nconfig = super().get_config()\nconfig.update({\"factor\": self.factor})\nreturn config\n</code></pre>"},{"location":"models/nafnet/#restorers.model.nafnet.nafblock.SimplifiedChannelAttention","title":"<code>SimplifiedChannelAttention</code>","text":"<p>         Bases: <code>keras.layers.Layer</code></p> <p>Simplified Channel Attention layer It is a modification of channel attention without any non-linear activations.</p> <p>The Squeeze and final rescaling step is identical to the ChannelAttention Layer. But following the philosophy of NAFNet paper, the excitation operation with     two conv layers with respective activations are replaced with a single conv     block. So the inter channel dependency is learnt but any gate or activation     is not used.     (Check the paper/doc string of NAFBlock for more details)</p> <p>References</p> <ol> <li>Simple Baselines for Image Restoration</li> </ol> <p>Parameters:</p> Name Type Description Default <code>channels</code> <code>int</code> <p>number of channels in input</p> required Source code in <code>restorers/model/nafnet/nafblock.py</code> <pre><code>class SimplifiedChannelAttention(keras.layers.Layer):\n\"\"\"\n    Simplified Channel Attention layer\n    It is a modification of channel attention without any non-linear activations.\n    The Squeeze and final rescaling step is identical to the ChannelAttention Layer.\n    But following the philosophy of NAFNet paper, the excitation operation with\n        two conv layers with respective activations are replaced with a single conv\n        block. So the inter channel dependency is learnt but any gate or activation\n        is not used.\n        (Check the paper/doc string of NAFBlock for more details)\n    !!! info \"References\"\n        1. [Simple Baselines for Image Restoration](https://arxiv.org/abs/2204.04676)\n    Args:\n        channels: number of channels in input\n    \"\"\"\ndef __init__(self, channels: int, **kwargs) -&gt; None:\nsuper().__init__(**kwargs)\nself.channels = channels\nself.avg_pool = keras.layers.GlobalAveragePooling2D()\nself.conv = keras.layers.Conv2D(filters=channels, kernel_size=1)\ndef call(self, inputs: tf.Tensor, *args, **kwargs) -&gt; tf.Tensor:\naverage_pooling = self.avg_pool(inputs)\nfeature_descriptor = tf.reshape(\naverage_pooling, shape=(-1, 1, 1, self.channels)\n)\nfeatures = self.conv(feature_descriptor)\nreturn inputs * features\ndef get_config(self) -&gt; dict:\nconfig = super().get_config()\nconfig.update({\"channels\": self.channels})\nreturn config\n</code></pre>"},{"location":"models/zero_dce_model/","title":"Zero-DCE","text":""},{"location":"models/zero_dce_model/#restorers.model.zero_dce.zero_dce.ZeroDCE","title":"<code>ZeroDCE</code>","text":"<p>         Bases: <code>tf.keras.Model</code></p> <p>The Zero-reference Deep Curve Estimation (Zero-DCE) model implemented as a <code>tf.keras.Model</code>.</p> <p>Zero-reference deep curve estimation is a method for unsupervised low-light image enhancement that utilizes a deep learning model to estimate the enhancement curve for an image without any reference to the original, well-lit image. The model is trained on a dataset of low-light and normal-light images, and learns to predict the enhancement curve that will best improve the visual quality of the low-light image. Once the enhancement curve is estimated, it can be applied to the low-light image to enhance its visibility. This approach allows for real-time enhancement of low-light images without the need for a reference image, making it useful in situations where a reference image is not available or impractical to obtain.</p> <p></p> Examples <ul> <li>Training an unsupervised low-light enhancement model using Zero-DCE.</li> </ul> <p>References</p> <ol> <li>Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement</li> <li>Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)</li> <li>Official PyTorch implementation of Zero-DCE</li> <li>Unofficial PyTorch implementation of Zero-DCE</li> <li>Tensorflow implementation of Zero-DCE</li> <li>Keras tutorial for implementing Zero-DCE</li> </ol> <p>Parameters:</p> Name Type Description Default <code>num_intermediate_filters</code> <code>int</code> <p>number of filters in the intermediate convolutional layers.</p> required <code>num_iterations</code> <code>int</code> <p>number of iterations of enhancement.</p> required <code>decoder_channel_factor</code> <code>int</code> <p>factor by which number filters in the decoder of deep curve estimation layer is multiplied.</p> required Source code in <code>restorers/model/zero_dce/zero_dce.py</code> <pre><code>class ZeroDCE(tf.keras.Model):\n\"\"\"The Zero-reference Deep Curve Estimation (Zero-DCE) model implemented as a\n    `tf.keras.Model`.\n    Zero-reference deep curve estimation is a method for unsupervised low-light image enhancement\n    that utilizes a deep learning model to estimate the enhancement curve for an image without any\n    reference to the original, well-lit image. The model is trained on a dataset of low-light and\n    normal-light images, and learns to predict the enhancement curve that will best improve the visual\n    quality of the low-light image. Once the enhancement curve is estimated, it can be applied to the\n    low-light image to enhance its visibility. This approach allows for real-time enhancement of\n    low-light images without the need for a reference image, making it useful in situations where a\n    reference image is not available or impractical to obtain.\n    ![](https://li-chongyi.github.io/Zero-DCE_files/framework.png){ loading=lazy }\n    ??? example \"Examples\"\n        - [Training an unsupervised low-light enhancement model using Zero-DCE](../../examples/train_zero_dce).\n    !!! info \"References\"\n        1. [Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement](https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf)\n        2. [Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Guo_Zero-Reference_Deep_Curve_CVPR_2020_supplemental.pdf)\n        3. [Official PyTorch implementation of Zero-DCE](https://github.com/Li-Chongyi/Zero-DCE)\n        4. [Unofficial PyTorch implementation of Zero-DCE](https://github.com/bsun0802/Zero-DCE)\n        5. [Tensorflow implementation of Zero-DCE](https://github.com/tuvovan/Zero_DCE_TF)\n        6. [Keras tutorial for implementing Zero-DCE](https://keras.io/examples/vision/zero_dce/#deep-curve-estimation-model)\n    Args:\n        num_intermediate_filters (int): number of filters in the intermediate convolutional layers.\n        num_iterations (int): number of iterations of enhancement.\n        decoder_channel_factor (int): factor by which number filters in the decoder of deep curve\n            estimation layer is multiplied.\n    \"\"\"\ndef __init__(\nself,\nnum_intermediate_filters: int,\nnum_iterations: int,\ndecoder_channel_factor: int,\n*args,\n**kwargs\n) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.num_intermediate_filters = num_intermediate_filters\nself.num_iterations = num_iterations\nself.decoder_channel_factor = decoder_channel_factor\nself.deep_curve_estimation = DeepCurveEstimationLayer(\nnum_intermediate_filters=self.num_intermediate_filters,\nnum_iterations=self.num_iterations,\ndecoder_channel_factor=self.decoder_channel_factor,\n)\ndef compile(\nself,\nweight_exposure_loss: float,\nweight_color_constancy_loss: float,\nweight_illumination_smoothness_loss: float,\n*args,\n**kwargs\n) -&gt; None:\n\"\"\"Configures the model for training.\n        Example:\n        ```python\n        model.compile(\n            optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n            weight_exposure_loss=1.0,\n            weight_color_constancy_loss=0.5,\n            weight_illumination_smoothness_loss=20.0,\n        )\n        ```\n        Args:\n            weight_exposure_loss (float): weight of the exposure control loss.\n            weight_color_constancy_loss (float): weight of the color constancy loss.\n            weight_illumination_smoothness_loss (float): weight of the illumination smoothness loss.\n        \"\"\"\nsuper().compile(*args, **kwargs)\nself.weight_exposure_loss = weight_exposure_loss\nself.weight_color_constancy_loss = weight_color_constancy_loss\nself.weight_illumination_smoothness_loss = weight_illumination_smoothness_loss\nself.spatial_constancy_loss = SpatialConsistencyLoss()\ndef get_enhanced_image(\nself, data: tf.Tensor, output: tf.Tensor\n) -&gt; Tuple[tf.Tensor]:\ncurves = tf.split(output, self.num_iterations, axis=-1)\nenhanced_image = data\nfor idx in range(self.num_iterations):\nenhanced_image = enhanced_image + curves[idx] * (\ntf.square(enhanced_image) - enhanced_image\n)\nreturn enhanced_image\ndef call(self, data: tf.Tensor, training=None, mask=None) -&gt; Tuple[tf.Tensor]:\ndce_net_output = self.deep_curve_estimation(data)\nreturn self.get_enhanced_image(data, dce_net_output)\ndef compute_losses(\nself, data: tf.Tensor, output: tf.Tensor\n) -&gt; Dict[str, tf.Tensor]:\nenhanced_image = self.get_enhanced_image(data, output)\nloss_illumination = illumination_smoothness_loss(output)\nloss_spatial_constancy = tf.reduce_mean(\nself.spatial_constancy_loss(enhanced_image, data)\n)\nloss_color_constancy = tf.reduce_mean(color_constancy(enhanced_image))\nloss_exposure = tf.reduce_mean(exposure_control_loss(enhanced_image))\ntotal_loss = (\nloss_spatial_constancy\n+ self.weight_illumination_smoothness_loss * loss_illumination\n+ self.weight_color_constancy_loss * loss_color_constancy\n+ self.weight_exposure_loss * loss_exposure\n)\nreturn {\n\"total_loss\": total_loss,\n\"illumination_smoothness_loss\": loss_illumination,\n\"spatial_constancy_loss\": loss_spatial_constancy,\n\"color_constancy\": loss_color_constancy,\n\"exposure_control_loss\": loss_exposure,\n}\ndef train_step(self, data: tf.Tensor) -&gt; Dict[str, tf.Tensor]:\nwith tf.GradientTape() as tape:\noutput = self.deep_curve_estimation(data)\nlosses = self.compute_losses(data, output)\ngradients = tape.gradient(losses[\"total_loss\"], self.trainable_weights)\nself.optimizer.apply_gradients(zip(gradients, self.trainable_weights))\nreturn losses\ndef test_step(self, data: tf.Tensor) -&gt; Dict[str, tf.Tensor]:\noutput = self.deep_curve_estimation(data)\nreturn self.compute_losses(data, output)\ndef get_config(self) -&gt; Dict:\nreturn {\n\"num_intermediate_filters\": self.num_intermediate_filters,\n\"num_iterations\": self.num_iterations,\n\"decoder_channel_factor\": self.decoder_channel_factor,\n}\ndef save(self, filepath: str, *args, **kwargs) -&gt; None:\ninput_tensor = tf.keras.Input(shape=[None, None, 3])\nsaved_model = tf.keras.Model(\ninputs=input_tensor, outputs=self.call(input_tensor)\n)\nsaved_model.save(filepath, *args, **kwargs)\n</code></pre>"},{"location":"models/zero_dce_model/#restorers.model.zero_dce.zero_dce.ZeroDCE.compile","title":"<code>compile(weight_exposure_loss, weight_color_constancy_loss, weight_illumination_smoothness_loss, *args, **kwargs)</code>","text":"<p>Configures the model for training.</p> <p>Example:</p> <pre><code>model.compile(\noptimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\nweight_exposure_loss=1.0,\nweight_color_constancy_loss=0.5,\nweight_illumination_smoothness_loss=20.0,\n)\n</code></pre> <p>Parameters:</p> Name Type Description Default <code>weight_exposure_loss</code> <code>float</code> <p>weight of the exposure control loss.</p> required <code>weight_color_constancy_loss</code> <code>float</code> <p>weight of the color constancy loss.</p> required <code>weight_illumination_smoothness_loss</code> <code>float</code> <p>weight of the illumination smoothness loss.</p> required Source code in <code>restorers/model/zero_dce/zero_dce.py</code> <pre><code>def compile(\nself,\nweight_exposure_loss: float,\nweight_color_constancy_loss: float,\nweight_illumination_smoothness_loss: float,\n*args,\n**kwargs\n) -&gt; None:\n\"\"\"Configures the model for training.\n    Example:\n    ```python\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n        weight_exposure_loss=1.0,\n        weight_color_constancy_loss=0.5,\n        weight_illumination_smoothness_loss=20.0,\n    )\n    ```\n    Args:\n        weight_exposure_loss (float): weight of the exposure control loss.\n        weight_color_constancy_loss (float): weight of the color constancy loss.\n        weight_illumination_smoothness_loss (float): weight of the illumination smoothness loss.\n    \"\"\"\nsuper().compile(*args, **kwargs)\nself.weight_exposure_loss = weight_exposure_loss\nself.weight_color_constancy_loss = weight_color_constancy_loss\nself.weight_illumination_smoothness_loss = weight_illumination_smoothness_loss\nself.spatial_constancy_loss = SpatialConsistencyLoss()\n</code></pre>"},{"location":"models/zero_dce_model/#restorers.model.zero_dce.fast_zero_dce.FastZeroDce","title":"<code>FastZeroDce</code>","text":"<p>         Bases: <code>ZeroDCE</code></p> <p>A faster version of the Zero-DCE (Zero-DCE++) model implemented as a <code>tf.keras.Model</code>.</p> <p>Zero-reference deep curve estimation is a method for unsupervised low-light image enhancement that utilizes a deep learning model to estimate the enhancement curve for an image without any reference to the original, well-lit image. The model is trained on a dataset of low-light and normal-light images, and learns to predict the enhancement curve that will best improve the visual quality of the low-light image. Once the enhancement curve is estimated, it can be applied to the low-light image to enhance its visibility. This approach allows for real-time enhancement of low-light images without the need for a reference image, making it useful in situations where a reference image is not available or impractical to obtain.</p> Examples <ul> <li>Training an unsupervised low-light enhancement model using Fast Zero-DCE.</li> </ul> <p>References</p> <ol> <li>Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation</li> <li>Official PyTorch implementation of Zero-DCE++</li> </ol> <p>Parameters:</p> Name Type Description Default <code>num_intermediate_filters</code> <code>int</code> <p>number of filters in the intermediate convolutional layers.</p> required <code>num_iterations</code> <code>int</code> <p>number of iterations of enhancement.</p> required <code>decoder_channel_factor</code> <code>int</code> <p>factor by which number filters in the decoder of deep curve estimation layer is multiplied.</p> required Source code in <code>restorers/model/zero_dce/fast_zero_dce.py</code> <pre><code>class FastZeroDce(ZeroDCE):\n\"\"\"A faster version of the Zero-DCE (Zero-DCE++) model implemented as a `tf.keras.Model`.\n    Zero-reference deep curve estimation is a method for unsupervised low-light image enhancement\n    that utilizes a deep learning model to estimate the enhancement curve for an image without any\n    reference to the original, well-lit image. The model is trained on a dataset of low-light and\n    normal-light images, and learns to predict the enhancement curve that will best improve the visual\n    quality of the low-light image. Once the enhancement curve is estimated, it can be applied to the\n    low-light image to enhance its visibility. This approach allows for real-time enhancement of\n    low-light images without the need for a reference image, making it useful in situations where a\n    reference image is not available or impractical to obtain.\n    ??? example \"Examples\"\n        - [Training an unsupervised low-light enhancement model using Fast Zero-DCE](../../examples/train_fast_zero_dce).\n    !!! info \"References\"\n        1. [Learning to Enhance Low-Light Image via Zero-Reference Deep Curve Estimation](https://li-chongyi.github.io/Proj_Zero-DCE++.html)\n        2. [Official PyTorch implementation of Zero-DCE++](https://github.com/Li-Chongyi/Zero-DCE_extension)\n    Args:\n        num_intermediate_filters (int): number of filters in the intermediate convolutional layers.\n        num_iterations (int): number of iterations of enhancement.\n        decoder_channel_factor (int): factor by which number filters in the decoder of deep curve\n            estimation layer is multiplied.\n    \"\"\"\ndef __init__(\nself,\nnum_intermediate_filters: int,\nnum_iterations: int,\ndecoder_channel_factor: int,\n*args,\n**kwargs\n):\nsuper().__init__(\nnum_intermediate_filters,\nnum_iterations,\ndecoder_channel_factor,\n*args,\n**kwargs\n)\nself.deep_curve_estimation = FastDeepCurveEstimationLayer(\nnum_intermediate_filters=self.num_intermediate_filters,\nnum_iterations=self.num_iterations,\ndecoder_channel_factor=self.decoder_channel_factor,\n)\ndef get_enhanced_image(self, data, output):\nenhanced_image = data\nfor idx in range(self.num_iterations):\nenhanced_image = enhanced_image + output * (\ntf.square(enhanced_image) - enhanced_image\n)\nreturn enhanced_image\n</code></pre>"},{"location":"models/zero_dce_model/#zero-dce-blocks","title":"Zero-DCE Blocks","text":""},{"location":"models/zero_dce_model/#restorers.model.zero_dce.dw_conv.DepthwiseSeparableConvolution","title":"<code>DepthwiseSeparableConvolution</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>Depthwise-separable convolution implemented as a <code>tf.keras.layers.Layer</code>.</p> <p>References</p> <ol> <li>Official PyTorch implementation of Zero-DCE++</li> </ol> <p>Parameters:</p> Name Type Description Default <code>intermediate_channels</code> <code>int</code> <p>number of input channels.</p> required <code>output_channels</code> <code>int</code> <p>number of output channels.</p> required Source code in <code>restorers/model/zero_dce/dw_conv.py</code> <pre><code>class DepthwiseSeparableConvolution(tf.keras.layers.Layer):\n\"\"\"Depthwise-separable convolution implemented as a `tf.keras.layers.Layer`.\n    !!! info \"References\"\n        1. [Official PyTorch implementation of Zero-DCE++](https://github.com/Li-Chongyi/Zero-DCE_extension/blob/main/Zero-DCE%2B%2B/model.py#L8)\n    Args:\n        intermediate_channels (int): number of input channels.\n        output_channels (int): number of output channels.\n    \"\"\"\ndef __init__(\nself, intermediate_channels: int, output_channels: int, *args, **kwargs\n) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.intermediate_channels = intermediate_channels\nself.output_channels = output_channels\nself.depthwise_convolution = tf.keras.layers.Conv2D(\nfilters=intermediate_channels,\nkernel_size=(3, 3),\npadding=\"same\",\ngroups=intermediate_channels,\n)\nself.pointwise_convolution = tf.keras.layers.Conv2D(\nfilters=output_channels, kernel_size=(1, 1), padding=\"valid\"\n)\ndef call(self, inputs: tf.Tensor) -&gt; tf.Tensor:\nreturn self.pointwise_convolution(self.depthwise_convolution(inputs))\ndef get_config(self) -&gt; Dict:\nconfig = super().get_config()\nconfig.update(\n{\n\"intermediate_channels\": self.intermediate_channels,\n\"output_channels\": self.output_channels,\n}\n)\nreturn config\n</code></pre>"},{"location":"models/zero_dce_model/#restorers.model.zero_dce.dce_layer.DeepCurveEstimationLayer","title":"<code>DeepCurveEstimationLayer</code>","text":"<p>         Bases: <code>tf.keras.layers.Layer</code></p> <p>The Deep Curve Estimation layer (also referred to as the DCE-Net) implemented as a <code>tf.keras.layers.Layer</code>.</p> <p>The input to the DCE layer is a low-light image while the outputs are a set of pixel-wise curve parameter maps for corresponding higherorder curves. The DCE layer contains seven convolutional layers with symmetrical skip-connection. In the first six convolutional layers, each convolutional layer consists of 32 convolutional kernels of size 3\u00d73 and stride 1 followed by the ReLU activation function. The last convolutional layer consists of 24 convolutional kernels of size 3\u00d73 and stride 1 followed by the Tanh activation function, which produces 24 curve parameter maps for eight iterations, where each iteration requires three curve parameter maps for the three channels (i.e., RGB channels).</p> <p></p> <p>References</p> <ol> <li>Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement</li> <li>Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)</li> <li>Official PyTorch implementation of Zero-DCE</li> <li>Unofficial PyTorch implementation of Zero-DCE</li> <li>Tensorflow implementation of Zero-DCE</li> <li>Keras tutorial for implementing Zero-DCE</li> </ol> <p>Parameters:</p> Name Type Description Default <code>num_intermediate_filters</code> <code>int</code> <p>number of filters in the intermediate convolutional layers.</p> required <code>num_iterations</code> <code>int</code> <p>number of iterations of enhancement.</p> required <code>decoder_channel_factor</code> <code>int</code> <p>factor by which number filters in the decoder is multiplied.</p> required Source code in <code>restorers/model/zero_dce/dce_layer.py</code> <pre><code>class DeepCurveEstimationLayer(tf.keras.layers.Layer):\n\"\"\"The Deep Curve Estimation layer (also referred to as the DCE-Net) implemented as a\n    `tf.keras.layers.Layer`.\n    The input to the DCE layer is a low-light image while the outputs are a set of pixel-wise\n    curve parameter maps for corresponding higherorder curves. The DCE layer contains seven\n    convolutional layers with symmetrical skip-connection. In the first six convolutional layers,\n    each convolutional layer consists of 32 convolutional kernels of size 3\u00d73 and stride 1 followed\n    by the ReLU activation function. The last convolutional layer consists of 24 convolutional kernels\n    of size 3\u00d73 and stride 1 followed by the Tanh activation function, which produces 24 curve parameter\n    maps for eight iterations, where each iteration requires three curve parameter maps for the three\n    channels (i.e., RGB channels).\n    ![](https://i.imgur.com/HtIg34W.png)\n    !!! info \"References\"\n        1. [Zero-DCE: Zero-reference Deep Curve Estimation for Low-light Image Enhancement](https://openaccess.thecvf.com/content_CVPR_2020/papers/Guo_Zero-Reference_Deep_Curve_Estimation_for_Low-Light_Image_Enhancement_CVPR_2020_paper.pdf)\n        2. [Zero-Reference Learning for Low-Light Image Enhancement (Supplementary Material)](https://openaccess.thecvf.com/content_CVPR_2020/supplemental/Guo_Zero-Reference_Deep_Curve_CVPR_2020_supplemental.pdf)\n        3. [Official PyTorch implementation of Zero-DCE](https://github.com/Li-Chongyi/Zero-DCE/blob/master/Zero-DCE_code/model.py)\n        4. [Unofficial PyTorch implementation of Zero-DCE](https://github.com/bsun0802/Zero-DCE/blob/master/code/model.py)\n        5. [Tensorflow implementation of Zero-DCE](https://github.com/tuvovan/Zero_DCE_TF)\n        6. [Keras tutorial for implementing Zero-DCE](https://keras.io/examples/vision/zero_dce/#dcenet)\n    Args:\n        num_intermediate_filters (int): number of filters in the intermediate convolutional layers.\n        num_iterations (int): number of iterations of enhancement.\n        decoder_channel_factor (int): factor by which number filters in the decoder is multiplied.\n    \"\"\"\ndef __init__(\nself,\nnum_intermediate_filters: int,\nnum_iterations: int,\ndecoder_channel_factor: int,\n*args,\n**kwargs\n) -&gt; None:\nsuper().__init__(*args, **kwargs)\nself.num_intermediate_filters = num_intermediate_filters\nself.num_iterations = num_iterations\nself.decoder_channel_factor = decoder_channel_factor\nself.define_convolution_layers()\ndef define_convolution_layers(self) -&gt; None:\nself.convolution_1 = tf.keras.layers.Conv2D(\nfilters=self.num_intermediate_filters,\nkernel_size=(3, 3),\npadding=\"same\",\nactivation=\"relu\",\n)\nself.convolution_2 = tf.keras.layers.Conv2D(\nfilters=self.num_intermediate_filters,\nkernel_size=(3, 3),\npadding=\"same\",\nactivation=\"relu\",\n)\nself.convolution_3 = tf.keras.layers.Conv2D(\nfilters=self.num_intermediate_filters,\nkernel_size=(3, 3),\npadding=\"same\",\nactivation=\"relu\",\n)\nself.convolution_4 = tf.keras.layers.Conv2D(\nfilters=self.num_intermediate_filters,\nkernel_size=(3, 3),\npadding=\"same\",\nactivation=\"relu\",\n)\nself.convolution_5 = tf.keras.layers.Conv2D(\nfilters=self.num_intermediate_filters,\nkernel_size=(3, 3),\npadding=\"same\",\nactivation=\"relu\",\n)\nself.convolution_6 = tf.keras.layers.Conv2D(\nfilters=self.num_intermediate_filters,\nkernel_size=(3, 3),\npadding=\"same\",\nactivation=\"relu\",\n)\nself.convolution_7 = tf.keras.layers.Conv2D(\nfilters=self.num_iterations * 3,\nkernel_size=(3, 3),\npadding=\"same\",\nactivation=\"tanh\",\n)\ndef call(\nself, inputs: tf.Tensor, training=None, mask=None, *args, **kwargs\n) -&gt; tf.Tensor:\nout_1 = self.convolution_1(inputs)\nout_2 = self.convolution_2(out_1)\nout_3 = self.convolution_3(out_2)\nout_4 = self.convolution_4(out_3)\nout_5 = self.convolution_5(tf.concat([out_3, out_4], axis=-1))\nout_6 = self.convolution_6(tf.concat([out_2, out_5], axis=-1))\nalphas_stacked = self.convolution_7(tf.concat([out_1, out_6], axis=-1))\nreturn alphas_stacked\ndef get_config(self) -&gt; Dict:\nreturn {\n\"num_intermediate_filters\": self.num_intermediate_filters,\n\"num_iterations\": self.num_iterations,\n\"decoder_channel_factor\": self.decoder_channel_factor,\n}\n</code></pre>"},{"location":"models/zero_dce_model/#restorers.model.zero_dce.dce_layer.FastDeepCurveEstimationLayer","title":"<code>FastDeepCurveEstimationLayer</code>","text":"<p>         Bases: <code>DeepCurveEstimationLayer</code></p> <p>A faster version of the Deep Curve Estimation layer implemented as a <code>tf.keras.layers.Layer</code>.</p> <p>Reference:</p> <ol> <li>Official PyTorch implementation of Zero-DCE++</li> </ol> <p>Parameters:</p> Name Type Description Default <code>num_intermediate_filters</code> <code>int</code> <p>number of filters in the intermediate convolutional layers.</p> required <code>num_iterations</code> <code>int</code> <p>number of iterations of enhancement.</p> required Source code in <code>restorers/model/zero_dce/dce_layer.py</code> <pre><code>class FastDeepCurveEstimationLayer(DeepCurveEstimationLayer):\n\"\"\"A faster version of the Deep Curve Estimation layer implemented as a\n    `tf.keras.layers.Layer`.\n    Reference:\n    1. [Official PyTorch implementation of Zero-DCE++](https://github.com/Li-Chongyi/Zero-DCE_extension/blob/main/Zero-DCE%2B%2B/model.py#L8)\n    Args:\n        num_intermediate_filters (int): number of filters in the intermediate convolutional layers.\n        num_iterations (int): number of iterations of enhancement.\n    \"\"\"\ndef __init__(\nself,\nnum_intermediate_filters: int,\nnum_iterations: int,\ndecoder_channel_factor: int,\n*args,\n**kwargs\n):\nsuper().__init__(\nnum_intermediate_filters,\nnum_iterations,\ndecoder_channel_factor,\n*args,\n**kwargs\n)\ndef define_convolution_layers(self):\nself.convolution_1 = DepthwiseSeparableConvolution(\nintermediate_channels=3, output_channels=self.num_intermediate_filters\n)\nself.convolution_2 = DepthwiseSeparableConvolution(\nintermediate_channels=self.num_intermediate_filters,\noutput_channels=self.num_intermediate_filters,\n)\nself.convolution_3 = DepthwiseSeparableConvolution(\nintermediate_channels=self.num_intermediate_filters,\noutput_channels=self.num_intermediate_filters,\n)\nself.convolution_4 = DepthwiseSeparableConvolution(\nintermediate_channels=self.num_intermediate_filters,\noutput_channels=self.num_intermediate_filters,\n)\nself.convolution_5 = DepthwiseSeparableConvolution(\nintermediate_channels=self.num_intermediate_filters\n* self.decoder_channel_factor,\noutput_channels=self.num_intermediate_filters,\n)\nself.convolution_6 = DepthwiseSeparableConvolution(\nintermediate_channels=self.num_intermediate_filters\n* self.decoder_channel_factor,\noutput_channels=self.num_intermediate_filters,\n)\nself.convolution_7 = DepthwiseSeparableConvolution(\nintermediate_channels=self.num_intermediate_filters * 2, output_channels=3\n)\ndef call(self, inputs):\nreturn tf.tanh(super().call(inputs))\n</code></pre>"}]}