{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/wandb/examples/blob/master/colabs/keras/restorers/Train_Nafnet_Restorers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "<!--- @wandbcode{restorers-nafnet-train} -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåà Restorers + WandB ü™Ñüêù\n",
    "\n",
    "<!--- @wandbcode{restorers-mirnetv2-train} -->\n",
    "\n",
    "This notebook shows how to train a [NAFNet](https://arxiv.org/abs/2204.04676) model for low-light enhancement using [**restorers**](https://github.com/soumik12345/restorers) and [**wandb**](https://wandb.ai/site). For more details regarding usage of restorers, refer to the following report:\n",
    "\n",
    "[![](https://raw.githubusercontent.com/wandb/assets/main/wandb-github-badge-gradient.svg)](https://wandb.ai/ml-colabs/low-light-enhancement/reports/Lighting-up-Images-in-the-Deep-Learning-Era--VmlldzozNzE4Njkz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q --upgrade pip setuptools\n",
    "!pip install git+https://github.com/soumik12345/restorers.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "import wandb\n",
    "import tensorflow as tf\n",
    "\n",
    "from restorers.dataloader import LOLDataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"low-light-enhancement\")\n",
    "\n",
    "# define dataloader for the LoL dataset\n",
    "data_loader = LOLDataLoader(\n",
    "    # size of image crops on which we will train\n",
    "    image_size=128,\n",
    "    # bit depth of the images\n",
    "    bit_depth=8,\n",
    "    # fraction of images for validation\n",
    "    val_split=0.2,\n",
    "    # visualize the dataset on WandB or not\n",
    "    visualize_on_wandb=True,\n",
    "    # the wandb artifact address of the dataset,\n",
    "    # this can be found from the `Usage` tab of\n",
    "    # the aforemenioned weave panel\n",
    "    dataset_artifact_address=\"ml-colabs/dataset/LoL:v0\",\n",
    ")\n",
    "\n",
    "# call `get_datasets` on the `data_loader` to get\n",
    "# the TensorFlow datasets corresponding to the \n",
    "# training and validation splits\n",
    "datasets = data_loader.get_datasets(batch_size=2)\n",
    "train_dataset, val_dataset = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# import MirNetv2 from restorers\n",
    "from restorers.model import NAFNet\n",
    "\n",
    "\n",
    "# define the MirNetv2 model; this gives us a `tf.keras.Model`\n",
    "model = NAFNet(\n",
    "    filters=16,\n",
    "    middle_block_num=1,\n",
    "    encoder_block_nums=(1, 1, 1, 1),\n",
    "    decoder_block_nums=(1, 1, 1, 1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "from restorers.losses import CharbonnierLoss\n",
    "# import Peak Signal-to-Noise Ratio and Structural Similarity metrics,\n",
    "# implemented as part of restorers\n",
    "from restorers.metrics import PSNRMetric, SSIMMetric\n",
    "\n",
    "\n",
    "loss = CharbonnierLoss(\n",
    "    # a small constant to avoid division by zero\n",
    "    epsilon=1e-3,\n",
    "    # type of reduction applied to the loss, it needs to be\n",
    "    # explicitly specified in case of distributed training\n",
    "    reduction=tf.keras.losses.Reduction.SUM,\n",
    ")\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.experimental.AdamW(learning_rate=2e-4,)\n",
    "\n",
    "psnr_metric = PSNRMetric(max_val=1.0) # peak signal-to-noise ratio metric\n",
    "ssim_metric = SSIMMetric(max_val=1.0) # structural similarity metric\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer, loss=loss, metrics=[psnr_metric, ssim_metric]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "# import the wandb callbacks for keras\n",
    "from wandb.keras import WandbMetricsLogger, WandbModelCheckpoint\n",
    "\n",
    "\n",
    "callbacks = [\n",
    "    # define the metrics logger callback;\n",
    "    # we set the `log_freq=\"batch\"` explicitly\n",
    "    # to the metrics are logged both batch-wise and epoch-wise\n",
    "    WandbMetricsLogger(log_freq=\"batch\"),\n",
    "    # define the model checkpoint callback\n",
    "    WandbModelCheckpoint(\n",
    "        filepath=\"checkpoint\",\n",
    "        monitor=\"val_loss\",\n",
    "        save_best_only=False,\n",
    "        save_weights_only=False,\n",
    "        initial_value_threshold=None,\n",
    "    )\n",
    "]\n",
    "\n",
    "# call model.fit()\n",
    "model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=callbacks,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
