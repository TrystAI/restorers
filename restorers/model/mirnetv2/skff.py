from typing import Optional, Tuple, Dict

import tensorflow as tf


class SelectiveKernelFeatureFusion(tf.keras.layers.Layer):
    """Implementation of the Selective Kernel Feature Fusion Layer.

    This layer adaptively adjusts the input receptive fields by using multi-scale
    feature generation (in the same layer) followed by feature aggregation and
    selection. This is done using two distinct operations:

    - **Fuse Operation:** The fuse operator generates global feature descriptors by
        combining the information from multiresolution streams.
    - **Select Operation:** The select operator uses the feature descriptors
        generated by the fuse operator to recalibrate the feature maps
        (of different streams) followed by their aggregation.
    
    ![Selective Kernel Feature Fusion](https://i.imgur.com/kVn5N8t.png){ loading=lazy }

    Reference:

    1. [Selective Kernel Networks](https://arxiv.org/abs/1903.06586)
    2. [Learning Enriched Features for Fast Image Restoration and Enhancement](https://www.waqaszamir.com/publication/zamir-2022-mirnetv2/zamir-2022-mirnetv2.pdf)
    3. [Official PyTorch implementation of MirNetv2](https://github.com/swz30/MIRNetv2/blob/main/basicsr/models/archs/mirnet_v2_arch.py#L17)

    Args:
        channels (int): number of channels in the feature map.
    """

    def __init__(self, channels: int, *args, **kwargs) -> None:
        super().__init__(*args, **kwargs)

        self.channels = channels
        self.hidden_channels = max(int(self.channels / 8), 4)
        self.average_pooling = tf.keras.layers.GlobalAveragePooling2D(keepdims=True)

        self.conv_channel_downscale = tf.keras.layers.Conv2D(
            self.hidden_channels, kernel_size=1, padding="same"
        )
        self.conv_attention_1 = tf.keras.layers.Conv2D(
            self.channels, kernel_size=1, strides=1, padding="same"
        )
        self.conv_attention_2 = tf.keras.layers.Conv2D(
            self.channels, kernel_size=1, strides=1, padding="same"
        )
        self.softmax = tf.keras.layers.Softmax(axis=-1)

    def call(
        self, inputs: Tuple[tf.Tensor], training: Optional[bool] = None
    ) -> tf.Tensor:
        # Fuse operation
        combined_input_features = inputs[0] + inputs[1]
        channel_wise_statistics = self.average_pooling(combined_input_features)
        downscaled_channel_wise_statistics = self.conv_channel_downscale(
            channel_wise_statistics
        )
        attention_vector_1 = self.softmax(
            self.conv_attention_1(downscaled_channel_wise_statistics)
        )
        attention_vector_2 = self.softmax(
            self.conv_attention_2(downscaled_channel_wise_statistics)
        )

        # Select operation
        selected_features = (
            inputs[0] * attention_vector_1 + inputs[1] * attention_vector_2
        )
        return selected_features

    def get_config(self) -> Dict:
        return {"channels": self.channels}
